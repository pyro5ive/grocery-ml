{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192abc4e-1a90-416f-b168-9093c1493c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import os\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from temporal_features import TemporalFeatures\n",
    "from holiday_features import HolidayFeatures\n",
    "from wallmart_rcpt_parser import WallmartRecptParser\n",
    "from winn_dixie_recpt_parser import WinnDixieRecptParser \n",
    "from hidden_layer_param_builder import HiddenLayerParamSetBuilder\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "\n",
    "print(os.getcwd())\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47046089-a2d9-4136-95bb-3888c94aab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def normalizeAndDropCols(df, cols):\n",
    "    for col in cols:\n",
    "        # Replace the sentinel 999 with NaN so it doesn't distort mean/std\n",
    "        df[col] = df[col].replace(999, np.nan)\n",
    "\n",
    "        # Compute mean/std ignoring NaN\n",
    "        mean = df[col].mean()\n",
    "        std  = df[col].std() or 1.0\n",
    "\n",
    "        # Normalize\n",
    "        df[col + \"_norm\"] = (df[col] - mean) / std\n",
    "\n",
    "        # After normalization: missing values become 0 (neutral)\n",
    "        df[col + \"_norm\"] = df[col + \"_norm\"].fillna(0.0)\n",
    "\n",
    "    return df.drop(columns=cols)\n",
    "\n",
    "\n",
    "#def normalizeAndDropCols(df, cols):\n",
    "#    for col in cols:\n",
    "#        std = df[col].std() or 1.0\n",
    "#        df[col + \"_norm\"] = (df[col] - df[col].mean()) / std\n",
    "#    return df.drop(columns=cols)\n",
    "\n",
    "\n",
    "\n",
    "def canonicalize_items(df, patterns, canonical_name):\n",
    "    \"\"\"\n",
    "    For each pattern in `patterns`, find rows where `item` contains the pattern\n",
    "    and replace df['item'] with `canonical_name`.\n",
    "    \"\"\"\n",
    "    for p in patterns:\n",
    "        mask = df[\"item\"].str.contains(p, case=False, na=False)\n",
    "        df.loc[mask, \"item\"] = canonical_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb5f82-f270-4a38-9414-341412e3c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WEATHER PREP ---\n",
    "weatherCols=[\"datetime\", \"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"]\n",
    "df_weather = pd.read_csv(\"datasets/VisualCrossing-70062 2000-01-01 to 2025-12-14.csv\", usecols=weatherCols)\n",
    "\n",
    "df_weather[\"datetime\"] = pd.to_datetime(df_weather[\"datetime\"])\n",
    "df_weather = df_weather.set_index(\"datetime\").sort_index()\n",
    "\n",
    "df_weather[\"temp_5day_avg\"] = df_weather[\"temp\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"feelsLike_5day_avg\"] = df_weather[\"feelslike\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"dew_5day_avg\"] = df_weather[\"dew\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"humidity_5day_avg\"] = df_weather[\"humidity\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"precip_5day_avg\"] = df_weather[\"precip\"].rolling(5, min_periods=1).mean()\n",
    "\n",
    "df_weather = df_weather.drop(columns=[\"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"])\n",
    "\n",
    "# convert index to date for merging\n",
    "df_weather[\"date\"] = df_weather.index.date\n",
    "df_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"])\n",
    "df_weather = df_weather.set_index(\"date\")\n",
    "\n",
    "#grouped.to_csv(\"grouped.csv\", index=False)\n",
    "#grouped.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6c3bb-900a-430b-b7b9-7eb518b2d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def ImportWallMart(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Import all Walmart receipt CSV files from a folder.\n",
    "    Adds a 'source' column set to the CSV filename.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.lower().endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            dataframe = pd.read_csv(file_path)\n",
    "            dataframe[\"source\"] = file_name\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "    if len(dataframes) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622cefe-d5e0-496d-b547-0cfd003d4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "recptParser  = WinnDixieRecptParser();\n",
    "\n",
    "for p in Path(\"winndixie rcpts/StevePhone2/pdf/text\").glob(\"*.txt\"):\n",
    "    result = recptParser.parse(p.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "    for r in result[\"items\"]:\n",
    "        rows.append({\n",
    "            \"source\": p.name,\n",
    "            \"date\": result[\"date\"],\n",
    "            \"time\": result[\"time\"],\n",
    "            #\"manager\": result[\"manager\"],\n",
    "            #\"cashier\": result[\"cashier\"],\n",
    "            \"item\": r[\"item\"]\n",
    "            #\"qty\": r[\"qty\"],\n",
    "            #\"reg\": r[\"reg\"],\n",
    "            #\"youPay\": r[\"youPay\"],\n",
    "            #\"reportedItemsSold\": result[\"reported\"],\n",
    "            #\"rowsMatchReported\": result[\"validation\"][\"rowsMatchReported\"],\n",
    "            #\"qtyMatchReported\": result[\"validation\"][\"qtyMatchReported\"],\n",
    "        })\n",
    "\n",
    "winndixie_df = pd.DataFrame(rows)\n",
    "\n",
    "winndixie_df[\"date\"] = pd.to_datetime(winndixie_df[\"date\"])\n",
    "winndixie_df[\"time\"] = winndixie_df[\"time\"].astype(str)\n",
    "\n",
    "winndixie_df = WinnDixieRecptParser.remove_duplicate_receipt_files(winndixie_df)\n",
    "\n",
    "winndixie_df = winndixie_df.sort_values(by=[\"date\", \"time\"]).reset_index(drop=True)\n",
    "winndixie_df = winndixie_df.drop(columns=[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a996e24-1868-4bf5-92e1-266b8d0719c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "wallmart_raw = WallmartRecptParser.ImportWallMart(\"./walmart\")\n",
    "\n",
    "## rename cols\n",
    "wallmart_df = wallmart_raw[[\"Order Date\",\"Product Description\", \"source\"]].copy()\n",
    "wallmart_df = wallmart_df.rename(columns={\n",
    "    \"Order Date\": \"date\",\n",
    "    \"Product Description\": \"item\"\n",
    "})\n",
    "\n",
    "wallmart_df[\"date\"] = pd.to_datetime(wallmart_df[\"date\"])\n",
    "winndixie_df[\"date\"] = pd.to_datetime(winndixie_df[\"date\"])\n",
    "\n",
    "combined_df = pd.concat(\n",
    "    [winndixie_df, wallmart_df[[\"date\", \"item\", \"source\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# remove - \n",
    "combined_df[\"item\"] = (combined_df[\"item\"]\n",
    "        .str.replace(r\"^\\s*[-–—]\\s*\", \"\", regex=True)\n",
    "        .str.strip()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd148d-3caa-4967-bb0d-ec482d400ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_patterns = [\"know-and-love-milk\", \"kandl-milk\", \"prairie-farm-milk\",\"kleinpeter-milk\", \"kl-milk\", \"Milk, Fat Free,\", \"Fat-Free Milk\"]\n",
    "canonicalize_items(combined_df, milk_patterns, \"milk\")\n",
    "\n",
    "bread_patterns = [\"bunny-bread\",\"se-grocers-bread\",\"seg-sandwich-bread\", \"seg-white-bread\"]\n",
    "canonicalize_items(combined_df, bread_patterns, \"bread\")\n",
    "\n",
    "cheese_patterns = [\"dandw-cheese\", \"kraft-cheese\", \"se-grocers-cheese\", \"know-and-love-cheese\"]\n",
    "canonicalize_items(combined_df, cheese_patterns, \"cheese\")\n",
    "\n",
    "mayo_patterns = [\"blue-plate-mayo\", \"blue-plate-mynnase\"]\n",
    "canonicalize_items(combined_df, mayo_patterns, \"mayo\")\n",
    "\n",
    "chicken_patterns = [\"chicken-cutlet\", \"chicken-leg\", \"chicken-thigh\", \"chicken-thighs\"]\n",
    "canonicalize_items(combined_df, chicken_patterns, \"chicken\")\n",
    "\n",
    "yogurt_patterns = [\"chobani-yogrt-flip\", \"chobani-yogurt\"]\n",
    "canonicalize_items(combined_df, yogurt_patterns, \"yogurt\")\n",
    "\n",
    "coke_patterns = [\"coca-cola\", \"coca-cola-cola\", \"cocacola-soda\"]\n",
    "canonicalize_items(combined_df, coke_patterns, \"coke\")\n",
    "\n",
    "hugbi_patterns = [\"hugbi-pies\", \"-hugbi-pies\"]\n",
    "canonicalize_items(combined_df, hugbi_patterns, \"hugbi-pies\")\n",
    "\n",
    "minute_maid_patterns = [\"minute-maid-drink\", \"minute-maid-drinks\", \"minute-maid-lmnade\"]\n",
    "canonicalize_items(combined_df, minute_maid_patterns, \"minute-maid-drink\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4b591-943b-444d-ae9c-1557b99a282f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### CREATE ITEM IDs\n",
    "unique_items = sorted(combined_df[\"item\"].unique())\n",
    "item_to_id = {item: idx for idx, item in enumerate(unique_items)}\n",
    "id_to_item = {idx: item for item, idx in item_to_id.items()}\n",
    "combined_df[\"itemId\"] = combined_df[\"item\"].map(item_to_id)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df.info()\n",
    "combined_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347749b-3977-40e3-a9a2-79c0f5c1389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Build full receipt × item table WITHOUT using qty\n",
    "# ============================================================\n",
    "\n",
    "# 1. Mark actual purchases in the raw receipt rows\n",
    "combined_df[\"didBuy\"] = 1\n",
    "\n",
    "# 2. Build complete grid\n",
    "all_items = combined_df[\"itemId\"].unique()\n",
    "all_dates = combined_df[\"date\"].unique()\n",
    "\n",
    "full = (\n",
    "    pd.MultiIndex.from_product(\n",
    "        [all_dates, all_items], \n",
    "        names=[\"date\", \"itemId\"]\n",
    "    ).to_frame(index=False)\n",
    ")\n",
    "\n",
    "# 3. Merge raw purchases onto the full grid\n",
    "df_full = full.merge(\n",
    "    combined_df[[\"date\", \"itemId\", \"item\", \"source\", \"didBuy\"]],\n",
    "    on=[\"date\", \"itemId\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4. Fill missing purchases with didBuy=0\n",
    "df_full[\"didBuy\"] = df_full[\"didBuy\"].fillna(0).astype(int)\n",
    "\n",
    "# 5. NOW REPLACE combined_df with df_full\n",
    "combined_df = df_full.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0664c4-32c3-4379-ba57-71018f599c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build grouped table (one row per trip date)\n",
    "\n",
    "grouped = ( combined_df[[\"date\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"date\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "grouped[\"daysSinceLastTrip\"] = TemporalFeatures.DaysSinceLastTrip(grouped)\n",
    "grouped[\"avgDaysBetweenTrips\"] = TemporalFeatures.AvgDaysBetweenTrips(grouped)\n",
    "\n",
    "# 3. Holiday / School features\n",
    "grouped[\"daysUntilNextHoliday\"] = grouped[\"date\"].apply(HolidayFeatures.daysUntilNextHoliday)\n",
    "grouped[\"daysSinceLastHoliday\"] = grouped[\"date\"].apply(HolidayFeatures.daysSinceLastHoliday)\n",
    "grouped[\"holidayProximityIndex\"] = grouped[\"date\"].apply(HolidayFeatures.holidayProximityIndex)\n",
    "grouped[\"daysUntilSchoolStart\"] = grouped[\"date\"].apply(HolidayFeatures.daysUntilSchoolStart)\n",
    "grouped[\"daysUntilSchoolEnd\"]   = grouped[\"date\"].apply(HolidayFeatures.daysUntilSchoolEnd)\n",
    "grouped[\"schoolSeasonIndex\"]    = grouped[\"date\"].apply(HolidayFeatures.schoolSeasonIndex)\n",
    "\n",
    "\n",
    "grouped = TemporalFeatures.CreateDateFeatures(grouped)\n",
    "\n",
    "# merge in weather\n",
    "grouped = grouped.merge(df_weather, on=\"date\", how=\"left\")\n",
    "\n",
    "combined_df = combined_df.merge(grouped, on=\"date\", how=\"left\")\n",
    "combined_df.info()\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf2225-3a20-45fe-810f-624a5e984ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# FREQUENCY WINDOWS (7, 15, 30, 90, 365)\n",
    "# True rolling-window implementation\n",
    "# ================================================\n",
    "def fill_freq(group):\n",
    "    group = group.copy()\n",
    "    group = group.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    history = []\n",
    "\n",
    "    col_date = group.columns.get_loc(\"date\")\n",
    "    col_buy = group.columns.get_loc(\"didBuy\")\n",
    "    col_freq = {w: group.columns.get_loc(f\"freq_{w}\") for w in freq_windows}\n",
    "\n",
    "    for i in range(len(group)):\n",
    "        cur_date = group.iat[i, col_date]\n",
    "\n",
    "        # record purchase\n",
    "        if group.iat[i, col_buy] == 1:\n",
    "            history.append(cur_date)\n",
    "\n",
    "        # prune history ONCE using largest window\n",
    "        cutoff_max = cur_date - pd.Timedelta(days=max_w)\n",
    "        history = [d for d in history if d >= cutoff_max]\n",
    "\n",
    "        # compute windowed counts\n",
    "        for w in freq_windows:\n",
    "            cutoff = cur_date - pd.Timedelta(days=w)\n",
    "            count = 0\n",
    "            for d in history:\n",
    "                if d >= cutoff:\n",
    "                    count += 1\n",
    "            group.iat[i, col_freq[w]] = count\n",
    "\n",
    "    return group\n",
    "#######################################################\n",
    "freq_windows = [7, 15, 30, 90, 365]\n",
    "max_w = max(freq_windows)\n",
    "\n",
    "# initialize columns\n",
    "for w in freq_windows:\n",
    "    combined_df[f\"freq_{w}\"] = np.nan\n",
    "\n",
    "combined_df = (\n",
    "    combined_df\n",
    "    .groupby(\"itemId\", group_keys=False)\n",
    "    .apply(fill_freq)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955044d3-645d-44ad-b9b0-f3129b1e565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INCREASING DAILY daysSinceLastPurchase (resets on purchase)\n",
    "# ============================================================\n",
    "def fill_item(group):\n",
    "    group = group.copy()\n",
    "    # iterate row-by-row using positional index\n",
    "    for i in range(1, len(group)):\n",
    "        if pd.isna(group.iat[i, group.columns.get_loc(\"daysSinceLastPurchase\")]):\n",
    "            prev_val = group.iat[i-1, group.columns.get_loc(\"daysSinceLastPurchase\")]\n",
    "            trip_gap = group.iat[i, group.columns.get_loc(\"daysSinceLastTrip\")]\n",
    "            group.iat[i, group.columns.get_loc(\"daysSinceLastPurchase\")] = prev_val + trip_gap\n",
    "    return group\n",
    "##########################################################################################\n",
    "\n",
    "combined_df = combined_df.sort_values([\"itemId\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Start with NaN everywhere\n",
    "combined_df[\"daysSinceLastPurchase\"] = np.nan\n",
    "\n",
    "# Set 0 on purchase days\n",
    "combined_df.loc[combined_df[\"didBuy\"] == 1, \"daysSinceLastPurchase\"] = 0\n",
    "combined_df = combined_df.groupby(\"itemId\", group_keys=False).apply(fill_item)\n",
    "\n",
    "# Items with no purchase history get 999\n",
    "combined_df[\"daysSinceLastPurchase\"] = combined_df[\"daysSinceLastPurchase\"].fillna(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e477f-aa81-4d9e-bb31-2acece50cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ITEM-LEVEL HABIT FEATURES (TF-IDF ANALOG)\n",
    "# ============================================================\n",
    "def build_habit_features(df, tau_days=120):\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    total_trips = df[\"date\"].nunique()\n",
    "    timeline_days = (df[\"date\"].max() - df[\"date\"].min()).days or 1\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for itemId, g in df.groupby(\"itemId\"):\n",
    "        buys = g[g[\"didBuy\"] == 1][\"date\"]\n",
    "\n",
    "        if len(buys) == 0:\n",
    "            rows.append({\n",
    "                \"itemId\": itemId,\n",
    "                \"habitFrequency\": 0.0,\n",
    "                \"habitSpan\": 0.0,\n",
    "                \"habitDecay\": 0.0,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        first = buys.min()\n",
    "        last = buys.max()\n",
    "\n",
    "        habitFrequency = len(buys) / total_trips\n",
    "        habitSpan = (last - first).days / timeline_days\n",
    "        days_since_last = (df[\"date\"].max() - last).days\n",
    "        habitDecay = np.exp(-days_since_last / tau_days)\n",
    "\n",
    "        rows.append({\n",
    "            \"itemId\": itemId,\n",
    "            \"habitFrequency\": habitFrequency,\n",
    "            \"habitSpan\": habitSpan,\n",
    "            \"habitDecay\": habitDecay,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def compute_due_score(df,itemId=None,use_sigmoid=True,normalize=False, weights=None):\n",
    "    \"\"\"\n",
    "    Compute due_score from RAW (non-normalized) features.\n",
    "\n",
    "    Required columns:\n",
    "      - itemId\n",
    "      - daysSinceLastPurchase\n",
    "      - freq_30\n",
    "      - freq_90\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "\n",
    "    itemId : int | None\n",
    "        If provided, compute only for this itemId.\n",
    "        If None, compute for all items.\n",
    "\n",
    "    use_sigmoid : bool\n",
    "        Apply sigmoid → (0,1)\n",
    "\n",
    "    normalize : bool\n",
    "        Z-normalize instead (ignored if use_sigmoid=True)\n",
    "\n",
    "    weights : dict | None\n",
    "        Optional override for feature weights\n",
    "    \"\"\"\n",
    "\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            \"daysSinceLastPurchase\": 1.5,\n",
    "            \"freq_30\": 1.0,\n",
    "            \"freq_90\": 0.5\n",
    "        }\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Optional itemId filter\n",
    "    # --------------------------------------------------------\n",
    "    if itemId is not None:\n",
    "        df = df[df[\"itemId\"] == itemId].copy()\n",
    "    else:\n",
    "        df = df.copy()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # RAW linear score (pre-normalization)\n",
    "    # --------------------------------------------------------\n",
    "    df[\"due_score_raw\"] = (\n",
    "        weights[\"daysSinceLastPurchase\"] * df[\"daysSinceLastPurchase\"]\n",
    "      + weights[\"freq_30\"]              * df[\"freq_30\"]\n",
    "      + weights[\"freq_90\"]              * df[\"freq_90\"]\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Final due_score\n",
    "    # --------------------------------------------------------\n",
    "    if use_sigmoid:\n",
    "        df[\"due_score\"] = 1 / (1 + np.exp(-df[\"due_score_raw\"]))\n",
    "\n",
    "    elif normalize:\n",
    "        mean = df[\"due_score_raw\"].mean()\n",
    "        std  = df[\"due_score_raw\"].std() or 1.0\n",
    "        df[\"due_score\"] = (df[\"due_score_raw\"] - mean) / std\n",
    "\n",
    "    else:\n",
    "        df[\"due_score\"] = df[\"due_score_raw\"]\n",
    "\n",
    "    return df\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MERGE HABIT FEATURES\n",
    "# ============================================================\n",
    "habit_df = build_habit_features(combined_df)\n",
    "\n",
    "combined_df = combined_df.merge(habit_df, on=\"itemId\",how=\"left\")\n",
    "\n",
    "combined_df[[\"habitFrequency\", \"habitSpan\", \"habitDecay\"]] = (\n",
    "    combined_df[[\"habitFrequency\", \"habitSpan\", \"habitDecay\"]].fillna(0.0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba56be9-f33c-4089-b0e9-aecf98c2e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## trim fat\n",
    "# find rows with freq_365 of 1 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdaa6a-c44f-4d32-97d2-7acdd4922864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NORMALIZE TO ENCODED_DF\n",
    "# ============================================================\n",
    "\n",
    "freq_cols = [c for c in combined_df.columns if c.startswith(\"freq_\")]\n",
    "weather_cols = [c for c in combined_df.columns if c.endswith(\"_5day_avg\")]\n",
    "holiday_cols = [c for c in combined_df.columns if \"holiday\" in c.lower()]\n",
    "school_cols = [c for c in combined_df.columns if \"school\" in c.lower()]\n",
    "\n",
    "daysSince_purchase_cols = [\"daysSinceLastPurchase\"]\n",
    "daysSince_trip_cols     = [\"daysSinceLastTrip\"]\n",
    "\n",
    "habit_cols = [\"habitFrequency\", \"habitSpan\", \"habitDecay\"]\n",
    "\n",
    "encoded_df = combined_df.copy()\n",
    "encoded_df = normalizeAndDropCols(encoded_df, freq_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, weather_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, holiday_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, school_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, daysSince_purchase_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, daysSince_trip_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, habit_cols)\n",
    "\n",
    "encoded_df.info()\n",
    "encoded_df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b8c92-d433-4873-9856-c3f7cb6e5f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------- CYCLICAL FEATURES ----------\n",
    "encoded_df[\"dow_sin\"]   = np.sin(2 * np.pi * encoded_df[\"dow\"] / 7.0)\n",
    "encoded_df[\"dow_cos\"]   = np.cos(2 * np.pi * encoded_df[\"dow\"] / 7.0)\n",
    "encoded_df[\"month_sin\"] = np.sin(2 * np.pi * encoded_df[\"month\"] / 12.0)\n",
    "encoded_df[\"month_cos\"] = np.cos(2 * np.pi * encoded_df[\"month\"] / 12.0)\n",
    "encoded_df[\"doy_sin\"]   = np.sin(2 * np.pi * encoded_df[\"doy\"] / 365.0)\n",
    "encoded_df[\"doy_cos\"]   = np.cos(2 * np.pi * encoded_df[\"doy\"] / 365.0)\n",
    "\n",
    "encoded_df = encoded_df.drop(columns=[\"dow\", \"month\", \"doy\"], errors=\"ignore\")\n",
    "\n",
    "# ---------- NON-CYCLIC TIME FEATURES ----------\n",
    "nonCycCols = [\"year\", \"day\", \"quarter\"]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, nonCycCols)\n",
    "\n",
    "# ---------- DROP NON-MODEL COLS ----------\n",
    "cols_to_drop = [\"source\", \"item\", \"date\"]\n",
    "encoded_df = encoded_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "encoded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d176f88-24c4-4d8a-899f-44c3a5d7cf71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_df[\"due_score\"] = (\n",
    "    1.0 * encoded_df[\"daysSinceLastPurchase_norm\"]\n",
    "  + 1.0 * encoded_df[\"freq_30_norm\"]\n",
    "  ##+ 0.5 * encoded_df[\"freq_90_norm\"]\n",
    ")\n",
    "\n",
    "encoded_df[\"due_score\"] = 1 / (1 + np.exp(-encoded_df[\"due_score\"]))\n",
    "\n",
    "encoded_df.info()\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d5b52-1522-4fb9-87a4-f72e07812c17",
   "metadata": {},
   "source": [
    "# TRAIN / BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9fcbe-e123-4498-9824-73f2f3f5d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75bb34-193a-47ba-8e96-db8eaf6a372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df(dataframes, dir):\n",
    "    for name, df in dataframes.items():\n",
    "        csv_path = os.path.join(dir, f\"{name}.csv\")\n",
    "        df.to_csv(csv_path, index=True)\n",
    "##################################################################################\n",
    "\n",
    "def save_experiment( model, history,  dataframes,  build_params, train_params, numeric_cols,item_id_to_idx,base_dir=\"experiments\"):\n",
    "    name_parts = []\n",
    "\n",
    "    if \"embedding_dim\" in build_params:\n",
    "        name_parts.append(f\"emb{build_params['embedding_dim']}\")\n",
    "\n",
    "    if \"hiddenLayers\" in build_params:\n",
    "        hl = \"-\".join(str(x) for x in build_params[\"hiddenLayers\"])\n",
    "        name_parts.append(f\"hl{hl}\")\n",
    "\n",
    "    if \"epochs\" in train_params:\n",
    "        name_parts.append(f\"ep{train_params['epochs']}\")\n",
    "\n",
    "    exp_name = \"__\".join(name_parts) if name_parts else \"exp_unlabeled\"\n",
    "    exp_dir = os.path.join(base_dir, exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "    export_df(dataframes, exp_dir)\n",
    "    # ------------------------------------------------------------\n",
    "    # Save model\n",
    "    # ------------------------------------------------------------\n",
    "    model.save(os.path.join(exp_dir, \"model\"))\n",
    "    model.save_weights(os.path.join(exp_dir, \"weights.h5\"))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Save history\n",
    "    # ------------------------------------------------------------\n",
    "    history_path = os.path.join(exp_dir, \"history.json\")\n",
    "    history_file = open(history_path, \"w\")\n",
    "    json.dump(history.history, history_file, indent=2)\n",
    "    history_file.close()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Save numeric features\n",
    "    # ------------------------------------------------------------\n",
    "    numeric_path = os.path.join(exp_dir, \"numeric_features.json\")\n",
    "    numeric_file = open(numeric_path, \"w\")\n",
    "    json.dump(numeric_cols, numeric_file, indent=2)\n",
    "    numeric_file.close()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Save item index mapping\n",
    "    # ------------------------------------------------------------\n",
    "    item_map_path = os.path.join(exp_dir, \"item_id_to_idx.json\")\n",
    "    item_map_file = open(item_map_path, \"w\")\n",
    "    json.dump(\n",
    "        {str(int(k)): int(v) for k, v in item_id_to_idx.items()},\n",
    "        item_map_file,\n",
    "        indent=2\n",
    "    )\n",
    "    item_map_file.close()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Save params\n",
    "    # ------------------------------------------------------------\n",
    "    build_params_path = os.path.join(exp_dir, \"build_params.json\")\n",
    "    build_params_file = open(build_params_path, \"w\")\n",
    "    json.dump(build_params, build_params_file, indent=2)\n",
    "    build_params_file.close()\n",
    "\n",
    "    train_params_path = os.path.join(exp_dir, \"train_params.json\")\n",
    "    train_params_file = open(train_params_path, \"w\")\n",
    "    json.dump(train_params, train_params_file, indent=2)\n",
    "    train_params_file.close()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Save predictions\n",
    "    # ------------------------------------------------------------\n",
    "    ##predictions.to_csv(os.path.join(exp_dir, \"predictions.csv\"), index=False)\n",
    "\n",
    "    print(\"Saved experiment →\", exp_dir)\n",
    "##########################################################################################\n",
    "\n",
    "def build_and_compile_model(num_numeric_features, num_items, params):\n",
    "    num_in = layers.Input(shape=(num_numeric_features,))\n",
    "    item_in = layers.Input(shape=(), dtype=\"int32\")\n",
    "\n",
    "    emb = layers.Embedding(\n",
    "        input_dim=num_items,\n",
    "        output_dim=params[\"embedding_dim\"]\n",
    "    )(item_in)\n",
    "\n",
    "    x = layers.Concatenate()([num_in, layers.Flatten()(emb)])\n",
    "\n",
    "    for units in params[\"hiddenLayers\"]:\n",
    "        x = layers.Dense(units, activation=\"relu\")(x)\n",
    "\n",
    "    out = layers.Dense(\n",
    "        1,\n",
    "        activation=params.get(\"output_activation\", \"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = models.Model([num_in, item_in], out)\n",
    "\n",
    "    optimizer_name = params.get(\"optimizer\", \"adam\")\n",
    "    learning_rate = params.get(\"learning_rate\", 0.001)\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = optimizer_name\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=params.get(\"loss\", \"mse\"),\n",
    "        metrics=params.get(\"metrics\", [\"mae\"])\n",
    "    )\n",
    "\n",
    "    return model\n",
    "##########################################################################################\n",
    "\n",
    "def train_model(model, encoded_df, params):\n",
    "   \n",
    "    numeric_cols = [\n",
    "        c for c in encoded_df.columns\n",
    "        if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "    ]\n",
    "\n",
    "    Xn = encoded_df[numeric_cols].to_numpy(np.float32)\n",
    "    Xi = encoded_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "    targetVar  = encoded_df[\"due_score\"].to_numpy(np.float32)\n",
    "\n",
    "    Xn_tr, Xn_te, Xi_tr, Xi_te, targetVar_tr, y_te = train_test_split(Xn, Xi, targetVar, test_size=0.2, random_state=42)\n",
    "\n",
    "    history = model.fit(\n",
    "        [Xn_tr, Xi_tr],\n",
    "        targetVar_tr,\n",
    "        validation_split=0.1,\n",
    "        epochs=params[\"epochs\"],\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return history\n",
    "##########################################################################################\n",
    "\n",
    "def run_predictions( model, encoded_df, combined_df, feature_stats, predict_date=None):\n",
    "    \"\"\"\n",
    "    Build one prediction row per item using:\n",
    "    - latest encoded feature state (encoded_df)\n",
    "    - raw timeline + names (combined_df)\n",
    "    - recomputed calendar features at predict_date\n",
    "    \"\"\"\n",
    "\n",
    "    if predict_date is None:\n",
    "        predict_date = pd.Timestamp.today().normalize()\n",
    "    else:\n",
    "        predict_date = pd.to_datetime(predict_date).normalize()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Discover numeric features (single source: encoded_df)\n",
    "    # --------------------------------------------------------\n",
    "    numeric_cols = [\n",
    "        c for c in encoded_df.columns\n",
    "        if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "    ]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Lookups from combined_df (single source of truth)\n",
    "    # --------------------------------------------------------\n",
    "    last_date_by_item = (\n",
    "        combined_df\n",
    "        .sort_values(\"date\")\n",
    "        .groupby(\"itemId\")[\"date\"]\n",
    "        .last()\n",
    "    )\n",
    "\n",
    "    item_lookup = (\n",
    "        combined_df[[\"itemId\", \"item\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"itemId\")[\"item\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for itemId, hist in encoded_df.groupby(\"itemId\"):\n",
    "        last = hist.iloc[-1]\n",
    "        last_date = pd.to_datetime(last_date_by_item.loc[itemId]).normalize()\n",
    "\n",
    "        row = {\n",
    "            \"itemId\": itemId,\n",
    "            \"item\": item_lookup.get(itemId, \"UNKNOWN\"),\n",
    "            \"itemIdx\": int(last[\"itemIdx\"])\n",
    "        }\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Copy model-stable numeric features (already normalized)\n",
    "        # ----------------------------------------------------\n",
    "        for col in numeric_cols:\n",
    "            row[col] = last[col]\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Recompute DATE-SENSITIVE features\n",
    "        # ----------------------------------------------------\n",
    "        raw_updates = {\n",
    "            \"daysSinceLastPurchase\": (predict_date - last_date).days,\n",
    "            \"daysUntilNextHoliday\": daysUntilNextHoliday(predict_date),\n",
    "            \"daysSinceLastHoliday\": daysSinceLastHoliday(predict_date),\n",
    "            \"holidayProximityIndex\": holidayProximityIndex(predict_date),\n",
    "            \"daysUntilSchoolStart\": daysUntilSchoolStart(predict_date),\n",
    "            \"daysUntilSchoolEnd\": daysUntilSchoolEnd(predict_date),\n",
    "            \"schoolSeasonIndex\": schoolSeasonIndex(predict_date),\n",
    "            \"year\": predict_date.year,\n",
    "            \"day\": predict_date.day,\n",
    "            \"quarter\": predict_date.quarter\n",
    "        }\n",
    "\n",
    "  \n",
    "        # ----------------------------------------------------\n",
    "        # Normalize recomputed features\n",
    "        # ----------------------------------------------------\n",
    "        for raw, val in raw_updates.items():\n",
    "            norm_col = raw + \"_norm\"\n",
    "            if norm_col in numeric_cols and raw in feature_stats:\n",
    "                stats = feature_stats[raw]\n",
    "                row[norm_col] = (val - stats[\"mean\"]) / stats[\"std\"]\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    pred_df = pd.DataFrame(rows)\n",
    "\n",
    "    Xn = pred_df[numeric_cols].to_numpy(np.float32)\n",
    "    Xi = pred_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "\n",
    "    scores = model.predict([Xn, Xi], verbose=0).ravel()\n",
    "\n",
    "    pred_df[\"due_intensity\"] = scores\n",
    "\n",
    "    return (\n",
    "        pred_df[[\"itemId\", \"item\", \"due_intensity\"]]\n",
    "        .sort_values(\"due_intensity\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "###############################################################################\n",
    "\n",
    "def BuildParamSets( baseline_params, property_name, start, step, stop):\n",
    "    \"\"\"\n",
    "    Creates multiple fully independent parameter dictionaries by varying one property.\n",
    "    Each iteration produces a brand-new baseline object.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    results = []\n",
    "\n",
    "    value = start\n",
    "    while value <= stop:\n",
    "        params_copy = copy.deepcopy(baseline_params)\n",
    "        params_copy[property_name] = value\n",
    "        results.append(params_copy)\n",
    "        value += step\n",
    "\n",
    "    return results\n",
    "###############################################################################\n",
    "\n",
    "def runExp(combined_df, encoded_df, buildParams, trainParams, baseDir):\n",
    "    #\n",
    "    # item index\n",
    "    item_ids = sorted(encoded_df[\"itemId\"].unique())\n",
    "    item_id_to_idx = {iid: i for i, iid in enumerate(item_ids)}\n",
    "    encoded_df[\"itemIdx\"] = encoded_df[\"itemId\"].map(item_id_to_idx).astype(\"int32\")\n",
    "    num_items = len(item_ids)\n",
    "    #\n",
    "    numeric_cols = [\n",
    "        c for c in encoded_df.columns\n",
    "        if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "    ]\n",
    "    num_numeric_features = len(numeric_cols)\n",
    "    #\n",
    "    feature_stats = {}\n",
    "    RECOMPUTED = [\n",
    "        \"daysSinceLastPurchase\",\n",
    "        \"daysUntilNextHoliday\",\n",
    "        \"daysSinceLastHoliday\",\n",
    "        \"holidayProximityIndex\",\n",
    "        \"daysUntilSchoolStart\",\n",
    "        \"daysUntilSchoolEnd\",\n",
    "        \"schoolSeasonIndex\",\n",
    "        \"year\", \"day\", \"quarter\"\n",
    "    ]\n",
    "\n",
    "    for raw in RECOMPUTED:\n",
    "        col = raw + \"_norm\"\n",
    "        if col in encoded_df.columns:\n",
    "            std = encoded_df[col].std()\n",
    "            feature_stats[raw] = {\n",
    "                \"mean\": encoded_df[col].mean(),\n",
    "                \"std\": std if std != 0 else 1.0\n",
    "            }\n",
    "\n",
    "    #\n",
    "    model = build_and_compile_model(num_numeric_features, num_items, buildParams)\n",
    "    #\n",
    "    history = train_model(model, encoded_df, trainParams)\n",
    "    #\n",
    "    predictions = run_predictions(model, encoded_df, combined_df, feature_stats)\n",
    "    # \n",
    "    dataframes = {\n",
    "        \"predictions\": predictions,\n",
    "        \"encoded_features\": encoded_df,\n",
    "        \"combined_df\": combined_df\n",
    "    }\n",
    "    save_experiment(model, history, dataframes, buildParams, trainParams, numeric_cols, item_id_to_idx, base_dir= baseDir)\n",
    "################################################################################################################################\n",
    "\n",
    "import multiprocessing as mp\n",
    "def run_param_sets_multiprocess(buildParamsSets, trainParams, max_parallel, feature_stats, combined_df,encoded_df, baseDir ):\n",
    "    #\n",
    "    processes = []\n",
    "\n",
    "    for buildParams in buildParamsSets:\n",
    "        p = mp.Process(\n",
    "            target=runExp,\n",
    "            args=(feature_stats, combined_df, encoded_df, buildParams, trainParams, baseDir)\n",
    "        )\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "        # limit concurrency\n",
    "        if len(processes) >= max_parallel:\n",
    "            for proc in processes:\n",
    "                proc.join()\n",
    "            processes = []\n",
    "\n",
    "    # wait for remaining\n",
    "    for proc in processes:\n",
    "        proc.join()\n",
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ca625-2d02-4b7c-b8c6-dd54e06c2324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature_stats = {}\n",
    "# RECOMPUTED = [\n",
    "#     \"daysSinceLastPurchase\",\n",
    "#     \"daysUntilNextHoliday\",\n",
    "#     \"daysSinceLastHoliday\",\n",
    "#     \"holidayProximityIndex\",\n",
    "#     \"daysUntilSchoolStart\",\n",
    "#     \"daysUntilSchoolEnd\",\n",
    "#     \"schoolSeasonIndex\",\n",
    "#     \"year\", \"day\", \"quarter\"\n",
    "# ]\n",
    "\n",
    "# for raw in RECOMPUTED:\n",
    "#     col = raw + \"_norm\"\n",
    "#     if col in encoded_df.columns:\n",
    "#         std = encoded_df[col].std()\n",
    "#         feature_stats[raw] = {\n",
    "#             \"mean\": encoded_df[col].mean(),\n",
    "#             \"std\": std if std != 0 else 1.0\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9098d1-6323-4af8-8647-aa06ba56f79b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainParams = {\n",
    "    \"loss\": \"mse\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"metrics\": [\"mae\"],\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 32,\n",
    "    \"validation_split\": 0.1\n",
    "}\n",
    "\n",
    "buildParamsHiddenLayerBase = {\n",
    "    \"embedding_dim\": 32,\n",
    "    \"hiddenLayers\": [1],\n",
    "    \"output_activation\": \"sigmoid\"\n",
    "}\n",
    "\n",
    "paramSets = HiddenLayerParamSetBuilder.BuildHiddenLayerDepthSets(buildParamsHiddenLayerBase, 64,1,20)\n",
    "\n",
    "\n",
    "for eachBuildParams in paramSets:\n",
    "    #\n",
    "    print(f\"Loop: {eachBuildParams['hiddenLayers']}\")\n",
    "    runExp(combined_df, encoded_df, eachBuildParams, trainParams, \"exp/keras/layer_depth\")\n",
    "\n",
    "\n",
    "paramSets = HiddenLayerParamSetBuilder.BuildHiddenLayerDepthSets(buildParamsHiddenLayerBase, 20,2,20)\n",
    "for eachBuildParams in paramSets:\n",
    "    #\n",
    "    print(f\"Loop: {eachBuildParams['hiddenLayers']}\")\n",
    "    runExp(combined_df, encoded_df, eachBuildParams, trainParams, \"exp/keras/layer_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf905ec1-d0c8-4467-ae6f-ae1382e183a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buildParams_embeddingsTest = {\n",
    "#     \"embedding_dim\": 1,\n",
    "#     \"hiddenLayers\": [512],\n",
    "#     \"output_activation\": \"sigmoid\"\n",
    "# }\n",
    "\n",
    "# # buildParams_embeddingsTest_relu = {\n",
    "# #     \"embedding_dim\": 1,\n",
    "# #     \"hiddenLayers\": [1024],\n",
    "# #     \"output_activation\": \"relu\"\n",
    "# # }\n",
    "\n",
    "\n",
    "# trainParams = {\n",
    "#     \"loss\": \"mse\",\n",
    "#     \"optimizer\": \"adam\",\n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     \"metrics\": [\"mae\"],\n",
    "#     \"epochs\": 40,\n",
    "#     \"batch_size\": 32,\n",
    "#     \"validation_split\": 0.1\n",
    "# }\n",
    "\n",
    "# # build sets\n",
    "# paramSets = BuildParamSets(buildParams_embeddingsTest, \"embedding_dim\", 33, 2, 64)\n",
    "# # run\n",
    "# run_param_sets_multiprocess(paramSets, trainParams, 4, feature_stats, combined_df,encoded_df, \"exp_mp\")\n",
    "# #paramSets_embeddingeTest_relu = BuildParamSets(buildParams_embeddingsTest_relu, \"embedding_dim\", 1, 2, 32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d87fa-809d-464e-86e2-dd1356bd3569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # ENSURE itemIdx\n",
    "# # ------------------------------------------------------------\n",
    "# item_ids = sorted(encoded_df[\"itemId\"].unique())\n",
    "# item_id_to_idx = {iid: i for i, iid in enumerate(item_ids)}\n",
    "# encoded_df[\"itemIdx\"] = encoded_df[\"itemId\"].map(item_id_to_idx).astype(\"int32\")\n",
    "# NUM_ITEMS = len(item_ids)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # FEATURES / TARGET\n",
    "# # ------------------------------------------------------------\n",
    "# numeric_cols = [\n",
    "#     c for c in encoded_df.columns\n",
    "#     if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "# ]\n",
    "\n",
    "# Xn = encoded_df[numeric_cols].to_numpy(np.float32)\n",
    "# Xi = encoded_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "# y  = encoded_df[\"due_score\"].to_numpy(np.float32)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # SPLIT\n",
    "# # ------------------------------------------------------------\n",
    "# Xn_tr, Xn_te, Xi_tr, Xi_te, y_tr, y_te = train_test_split(\n",
    "#     Xn, Xi, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # MODEL\n",
    "# # ------------------------------------------------------------\n",
    "# num_in = layers.Input(shape=(Xn_tr.shape[1],))\n",
    "# itm_in = layers.Input(shape=(), dtype=\"int32\")\n",
    "\n",
    "# emb = layers.Embedding(NUM_ITEMS, 64)(itm_in)\n",
    "# emb = layers.Flatten()(emb)\n",
    "\n",
    "# x = layers.Concatenate()([num_in, emb])\n",
    "# x = layers.Dense(4096, activation=\"relu\")(x)\n",
    "# #x = layers.Dense(2048, activation=\"relu\")(x)\n",
    "# out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# model = models.Model([num_in, itm_in], out)\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# history = model.fit(\n",
    "#     [Xn_tr, Xi_tr],\n",
    "#     y_tr,\n",
    "#     validation_split=0.1,\n",
    "#     epochs=10,\n",
    "#     batch_size=32,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # FEATURE STATS (ONLY recomputed features)\n",
    "# # (NOTE: stats are for *_norm columns since inference writes *_norm)\n",
    "# # ------------------------------------------------------------\n",
    "# feature_stats = {}\n",
    "# RECOMPUTED = [\n",
    "#     \"daysSinceLastPurchase\",\n",
    "#     \"daysUntilNextHoliday\",\n",
    "#     \"daysSinceLastHoliday\",\n",
    "#     \"holidayProximityIndex\",\n",
    "#     \"daysUntilSchoolStart\",\n",
    "#     \"daysUntilSchoolEnd\",\n",
    "#     \"schoolSeasonIndex\",\n",
    "#     \"year\", \"day\", \"quarter\",\n",
    "#     \"daysUntilBirthday_steve\", \"daysSinceBirthday_steve\",\n",
    "#     \"daysUntilBirthday_maggie\", \"daysSinceBirthday_maggie\",\n",
    "#     \"daysUntilBirthday_mil\", \"daysSinceBirthday_mil\",\n",
    "#     \"daysUntilBirthday_angie\", \"daysSinceBirthday_angie\",\n",
    "# ]\n",
    "\n",
    "# for raw in RECOMPUTED:\n",
    "#     col = raw + \"_norm\"\n",
    "#     if col in encoded_df.columns:\n",
    "#         std = encoded_df[col].std()\n",
    "#         feature_stats[raw] = {\n",
    "#             \"mean\": encoded_df[col].mean(),\n",
    "#             \"std\": std if std != 0 else 1.0\n",
    "#         }\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # BIRTHDAYS\n",
    "# # ------------------------------------------------------------\n",
    "# BIRTHDAYS = { \"steve\":  \"03-05-1980\", \"maggie\": \"03-03-2016\",\"mil\": \"01-27-1962\", \"angie\":  \"08-11-1981\"}\n",
    "# birthdays = {k: pd.to_datetime(v) for k, v in BIRTHDAYS.items()}\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # PREDICT (UPDATED CALL)\n",
    "# # ------------------------------------------------------------\n",
    "# predictions = run_predictions(model=model, encoded_df=encoded_df, combined_df=combined_df, feature_stats=feature_stats, birthdays=birthdays, predict_date=None)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # SAVE\n",
    "# # ------------------------------------------------------------\n",
    "# save_experiment( model=model, history=history, predictions=predictions, params={}, numeric_cols=numeric_cols, item_id_to_idx=item_id_to_idx)\n",
    "\n",
    "# predictions.head(50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Grocery ML",
   "language": "python",
   "name": "grocery-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
