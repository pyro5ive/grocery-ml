{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192abc4e-1a90-416f-b168-9093c1493c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from winn_dixie_recpt_parser import WinnDixieRecptParser \n",
    "\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e514d2-7c8e-45ea-a1bd-88d5d4e6ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grouped(grouped, rows=10):\n",
    "    # collect only the daysSinceLastPurchase_* columns\n",
    "    feature_cols = [c for c in grouped.columns if c.startswith(\"daysSinceLastPurchase_\")]\n",
    "\n",
    "    for i in range(min(rows, len(grouped))):\n",
    "        print(\"Row:\", i)\n",
    "        print(\"Date:\", grouped.iloc[i][\"date\"])\n",
    "        print(\"Time:\", grouped.iloc[i][\"time\"])\n",
    "        print(\"Items:\", grouped.iloc[i][\"item\"])\n",
    "        print(\"------ daysSinceLastPurchase ------\")\n",
    "\n",
    "        for col in feature_cols:\n",
    "            print(f\"{col}: {grouped.iloc[i][col]}\")\n",
    "\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "def show_encoded(encoded_df, rows=10):\n",
    "    # Identify columns\n",
    "    days_cols = [c for c in encoded_df.columns if c.startswith(\"daysSinceLastPurchase_\")]\n",
    "    weather_cols = [c for c in encoded_df.columns if c.endswith(\"_5day_avg\")]\n",
    "    item_cols = [\n",
    "        c for c in encoded_df.columns \n",
    "        if c not in days_cols \n",
    "        and c not in weather_cols\n",
    "        and c not in [\"date\", \"time\"]\n",
    "    ]\n",
    "\n",
    "    for i in range(min(rows, len(encoded_df))):\n",
    "        print(\"Row:\", i)\n",
    "        print(\"Date:\", encoded_df.iloc[i][\"date\"])\n",
    "        print(\"Time:\", encoded_df.iloc[i][\"time\"])\n",
    "\n",
    "        # Show the items purchased (reverse one-hot)\n",
    "        purchased_items = []\n",
    "        row_vals = encoded_df.iloc[i]\n",
    "\n",
    "        for item in item_cols:\n",
    "            if row_vals[item] == 1:\n",
    "                purchased_items.append(item)\n",
    "\n",
    "        print(\"Items:\", purchased_items)\n",
    "\n",
    "        print(\"------ daysSinceLastPurchase ------\")\n",
    "        for col in days_cols:\n",
    "            print(f\"{col}: {encoded_df.iloc[i][col]}\")\n",
    "\n",
    "        print(\"------ weather (rolling windows) ------\")\n",
    "        for col in weather_cols:\n",
    "            print(f\"{col}: {encoded_df.iloc[i][col]}\")\n",
    "\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "def remove_duplicate_receipt_files(df):\n",
    "    \"\"\"\n",
    "    Remove whole source files that contain an identical receipt\n",
    "    to another file with the same date+time.\n",
    "    Minimal console output. Resets index at end.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"__signature\"] = (\n",
    "        df[\"date\"].astype(str) + \"|\" +\n",
    "        df[\"time\"].astype(str) + \"|\" +\n",
    "        df[\"item\"].astype(str) + \"|\" +\n",
    "        df[\"qty\"].astype(str) + \"|\" +\n",
    "        df[\"youPay\"].astype(str) + \"|\" +\n",
    "        df[\"reg\"].astype(str) + \"|\" +\n",
    "        df[\"reportedItemsSold\"].astype(str) + \"|\" +\n",
    "        df[\"cashier\"].astype(str) + \"|\" +\n",
    "        df[\"manager\"].astype(str)\n",
    "    )\n",
    "\n",
    "    keep_sources = set()\n",
    "\n",
    "    for (dt_date, dt_time), group in df.groupby([\"date\", \"time\"]):\n",
    "\n",
    "        # Build signature per source\n",
    "        source_signatures = {}\n",
    "        for source, rows in group.groupby(\"source\"):\n",
    "            sig = tuple(sorted(rows[\"__signature\"].tolist()))\n",
    "            source_signatures[source] = sig\n",
    "\n",
    "        # signature → list of sources\n",
    "        signature_groups = {}\n",
    "        for src, sig in source_signatures.items():\n",
    "            signature_groups.setdefault(sig, []).append(src)\n",
    "\n",
    "        # Handle duplicates\n",
    "        for sig, sources in signature_groups.items():\n",
    "            if len(sources) == 1:\n",
    "                keep_sources.add(sources[0])\n",
    "                continue\n",
    "\n",
    "            sorted_sources = sorted(sources)\n",
    "            kept = sorted_sources[0]\n",
    "            removed = sorted_sources[1:]\n",
    "\n",
    "            # Minimal output\n",
    "            print(f\"DUP: {dt_date} {dt_time} → keep {kept} ← drop {', '.join(removed)}\")\n",
    "\n",
    "            keep_sources.add(kept)\n",
    "\n",
    "    # Filter and clean\n",
    "    result = df[df[\"source\"].isin(keep_sources)].copy()\n",
    "    result.drop(columns=[\"__signature\"], inplace=True)\n",
    "\n",
    "    # ✔ Reset index here\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def purchases_per_window(df, item, window_days):\n",
    "    \"\"\"\n",
    "    Returns how many times `item` is purchased per `window_days`.\n",
    "    \"\"\"\n",
    "    item_df = df[df[\"item\"] == item]\n",
    "\n",
    "    if len(item_df) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    total_days = (df[\"date\"].max() - df[\"date\"].min()).days + 1\n",
    "    total_purchases = len(item_df)\n",
    "\n",
    "    if total_days == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (total_purchases / total_days) * float(window_days)\n",
    "    #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47046089-a2d9-4136-95bb-3888c94aab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNearHoliday(d, xDays):\n",
    "    d = pd.to_datetime(d)\n",
    "    holidays = USFederalHolidayCalendar().holidays()\n",
    "    diffs = (holidays - d).days\n",
    "    return (diffs >= 1).any() and (diffs.min() <= xDays)\n",
    "#########################################################\n",
    "\n",
    "def isOnHoliday(d):\n",
    "    d = pd.to_datetime(d)\n",
    "    holidays = USFederalHolidayCalendar().holidays()\n",
    "    return d.normalize() in holidays\n",
    "#########################################################\n",
    "\n",
    "def isPostHoliday(d, xDays):\n",
    "    d = pd.to_datetime(d)\n",
    "    holidays = USFederalHolidayCalendar().holidays()\n",
    "    diffs = (d - holidays).days\n",
    "    return (diffs >= 1).any() and (diffs.min() <= xDays)\n",
    "#########################################################\n",
    "\n",
    "def isBeforeBirthday(d, xDays, bday):\n",
    "    d = pd.to_datetime(d)\n",
    "    bday = pd.to_datetime(bday)\n",
    "\n",
    "    this_year_bday = pd.Timestamp(year=d.year, month=bday.month, day=bday.day)\n",
    "\n",
    "    if d >= this_year_bday:\n",
    "        return False\n",
    "\n",
    "    days_before = (this_year_bday - d).days\n",
    "    return 1 <= days_before <= xDays\n",
    "#########################################################\n",
    "\n",
    "def isOnBirthdate(d, bday):\n",
    "    d = pd.to_datetime(d)\n",
    "    bday = pd.to_datetime(bday)\n",
    "    return (d.month == bday.month) and (d.day == bday.day)\n",
    "#########################################################\n",
    "\n",
    "def weatherDeviation(actualTemp, avgTemp):\n",
    "    return actualTemp - avgTemp\n",
    "    # negative = colder than avg\n",
    "    # positive = hotter than avg\n",
    "#########################################################\n",
    "\n",
    "def isColderThanAvg(actualTemp, avgTemp, threshold):\n",
    "    return (avgTemp - actualTemp) >= threshold\n",
    "#########################################################\n",
    "\n",
    "def isHotterThanAvg(actualTemp, avgTemp, threshold):\n",
    "    return (actualTemp - avgTemp) >= threshold\n",
    "#########################################################\n",
    "\n",
    "def isSchoolSeason(d):\n",
    "    d = pd.to_datetime(d)\n",
    "\n",
    "    start = pd.Timestamp(year=d.year, month=8, day=15)   # Aug 15\n",
    "    end   = pd.Timestamp(year=d.year, month=5, day=31)   # May 31\n",
    "\n",
    "    # School season spans Aug 15 → Dec 31 AND Jan 1 → May 31\n",
    "    if start <= d or d <= end:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241f8fa-6c93-49b0-848a-a784845f01fa",
   "metadata": {},
   "source": [
    "# DF.TRIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc3103-8878-4ae3-bb2f-f13bdf094e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = []\n",
    "\n",
    "recptParser  = WinnDixieRecptParser();\n",
    "\n",
    "for p in Path(\"winndixie rcpts/StevePhone2/pdf/text\").glob(\"*.txt\"):\n",
    "    result = recptParser.parse(p.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "    for r in result[\"items\"]:\n",
    "        rows.append({\n",
    "            \"source\": p.name,\n",
    "            \"date\": result[\"date\"],\n",
    "            \"time\": result[\"time\"],\n",
    "            \"manager\": result[\"manager\"],\n",
    "            \"cashier\": result[\"cashier\"],\n",
    "            \"item\": r[\"item\"],\n",
    "            \"qty\": r[\"qty\"],\n",
    "            \"reg\": r[\"reg\"],\n",
    "            \"youPay\": r[\"youPay\"],\n",
    "            \"reportedItemsSold\": result[\"reported\"],\n",
    "            #\"rowsMatchReported\": result[\"validation\"][\"rowsMatchReported\"],\n",
    "            \"qtyMatchReported\": result[\"validation\"][\"qtyMatchReported\"],\n",
    "        })\n",
    "\n",
    "df_trips = pd.DataFrame(rows)\n",
    "\n",
    "df_trips[\"date\"] = pd.to_datetime(df_trips[\"date\"])\n",
    "df_trips[\"time\"] = df_trips[\"time\"].astype(str)\n",
    "df_trips = remove_duplicate_receipt_files(df_trips)\n",
    "df_trips = df_trips.sort_values(by=[\"date\", \"time\"]).reset_index(drop=True)\n",
    "df_trips.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514bad1-e2c8-481c-8cfe-d33047ded93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Frequency\n",
    "\n",
    "unique_items = df_trips[\"item\"].unique()\n",
    "\n",
    "for item in unique_items:\n",
    "\n",
    "    # Compute per-window frequencies for THIS item\n",
    "    freq_14  = purchases_per_window(df_trips, item, 14)\n",
    "    freq_30  = purchases_per_window(df_trips, item, 30)\n",
    "    freq_90  = purchases_per_window(df_trips, item, 90)\n",
    "    freq_365 = purchases_per_window(df_trips, item, 365)\n",
    "\n",
    "    # Assign them to item-specific columns\n",
    "    df_trips.loc[df_trips[\"item\"] == item, f\"freq_14_days_{item}\"]  = freq_14\n",
    "    df_trips.loc[df_trips[\"item\"] == item, f\"freq_30_days_{item}\"]  = freq_30\n",
    "    df_trips.loc[df_trips[\"item\"] == item, f\"freq_90_days_{item}\"]  = freq_90\n",
    "    df_trips.loc[df_trips[\"item\"] == item, f\"freq_365_days_{item}\"] = freq_365\n",
    "\n",
    "    days = compute_days_since(df_trips, item)\n",
    "    df_trips.loc[df_trips[\"item\"] == item, f\"daysSinceLastPurchase_{item}\"] = days\n",
    "\n",
    "df_trips.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b9513-cd57-41db-8c9f-47dbec544c46",
   "metadata": {},
   "source": [
    "### GROUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161bd828-43a1-4b32-b137-5c48b465b7e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Identify all item-level columns (freq + daysSinceLastPurchase)\n",
    "freq_cols = [c for c in df_trips.columns if c.startswith(\"freq_\")]\n",
    "days_cols = [c for c in df_trips.columns if c.startswith(\"daysSinceLastPurchase_\")]\n",
    "\n",
    "# Combine them\n",
    "item_level_cols = freq_cols + days_cols\n",
    "\n",
    "# 2. Build grouped trip-level table\n",
    "grouped = df_trips.groupby([\"date\", \"time\"], as_index=False).agg(\n",
    "    {\n",
    "        \"item\": list,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Aggregate item-level values across all rows of each trip\n",
    "#    We use max() because each item column only contains a real value\n",
    "#    for the matching row, and 0 for the others.\n",
    "item_level_agg = (\n",
    "    df_trips.groupby([\"date\", \"time\"])[item_level_cols]\n",
    "    .max()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Add all item-level columns to grouped\n",
    "grouped[item_level_cols] = item_level_agg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193c3ef-9c77-44f7-9765-6d482d5f8da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------  daysSinceLastTrip -------------------\n",
    "days_since_trip = []\n",
    "prev_date = None\n",
    "\n",
    "for idx in range(len(grouped)):\n",
    "    current_date = grouped.iloc[idx][\"date\"]\n",
    "    if prev_date is None:\n",
    "        days_since_trip.append(0)\n",
    "    else:\n",
    "        days_since_trip.append((current_date - prev_date).days)\n",
    "    prev_date = current_date\n",
    "\n",
    "grouped[\"daysSinceLastTrip\"] = days_since_trip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66359386-f457-4ced-bf82-69e99a4b7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped[\"daysUntilNextHoliday\"] = grouped[\"date\"].apply(daysUntilNextHoliday)\n",
    "grouped[\"daysSinceLastHoliday\"] = grouped[\"date\"].apply(daysSinceLastHoliday)\n",
    "grouped[\"holidayProximityIndex\"] = grouped[\"date\"].apply(holidayProximityIndex)\n",
    "\n",
    "#grouped[\"tempDeviation\"] = df.apply(lambda row: tempDeviation(row[\"actualTemp\"], row[\"avgTemp\"]), axis=1)\n",
    "#grouped[\"humidityDeviation\"] = df.apply(lambda row: humidityDeviation(row[\"actualHumidity\"], row[\"avgHumidity\"]), axis=1)\n",
    "#grouped[\"precipDeviation\"] = df.apply(lambda row: precipDeviation(row[\"actualPrecip\"], row[\"avgPrecip\"]),axis=1)\n",
    "\n",
    "angieBday = \"08-11-1981\"\n",
    "steveBday = \"03-05-1980\"\n",
    "maggieBday = \"03-03-2016\"\n",
    "\n",
    "grouped[\"daysUntilBirthday_steve\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, steveBday))\n",
    "grouped[\"daysUntilBirthday_angie\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, angieBday))\n",
    "grouped[\"daysUntilBirthday_maggie\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, maggieBday))\n",
    "\n",
    "grouped[\"daysSinceBirthday_steve\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, steveBday))\n",
    "grouped[\"daysSinceBirthday_angie\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, angieBday))\n",
    "grouped[\"daysSinceBirthday_maggie\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, maggieBday))\n",
    "\n",
    "grouped[\"daysUntilSchoolStart\"] = grouped[\"date\"].apply(daysUntilSchoolStart)\n",
    "grouped[\"daysUntilSchoolEnd\"]   = grouped[\"date\"].apply(daysUntilSchoolEnd)\n",
    "grouped[\"schoolSeasonIndex\"]    = grouped[\"date\"].apply(schoolSeasonIndex)\n",
    "\n",
    "\n",
    "grouped.to_csv(\"grouped.csv\", index=False)\n",
    "\n",
    "holidayCols = [\"daysUntilNextHoliday\", \"daysSinceLastHoliday\", \"holidayProximityIndex\"]\n",
    "bdayCols = [c for c in df.columns if c.startswith(\"daysUntilBirthday_\") or c.startswith(\"daysSinceBirthday_\")]\n",
    "schoolCols = [c for c in df.columns if \"School\" in c]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb5f82-f270-4a38-9414-341412e3c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherCols=[\"datetime\", \"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"]\n",
    "df_weather = pd.read_csv(\"datasets/VisualCrossing-70062 2000-01-01 to 2025-12-14.csv\", usecols=weatherCols);\n",
    "df_weather[\"datetime\"] = pd.to_datetime(df_weather[\"datetime\"])\n",
    "df_weather = df_weather.set_index(\"datetime\")\n",
    "df_weather = df_weather.sort_index()\n",
    "df_weather.info()\n",
    "df_weather[\"temp_5day_avg\"] = df_weather[\"temp\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"feelsLike_5day_avg\"] = df_weather[\"feelslike\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"dew_5day_avg\"] = df_weather[\"dew\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"humidity_5day_avg\"] = df_weather[\"humidity\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"precip_5day_avg\"] = df_weather[\"precip\"].rolling(5, min_periods=1).mean()\n",
    "df_weather = df_weather.drop(columns=[\"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"])\n",
    "\n",
    "\n",
    "df_merged = df_trips.merge(df_weather, left_on=\"date\",  right_index=True, how=\"left\")\n",
    "df_merged.to_csv(\"df_merged.csv\", index=False, encoding=\"utf-8\")\n",
    "df_merged.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5e0646-9d64-43db-82d2-3481ff96cf6c",
   "metadata": {},
   "source": [
    "## ENCODED_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c421a431-c494-45a2-869c-62f3cb3296a2",
   "metadata": {},
   "source": [
    "### One hot items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514ec48-2b1f-4dd3-bba9-39d8e3eca0eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. BUILD ONE-HOT ITEMS + MERGE WITH GROUPED DATA\n",
    "# ============================================================\n",
    "\n",
    "num_items = len(unique_items)\n",
    "vectors = []\n",
    "\n",
    "for item_list in grouped[\"item\"]:\n",
    "    vector = np.zeros(num_items, dtype=np.int32)\n",
    "    for name in item_list:\n",
    "        index = item_to_index[name]\n",
    "        vector[index] = 1\n",
    "    vectors.append(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdaa6a-c44f-4d32-97d2-7acdd4922864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# columns from grouped\n",
    "days_cols = [c for c in grouped.columns if c.startswith(\"daysSinceLastPurchase_\")]\n",
    "weather_cols = [c for c in grouped.columns if \"_5day\" in c]\n",
    "freq_cols = [c for c in df_trips.columns if c.startswith(\"freq_\")]\n",
    "\n",
    "encoded_items_df = pd.DataFrame(vectors, columns=unique_items)\n",
    "\n",
    "encoded_df = pd.concat(\n",
    "    [\n",
    "        grouped[[\"date\", \"time\", \"daysSinceLastTrip\"]],\n",
    "        grouped[days_cols],\n",
    "        grouped[weather_cols],\n",
    "        grouped[freq_cols],\n",
    "        \n",
    "        encoded_items_df\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "def normalizeAndDropCols(df, cols):\n",
    "    for col in cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "\n",
    "        if std == 0:\n",
    "            std = 1.0\n",
    "\n",
    "        df[col + \"_norm\"] = (df[col] - mean) / std\n",
    "\n",
    "    df = df.drop(columns=cols)\n",
    "    return df\n",
    "    #####################################################\n",
    "\n",
    "\n",
    "cols = [c for c in encoded_df.columns if c.startswith(\"daysSinceLastPurchase_\")]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, cols);\n",
    "##\n",
    "cols = [c for c in encoded_df.columns if c.startswith(\"freq_\")]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, cols);\n",
    "##\n",
    "cols = [c for c in encoded_df.columns if c.endswith(\"_5day_avg\")]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, cols);\n",
    "##\n",
    "\n",
    "\n",
    "show_encoded(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f2aa26-3fa8-4d01-842e-ec8d4c6afd51",
   "metadata": {},
   "source": [
    "### BUILD DATETIME FEATURES + Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b8c92-d433-4873-9856-c3f7cb6e5f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3. BUILD DATETIME FEATURES\n",
    "# ============================================================\n",
    "\n",
    "encoded_df[\"dateTime\"] = pd.to_datetime(\n",
    "    encoded_df[\"date\"].astype(str) + \" \" + encoded_df[\"time\"].astype(str),\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "encoded_df = encoded_df.drop(columns=[\"date\", \"time\"])\n",
    "\n",
    "dt = encoded_df[\"dateTime\"]\n",
    "\n",
    "encoded_df[\"year\"]    = dt.dt.year\n",
    "encoded_df[\"month\"]   = dt.dt.month\n",
    "encoded_df[\"day\"]     = dt.dt.day\n",
    "encoded_df[\"hour\"]    = dt.dt.hour\n",
    "encoded_df[\"minute\"]  = dt.dt.minute\n",
    "encoded_df[\"dow\"]     = dt.dt.dayofweek\n",
    "encoded_df[\"doy\"]     = dt.dt.dayofyear\n",
    "encoded_df[\"quarter\"] = dt.dt.quarter\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. SINE COSINE on CYCLICAL FEATURES\n",
    "# ============================================================\n",
    "\n",
    "encoded_df[\"hour_sin\"] = np.sin(2 * np.pi * encoded_df[\"hour\"] / 24.0)\n",
    "encoded_df[\"hour_cos\"] = np.cos(2 * np.pi * encoded_df[\"hour\"] / 24.0)\n",
    "\n",
    "encoded_df[\"minute_sin\"] = np.sin(2 * np.pi * encoded_df[\"minute\"] / 60.0)\n",
    "encoded_df[\"minute_cos\"] = np.cos(2 * np.pi * encoded_df[\"minute\"] / 60.0)\n",
    "\n",
    "encoded_df[\"dow_sin\"] =   np.sin(2 * np.pi * encoded_df[\"dow\"] / 7.0)\n",
    "encoded_df[\"dow_cos\"] =   np.cos(2 * np.pi * encoded_df[\"dow\"] / 7.0)\n",
    "\n",
    "encoded_df[\"month_sin\"] = np.sin(2 * np.pi * encoded_df[\"month\"] / 12.0)\n",
    "encoded_df[\"month_cos\"] = np.cos(2 * np.pi * encoded_df[\"month\"] / 12.0)\n",
    "\n",
    "encoded_df[\"doy_sin\"] =   np.sin(2 * np.pi * encoded_df[\"doy\"] / 365.0)\n",
    "encoded_df[\"doy_cos\"] =   np.cos(2 * np.pi * encoded_df[\"doy\"] / 365.0)\n",
    "\n",
    "encoded_df = encoded_df.drop(columns=[\"month\", \"hour\", \"minute\", \"dow\", \"doy\"])\n",
    "\n",
    "# ============================================================\n",
    "# 5. SCALE REMAINING NON-CYCLIC TIME FEATURES\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "cols = [c for c in encoded_df.columns if c.startswith(\"year\")]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, cols);\n",
    "\n",
    "cols = [c for c in encoded_df.columns if c.startswith(\"day\")]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, cols);\n",
    "\n",
    "cols = [c for c in encoded_df.columns if c.startswith(\"quarter\")]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, cols);\n",
    "\n",
    "cols = [c for c in encoded_df.columns if c.startswith(\"daysSinceLastTrip\")]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, cols);\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. DROP DATETIME + SAVE\n",
    "# ============================================================\n",
    "\n",
    "encoded_df = encoded_df.drop(columns=[\"dateTime\"])\n",
    "encoded_df.to_csv(\"encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7672fca-abf7-440c-a26e-6183edce256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME FEATURES\n",
    "time_feature_cols = [\n",
    "    \"hour_sin\", \"hour_cos\",\n",
    "    \"minute_sin\", \"minute_cos\",\n",
    "    \"dow_sin\", \"dow_cos\",\n",
    "    \"month_sin\", \"month_cos\",\n",
    "    \"doy_sin\", \"doy_cos\",\n",
    "    \"year_norm\", \"day_norm\", \"quarter_norm\",\n",
    "    \"daysSinceLastTrip_norm\"\n",
    "]\n",
    "\n",
    "weather_norm_cols = [c for c in encoded_df.columns if c.endswith(\"_5day_avg_norm\")]\n",
    "freq_norm_cols = [ c for c in encoded_df.columns if c.startswith(\"freq_\") and not c.endswith(\"_norm\")]\n",
    "purchase_norm_cols = [c for c in encoded_df.columns if c.startswith(\"daysSinceLastPurchase_\") and c.endswith(\"_norm\")]\n",
    "\n",
    "# FINAL INPUT FEATURE LIST\n",
    "input_feature_cols = time_feature_cols + weather_norm_cols + purchase_norm_cols + freq_norm_cols\n",
    "\n",
    "# Define item columns (must be 0/1 binary purchase values)\n",
    "item_cols = [\n",
    "    c for c in encoded_df.columns\n",
    "    if c not in input_feature_cols \n",
    "    and c not in [\"date\", \"time\", \"dateTime\"]\n",
    "    and encoded_df[c].dropna().isin([0,1]).all()\n",
    "]\n",
    "\n",
    "# keep items >=5 purchases\n",
    "frequent_items = [col for col in item_cols if encoded_df[col].sum() >= 5]\n",
    "\n",
    "# Y MUST COME FROM frequent_items\n",
    "y = encoded_df[frequent_items].to_numpy(dtype=np.float32)\n",
    "\n",
    "# X MUST COME FROM THE INPUT FEATURE COLS\n",
    "X = encoded_df[input_feature_cols].to_numpy(dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e75d1-6cb4-4e1c-939d-d9f79c9ee51e",
   "metadata": {},
   "source": [
    "# TRAIN !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19f806-8d3a-4cf6-b60d-b926d86e6ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# DEFINE THE MODEL\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    #layers.Dense(128, activation=\"relu\"),\n",
    "    #layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(output_dim, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(threshold=0.5, name=\"bin_acc\"),\n",
    "        tf.keras.metrics.AUC(curve=\"ROC\", name=\"auc\"),\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# CLASS WEIGHTS FOR IMBALANCE\n",
    "class_weights = {}\n",
    "for i in range(len(frequent_items)):      # <-- FIXED\n",
    "    positives = y_train[:, i].sum()\n",
    "    negatives = len(y_train) - positives\n",
    "\n",
    "    w = negatives / (positives + 1e-6)\n",
    "\n",
    "    if w > 50:\n",
    "        w = 50.0\n",
    "\n",
    "    class_weights[i] = w\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fd429-c69f-4a7b-a455-6235a92ab9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Apply threshold\n",
    "threshold = 0.5\n",
    "y_pred_bin = (y_pred >= threshold).astype(int)\n",
    "\n",
    "\n",
    "#pd.DataFrame(y_pred_bin, columns=frequent_items).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f447f1-6f26-411e-9904-3854f88b06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_next_trip(model, encoded_df, input_feature_cols, frequent_items):\n",
    "    \"\"\"\n",
    "    Build a single input row for prediction using the most recent trip in encoded_df.\n",
    "    Returns a sorted DataFrame of predicted probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # 1. Get the most recent row (latest trip)\n",
    "    # ------------------------------\n",
    "    last = encoded_df.iloc[-1]\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2. Build a new row using last-known feature values\n",
    "    # ------------------------------\n",
    "    x = {}\n",
    "\n",
    "    for col in input_feature_cols:\n",
    "        if col in encoded_df.columns:\n",
    "            x[col] = last[col]\n",
    "        else:\n",
    "            # safety: unknown column\n",
    "            x[col] = 0.0\n",
    "\n",
    "    # Convert to model input shape\n",
    "    X_input = np.array([x[col] for col in input_feature_cols], dtype=np.float32)\n",
    "    X_input = X_input.reshape(1, -1)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 3. Predict probabilities\n",
    "    # ------------------------------\n",
    "    y_pred = model.predict(X_input)[0]   # shape: (num_items,)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 4. Build labeled output table\n",
    "    # ------------------------------\n",
    "    result = pd.DataFrame({\n",
    "        \"item\": frequent_items,\n",
    "        \"probability\": y_pred\n",
    "    })\n",
    "\n",
    "    # Sort highest-probability first\n",
    "    result = result.sort_values(by=\"probability\", ascending=False)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66279c7-4e27-4294-91db-6a5e87493a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_next_trip(\n",
    "    model=model,\n",
    "    encoded_df=encoded_df,\n",
    "    input_feature_cols=input_feature_cols,\n",
    "    frequent_items=frequent_items\n",
    ")\n",
    "\n",
    "print(pred.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e2ba3-e77e-46ee-ac02-69de35218408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1fc84-8faf-4d35-888a-53daa9b16971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a482c6cc-7a20-47ae-8098-04c377d289c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929f742-2b1d-4630-9d81-6cfc8ab2f86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b564ea-6b8f-404a-a12d-3ca3cd9694ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
