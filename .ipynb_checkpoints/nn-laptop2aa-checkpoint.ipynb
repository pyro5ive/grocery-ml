{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192abc4e-1a90-416f-b168-9093c1493c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from winn_dixie_recpt_parser import WinnDixieRecptParser \n",
    "\n",
    "import asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "\n",
    "print(os.getcwd())\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e514d2-7c8e-45ea-a1bd-88d5d4e6ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_receipt_files(df):\n",
    "    \"\"\"\n",
    "    Remove whole source files that contain an identical receipt\n",
    "    to another file with the same date+time.\n",
    "    Minimal console output. Resets index at end.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"__signature\"] = (\n",
    "        df[\"date\"].astype(str) + \"|\" +\n",
    "        df[\"time\"].astype(str) + \"|\" +\n",
    "        df[\"item\"].astype(str) + \"|\" \n",
    "        #df[\"qty\"].astype(str) + \"|\" +\n",
    "        #df[\"youPay\"].astype(str) + \"|\" +\n",
    "        #df[\"reg\"].astype(str) + \"|\" +\n",
    "        #df[\"reportedItemsSold\"].astype(str) + \"|\" +\n",
    "        #df[\"cashier\"].astype(str) + \"|\" +\n",
    "        #df[\"manager\"].astype(str)\n",
    "    )\n",
    "\n",
    "    keep_sources = set()\n",
    "\n",
    "    for (dt_date, dt_time), group in df.groupby([\"date\", \"time\"]):\n",
    "\n",
    "        # Build signature per source\n",
    "        source_signatures = {}\n",
    "        for source, rows in group.groupby(\"source\"):\n",
    "            sig = tuple(sorted(rows[\"__signature\"].tolist()))\n",
    "            source_signatures[source] = sig\n",
    "\n",
    "        # signature → list of sources\n",
    "        signature_groups = {}\n",
    "        for src, sig in source_signatures.items():\n",
    "            signature_groups.setdefault(sig, []).append(src)\n",
    "\n",
    "        # Handle duplicates\n",
    "        for sig, sources in signature_groups.items():\n",
    "            if len(sources) == 1:\n",
    "                keep_sources.add(sources[0])\n",
    "                continue\n",
    "\n",
    "            sorted_sources = sorted(sources)\n",
    "            kept = sorted_sources[0]\n",
    "            removed = sorted_sources[1:]\n",
    "\n",
    "            # Minimal output\n",
    "            print(f\"DUP: {dt_date} {dt_time} → keep {kept} ← drop {', '.join(removed)}\")\n",
    "\n",
    "            keep_sources.add(kept)\n",
    "\n",
    "    # Filter and clean\n",
    "    result = df[df[\"source\"].isin(keep_sources)].copy()\n",
    "    result.drop(columns=[\"__signature\"], inplace=True)\n",
    "\n",
    "    # ✔ Reset index here\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return result\n",
    "#################################################################\n",
    "\n",
    "# def rolling_freq(df, window_days):\n",
    "#     out = []\n",
    "#     for idx, row in df.iterrows():\n",
    "#         item = row[\"item\"]\n",
    "#         cutoff = row[\"date\"] - pd.Timedelta(days=window_days)\n",
    "#         count = df[(df[\"item\"] == item) &\n",
    "#                    (df[\"date\"] > cutoff) &\n",
    "#                    (df[\"date\"] < row[\"date\"])].shape[0]\n",
    "#         out.append(count)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47046089-a2d9-4136-95bb-3888c94aab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daysUntilNextHoliday(d):\n",
    "    d = pd.to_datetime(d)\n",
    "    holidays = USFederalHolidayCalendar().holidays()\n",
    "    diffs = (holidays - d).days\n",
    "    diffs = diffs[diffs >= 0]\n",
    "    return diffs.min() if len(diffs) > 0 else np.nan\n",
    "####################################################################\n",
    "\n",
    "def daysSinceLastHoliday(d):\n",
    "    d = pd.to_datetime(d)\n",
    "    holidays = USFederalHolidayCalendar().holidays()\n",
    "    diffs = (d - holidays).days\n",
    "    diffs = diffs[diffs >= 0]\n",
    "    return diffs.min() if len(diffs) > 0 else np.nan\n",
    "####################################################################\n",
    "\n",
    "def holidayProximityIndex(d, scale=30):\n",
    "    \"\"\"\n",
    "    Returns a smooth value between -1 and +1 depending on\n",
    "    distance to holidays. Neural networks LOVE this.\n",
    "    Negative = after holiday\n",
    "    Positive = before holiday\n",
    "    \"\"\"\n",
    "    before = daysUntilNextHoliday(d)\n",
    "    after = daysSinceLastHoliday(d)\n",
    "\n",
    "    if pd.isna(before) and pd.isna(after):\n",
    "        return 0\n",
    "\n",
    "    # choose the nearest side (before or after)\n",
    "    if before <= after:\n",
    "        return +max(0, (scale - before) / scale)\n",
    "    else:\n",
    "        return -max(0, (scale - after) / scale)\n",
    "####################################################################\n",
    "\n",
    "def daysUntilBirthday(d, bday):\n",
    "    d = pd.to_datetime(d)\n",
    "    bday = pd.to_datetime(bday)\n",
    "\n",
    "    this_year = pd.Timestamp(d.year, bday.month, bday.day)\n",
    "    if d <= this_year:\n",
    "        return (this_year - d).days\n",
    "    else:\n",
    "        next_year = pd.Timestamp(d.year + 1, bday.month, bday.day)\n",
    "        return (next_year - d).days\n",
    "####################################################################\n",
    "\n",
    "def daysSinceBirthday(d, bday):\n",
    "    d = pd.to_datetime(d)\n",
    "    bday = pd.to_datetime(bday)\n",
    "\n",
    "    this_year = pd.Timestamp(d.year, bday.month, bday.day)\n",
    "    if d >= this_year:\n",
    "        return (d - this_year).days\n",
    "    else:\n",
    "        last_year = pd.Timestamp(d.year - 1, bday.month, bday.day)\n",
    "        return (d - last_year).days\n",
    "####################################################################\n",
    "\n",
    "def tempDeviation(actualTemp, avgTemp):\n",
    "    \"\"\"Signed deviation (continuous). Neural-network gold.\"\"\"\n",
    "    return actualTemp - avgTemp\n",
    "####################################################################\n",
    "\n",
    "def humidityDeviation(actualHumidity, avgHumidity):\n",
    "    return actualHumidity - avgHumidity\n",
    "####################################################################\n",
    "\n",
    "def precipDeviation(actual, avg):\n",
    "    return actual - avg\n",
    "####################################################################\n",
    "\n",
    "def daysUntilSchoolStart(d):\n",
    "    d = pd.to_datetime(d)\n",
    "    start = pd.Timestamp(d.year, 8, 15)\n",
    "    if d <= start:\n",
    "        return (start - d).days\n",
    "    else:\n",
    "        next_start = pd.Timestamp(d.year + 1, 8, 15)\n",
    "        return (next_start - d).days\n",
    "####################################################################\n",
    "\n",
    "def daysUntilSchoolEnd(d):\n",
    "    d = pd.to_datetime(d)\n",
    "    end = pd.Timestamp(d.year, 5, 31)\n",
    "    if d <= end:\n",
    "        return (end - d).days\n",
    "    else:\n",
    "        next_end = pd.Timestamp(d.year + 1, 5, 31)\n",
    "        return (next_end - d).days\n",
    "####################################################################\n",
    "\n",
    "def schoolSeasonIndex(d):\n",
    "    \"\"\"\n",
    "    Smooth 0→1 curve inside school season.\n",
    "    <0 before season, >1 after.\n",
    "    Good for neural nets.\n",
    "    \"\"\"\n",
    "    d = pd.to_datetime(d)\n",
    "    start = pd.Timestamp(d.year, 8, 15)\n",
    "    end   = pd.Timestamp(d.year, 5, 31)\n",
    "\n",
    "    # If date is after Dec, school season continues in Jan–May.\n",
    "    if d < start:\n",
    "        return -((start - d).days) / 365.0\n",
    "    elif start <= d <= end:\n",
    "        return (d - start).days / (end - start).days\n",
    "    else:\n",
    "        return (d - end).days / 365.0\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def normalizeAndDropCols(df, cols):\n",
    "    for col in cols:\n",
    "        # Replace the sentinel 999 with NaN so it doesn't distort mean/std\n",
    "        df[col] = df[col].replace(999, np.nan)\n",
    "\n",
    "        # Compute mean/std ignoring NaN\n",
    "        mean = df[col].mean()\n",
    "        std  = df[col].std() or 1.0\n",
    "\n",
    "        # Normalize\n",
    "        df[col + \"_norm\"] = (df[col] - mean) / std\n",
    "\n",
    "        # After normalization: missing values become 0 (neutral)\n",
    "        df[col + \"_norm\"] = df[col + \"_norm\"].fillna(0.0)\n",
    "\n",
    "    return df.drop(columns=cols)\n",
    "\n",
    "\n",
    "#def normalizeAndDropCols(df, cols):\n",
    "#    for col in cols:\n",
    "#        std = df[col].std() or 1.0\n",
    "#        df[col + \"_norm\"] = (df[col] - df[col].mean()) / std\n",
    "#    return df.drop(columns=cols)\n",
    "\n",
    "\n",
    "\n",
    "def canonicalize_items(df, patterns, canonical_name):\n",
    "    \"\"\"\n",
    "    For each pattern in `patterns`, find rows where `item` contains the pattern\n",
    "    and replace df['item'] with `canonical_name`.\n",
    "    \"\"\"\n",
    "    for p in patterns:\n",
    "        mask = df[\"item\"].str.contains(p, case=False, na=False)\n",
    "        df.loc[mask, \"item\"] = canonical_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb5f82-f270-4a38-9414-341412e3c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WEATHER PREP ---\n",
    "weatherCols=[\"datetime\", \"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"]\n",
    "df_weather = pd.read_csv(\"datasets/VisualCrossing-70062 2000-01-01 to 2025-12-14.csv\", usecols=weatherCols)\n",
    "\n",
    "df_weather[\"datetime\"] = pd.to_datetime(df_weather[\"datetime\"])\n",
    "df_weather = df_weather.set_index(\"datetime\").sort_index()\n",
    "\n",
    "df_weather[\"temp_5day_avg\"] = df_weather[\"temp\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"feelsLike_5day_avg\"] = df_weather[\"feelslike\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"dew_5day_avg\"] = df_weather[\"dew\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"humidity_5day_avg\"] = df_weather[\"humidity\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"precip_5day_avg\"] = df_weather[\"precip\"].rolling(5, min_periods=1).mean()\n",
    "\n",
    "df_weather = df_weather.drop(columns=[\"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"])\n",
    "\n",
    "# convert index to date for merging\n",
    "df_weather[\"date\"] = df_weather.index.date\n",
    "df_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"])\n",
    "df_weather = df_weather.set_index(\"date\")\n",
    "\n",
    "#grouped.to_csv(\"grouped.csv\", index=False)\n",
    "#grouped.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6c3bb-900a-430b-b7b9-7eb518b2d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def ImportWallMart(folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Import all Walmart receipt CSV files from a folder.\n",
    "    Adds a 'source' column set to the CSV filename.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.lower().endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            dataframe = pd.read_csv(file_path)\n",
    "            dataframe[\"source\"] = file_name\n",
    "            dataframes.append(dataframe)\n",
    "\n",
    "    if len(dataframes) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622cefe-d5e0-496d-b547-0cfd003d4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "recptParser  = WinnDixieRecptParser();\n",
    "\n",
    "for p in Path(\"winndixie rcpts/StevePhone2/pdf/text\").glob(\"*.txt\"):\n",
    "    result = recptParser.parse(p.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "    for r in result[\"items\"]:\n",
    "        rows.append({\n",
    "            \"source\": p.name,\n",
    "            \"date\": result[\"date\"],\n",
    "            \"time\": result[\"time\"],\n",
    "            #\"manager\": result[\"manager\"],\n",
    "            #\"cashier\": result[\"cashier\"],\n",
    "            \"item\": r[\"item\"]\n",
    "            #\"qty\": r[\"qty\"],\n",
    "            #\"reg\": r[\"reg\"],\n",
    "            #\"youPay\": r[\"youPay\"],\n",
    "            #\"reportedItemsSold\": result[\"reported\"],\n",
    "            #\"rowsMatchReported\": result[\"validation\"][\"rowsMatchReported\"],\n",
    "            #\"qtyMatchReported\": result[\"validation\"][\"qtyMatchReported\"],\n",
    "        })\n",
    "\n",
    "winndixie_df = pd.DataFrame(rows)\n",
    "\n",
    "winndixie_df[\"date\"] = pd.to_datetime(winndixie_df[\"date\"])\n",
    "winndixie_df[\"time\"] = winndixie_df[\"time\"].astype(str)\n",
    "winndixie_df = remove_duplicate_receipt_files(winndixie_df)\n",
    "winndixie_df = winndixie_df.sort_values(by=[\"date\", \"time\"]).reset_index(drop=True)\n",
    "winndixie_df = winndixie_df.drop(columns=[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a996e24-1868-4bf5-92e1-266b8d0719c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wallmart_raw = ImportWallMart(\"./walmart\")\n",
    "wallmart_raw[\"Product Description\"] = (\n",
    "    wallmart_raw[\"Product Description\"]\n",
    "    .str.replace(\"Great Value\", \"\", regex=False)\n",
    "    .str.replace(\"Freshness Guaranteed\", \"\", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "## remove some non-food items\n",
    "wallmart_raw = wallmart_raw[\n",
    "    ~wallmart_raw[\"Product Description\"].str.contains(\"Mainstays\", case=False, na=False)\n",
    "    &\n",
    "    ~wallmart_raw[\"Product Description\"].str.contains(\"Sizes\", case=False, na=False)\n",
    "    &\n",
    "    ~wallmart_raw[\"Product Description\"].str.contains(\"Pen+Gear\", case=False, na=False, regex=False)\n",
    "    &\n",
    "    ~wallmart_raw[\"Product Description\"].str.contains(\"Athletic\", case=False, na=False)  \n",
    "]\n",
    "\n",
    "## rename cols\n",
    "wallmart_df = wallmart_raw[[\"Order Date\",\"Product Description\", \"source\"]].copy()\n",
    "wallmart_df = wallmart_df.rename(columns={\n",
    "    \"Order Date\": \"date\",\n",
    "    \"Product Description\": \"item\"\n",
    "})\n",
    "\n",
    "wallmart_df[\"date\"] = pd.to_datetime(wallmart_df[\"date\"])\n",
    "wallmart_df.to_csv(\"wallmart_df.csv\", index=False)\n",
    "\n",
    "winndixie_df[\"date\"] = pd.to_datetime(winndixie_df[\"date\"])\n",
    "\n",
    "combined_df = pd.concat(\n",
    "    [winndixie_df, wallmart_df[[\"date\", \"item\", \"source\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "combined_df.info()\n",
    "combined_df.head()\n",
    "combined_df.to_csv(\"combined_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd148d-3caa-4967-bb0d-ec482d400ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_patterns = [\"know-and-love-milk\", \"kandl-milk\", \"prairie-farm-milk\",\"kleinpeter-milk\", \"kl-milk\", \"Milk, Fat Free,\", \"Fat-Free Milk\"]\n",
    "canonicalize_items(combined_df, milk_patterns, \"milk\")\n",
    "\n",
    "bread_patterns = [\"bunny-bread\",\"se-grocers-bread\",\"seg-sandwich-bread\", \"seg-white-bread\"]\n",
    "canonicalize_items(combined_df, bread_patterns, \"bread\")\n",
    "\n",
    "cheese_patterns = [\"dandw-cheese\", \"kraft-cheese\", \"se-grocers-cheese\", \"know-and-love-cheese\"]\n",
    "canonicalize_items(combined_df, cheese_patterns, \"cheese\")\n",
    "\n",
    "mayo_patterns = [\"blue-plate-mayo\", \"blue-plate-mynnase\"]\n",
    "canonicalize_items(combined_df, mayo_patterns, \"mayo\")\n",
    "\n",
    "chicken_patterns = [\"chicken-cutlet\", \"chicken-leg\", \"chicken-thigh\", \"chicken-thighs\"]\n",
    "canonicalize_items(combined_df, chicken_patterns, \"chicken\")\n",
    "\n",
    "yogurt_patterns = [\"chobani-yogrt-flip\", \"chobani-yogurt\"]\n",
    "canonicalize_items(combined_df, yogurt_patterns, \"yogurt\")\n",
    "\n",
    "coke_patterns = [\"coca-cola\", \"coca-cola-cola\", \"cocacola-soda\"]\n",
    "canonicalize_items(combined_df, coke_patterns, \"coke\")\n",
    "\n",
    "hugbi_patterns = [\"hugbi-pies\", \"-hugbi-pies\"]\n",
    "canonicalize_items(combined_df, hugbi_patterns, \"hugbi-pies\")\n",
    "\n",
    "minute_maid_patterns = [\"minute-maid-drink\", \"minute-maid-drinks\", \"minute-maid-lmnade\"]\n",
    "canonicalize_items(combined_df, minute_maid_patterns, \"minute-maid-drink\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4b591-943b-444d-ae9c-1557b99a282f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### CREATE ITEM IDs\n",
    "unique_items = sorted(combined_df[\"item\"].unique())\n",
    "item_to_id = {item: idx for idx, item in enumerate(unique_items)}\n",
    "id_to_item = {idx: item for item, idx in item_to_id.items()}\n",
    "combined_df[\"itemId\"] = combined_df[\"item\"].map(item_to_id)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "combined_df.info()\n",
    "combined_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347749b-3977-40e3-a9a2-79c0f5c1389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Build full receipt × item table WITHOUT using qty\n",
    "# ============================================================\n",
    "\n",
    "# 1. Mark actual purchases in the raw receipt rows\n",
    "combined_df[\"didBuy\"] = 1\n",
    "\n",
    "# 2. Build complete grid\n",
    "all_items = combined_df[\"itemId\"].unique()\n",
    "all_dates = combined_df[\"date\"].unique()\n",
    "\n",
    "full = (\n",
    "    pd.MultiIndex.from_product(\n",
    "        [all_dates, all_items], \n",
    "        names=[\"date\", \"itemId\"]\n",
    "    ).to_frame(index=False)\n",
    ")\n",
    "\n",
    "# 3. Merge raw purchases onto the full grid\n",
    "df_full = full.merge(\n",
    "    combined_df[[\"date\", \"itemId\", \"item\", \"source\", \"didBuy\"]],\n",
    "    on=[\"date\", \"itemId\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4. Fill missing purchases with didBuy=0\n",
    "df_full[\"didBuy\"] = df_full[\"didBuy\"].fillna(0).astype(int)\n",
    "\n",
    "# 5. NOW REPLACE combined_df with df_full\n",
    "combined_df = df_full.copy()\n",
    "\n",
    "combined_df.to_csv(\"df_fullreceipts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0664c4-32c3-4379-ba57-71018f599c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build grouped table (one row per trip date)\n",
    "\n",
    "grouped = ( combined_df[[\"date\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"date\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 2. daysSinceLastTrip\n",
    "grouped[\"daysSinceLastTrip\"] = grouped[\"date\"].diff().dt.days.fillna(0)\n",
    "\n",
    "# 3. Holiday / Birthday / School features\n",
    "grouped[\"daysUntilNextHoliday\"] = grouped[\"date\"].apply(daysUntilNextHoliday)\n",
    "grouped[\"daysSinceLastHoliday\"] = grouped[\"date\"].apply(daysSinceLastHoliday)\n",
    "grouped[\"holidayProximityIndex\"] = grouped[\"date\"].apply(holidayProximityIndex)\n",
    "grouped[\"daysUntilSchoolStart\"] = grouped[\"date\"].apply(daysUntilSchoolStart)\n",
    "grouped[\"daysUntilSchoolEnd\"]   = grouped[\"date\"].apply(daysUntilSchoolEnd)\n",
    "grouped[\"schoolSeasonIndex\"]    = grouped[\"date\"].apply(schoolSeasonIndex)\n",
    "\n",
    "dt = grouped[\"date\"]\n",
    "grouped[\"year\"]    = dt.dt.year\n",
    "grouped[\"month\"]   = dt.dt.month\n",
    "grouped[\"day\"]     = dt.dt.day\n",
    "grouped[\"dow\"]     = dt.dt.dayofweek\n",
    "grouped[\"doy\"]     = dt.dt.dayofyear\n",
    "grouped[\"quarter\"] = dt.dt.quarter\n",
    "\n",
    "BIRTHDAYS = {\n",
    "    \"steve\":  \"03-05-1980\",  # fill with your real dates\n",
    "    \"maggie\": \"03-03-2016\",\n",
    "    \"mil\":    \"01-27-1962\",\n",
    "    \"angie\":  \"08-11-1981\",\n",
    "}\n",
    "\n",
    "grouped[\"daysUntilBirthday_steve\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, BIRTHDAYS[\"steve\"]))\n",
    "grouped[\"daysSinceBirthday_steve\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, BIRTHDAYS[\"steve\"]))\n",
    "grouped[\"daysUntilBirthday_maggie\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, BIRTHDAYS[\"maggie\"]))\n",
    "grouped[\"daysSinceBirthday_maggie\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, BIRTHDAYS[\"maggie\"]))\n",
    "grouped[\"daysUntilBirthday_mil\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, BIRTHDAYS[\"mil\"]))\n",
    "grouped[\"daysSinceBirthday_mil\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, BIRTHDAYS[\"mil\"]))\n",
    "grouped[\"daysUntilBirthday_angie\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, BIRTHDAYS[\"angie\"]))\n",
    "grouped[\"daysSinceBirthday_angie\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, BIRTHDAYS[\"angie\"]))\n",
    "\n",
    "# merge in weather\n",
    "grouped = grouped.merge(df_weather, on=\"date\", how=\"left\")\n",
    "\n",
    "combined_df = combined_df.merge(grouped, on=\"date\", how=\"left\")\n",
    "combined_df.info()\n",
    "combined_df.head(10)\n",
    "\n",
    "combined_df.to_csv(\"df_merged_group_level_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf2225-3a20-45fe-810f-624a5e984ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# FREQUENCY WINDOWS (7, 15, 30, 90, 365)\n",
    "# True rolling-window implementation\n",
    "# ================================================\n",
    "def fill_freq(group):\n",
    "    group = group.copy()\n",
    "    group = group.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    history = []\n",
    "\n",
    "    col_date = group.columns.get_loc(\"date\")\n",
    "    col_buy = group.columns.get_loc(\"didBuy\")\n",
    "    col_freq = {w: group.columns.get_loc(f\"freq_{w}\") for w in freq_windows}\n",
    "\n",
    "    for i in range(len(group)):\n",
    "        cur_date = group.iat[i, col_date]\n",
    "\n",
    "        # record purchase\n",
    "        if group.iat[i, col_buy] == 1:\n",
    "            history.append(cur_date)\n",
    "\n",
    "        # prune history ONCE using largest window\n",
    "        cutoff_max = cur_date - pd.Timedelta(days=max_w)\n",
    "        history = [d for d in history if d >= cutoff_max]\n",
    "\n",
    "        # compute windowed counts\n",
    "        for w in freq_windows:\n",
    "            cutoff = cur_date - pd.Timedelta(days=w)\n",
    "            count = 0\n",
    "            for d in history:\n",
    "                if d >= cutoff:\n",
    "                    count += 1\n",
    "            group.iat[i, col_freq[w]] = count\n",
    "\n",
    "    return group\n",
    "#######################################################\n",
    "freq_windows = [7, 15, 30, 90, 365]\n",
    "max_w = max(freq_windows)\n",
    "\n",
    "# initialize columns\n",
    "for w in freq_windows:\n",
    "    combined_df[f\"freq_{w}\"] = np.nan\n",
    "\n",
    "combined_df = (\n",
    "    combined_df\n",
    "    .groupby(\"itemId\", group_keys=False)\n",
    "    .apply(fill_freq)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955044d3-645d-44ad-b9b0-f3129b1e565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INCREASING DAILY daysSinceLastPurchase (resets on purchase)\n",
    "# ============================================================\n",
    "def fill_item(group):\n",
    "    group = group.copy()\n",
    "    # iterate row-by-row using positional index\n",
    "    for i in range(1, len(group)):\n",
    "        if pd.isna(group.iat[i, group.columns.get_loc(\"daysSinceLastPurchase\")]):\n",
    "            prev_val = group.iat[i-1, group.columns.get_loc(\"daysSinceLastPurchase\")]\n",
    "            trip_gap = group.iat[i, group.columns.get_loc(\"daysSinceLastTrip\")]\n",
    "            group.iat[i, group.columns.get_loc(\"daysSinceLastPurchase\")] = prev_val + trip_gap\n",
    "    return group\n",
    "##########################################################################################\n",
    "\n",
    "combined_df = combined_df.sort_values([\"itemId\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Start with NaN everywhere\n",
    "combined_df[\"daysSinceLastPurchase\"] = np.nan\n",
    "\n",
    "# Set 0 on purchase days\n",
    "combined_df.loc[combined_df[\"didBuy\"] == 1, \"daysSinceLastPurchase\"] = 0\n",
    "combined_df = combined_df.groupby(\"itemId\", group_keys=False).apply(fill_item)\n",
    "\n",
    "# Items with no purchase history get 999\n",
    "combined_df[\"daysSinceLastPurchase\"] = combined_df[\"daysSinceLastPurchase\"].fillna(999)\n",
    "combined_df.to_csv(\"daysSinceLastPurchase.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e477f-aa81-4d9e-bb31-2acece50cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ITEM-LEVEL HABIT FEATURES (TF-IDF ANALOG)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_habit_features(df, tau_days=120):\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    total_trips = df[\"date\"].nunique()\n",
    "    timeline_days = (df[\"date\"].max() - df[\"date\"].min()).days or 1\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for itemId, g in df.groupby(\"itemId\"):\n",
    "        buys = g[g[\"didBuy\"] == 1][\"date\"]\n",
    "\n",
    "        if len(buys) == 0:\n",
    "            rows.append({\n",
    "                \"itemId\": itemId,\n",
    "                \"habitFrequency\": 0.0,\n",
    "                \"habitSpan\": 0.0,\n",
    "                \"habitDecay\": 0.0,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        first = buys.min()\n",
    "        last = buys.max()\n",
    "\n",
    "        habitFrequency = len(buys) / total_trips\n",
    "        habitSpan = (last - first).days / timeline_days\n",
    "        days_since_last = (df[\"date\"].max() - last).days\n",
    "        habitDecay = np.exp(-days_since_last / tau_days)\n",
    "\n",
    "        rows.append({\n",
    "            \"itemId\": itemId,\n",
    "            \"habitFrequency\": habitFrequency,\n",
    "            \"habitSpan\": habitSpan,\n",
    "            \"habitDecay\": habitDecay,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def compute_due_score(\n",
    "    df,\n",
    "    itemId=None,\n",
    "    use_sigmoid=True,\n",
    "    normalize=False,\n",
    "    weights=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute due_score from RAW (non-normalized) features.\n",
    "\n",
    "    Required columns:\n",
    "      - itemId\n",
    "      - daysSinceLastPurchase\n",
    "      - freq_30\n",
    "      - freq_90\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "\n",
    "    itemId : int | None\n",
    "        If provided, compute only for this itemId.\n",
    "        If None, compute for all items.\n",
    "\n",
    "    use_sigmoid : bool\n",
    "        Apply sigmoid → (0,1)\n",
    "\n",
    "    normalize : bool\n",
    "        Z-normalize instead (ignored if use_sigmoid=True)\n",
    "\n",
    "    weights : dict | None\n",
    "        Optional override for feature weights\n",
    "    \"\"\"\n",
    "\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            \"daysSinceLastPurchase\": 1.5,\n",
    "            \"freq_30\": 1.0,\n",
    "            \"freq_90\": 0.5\n",
    "        }\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Optional itemId filter\n",
    "    # --------------------------------------------------------\n",
    "    if itemId is not None:\n",
    "        df = df[df[\"itemId\"] == itemId].copy()\n",
    "    else:\n",
    "        df = df.copy()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # RAW linear score (pre-normalization)\n",
    "    # --------------------------------------------------------\n",
    "    df[\"due_score_raw\"] = (\n",
    "        weights[\"daysSinceLastPurchase\"] * df[\"daysSinceLastPurchase\"]\n",
    "      + weights[\"freq_30\"]              * df[\"freq_30\"]\n",
    "      + weights[\"freq_90\"]              * df[\"freq_90\"]\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Final due_score\n",
    "    # --------------------------------------------------------\n",
    "    if use_sigmoid:\n",
    "        df[\"due_score\"] = 1 / (1 + np.exp(-df[\"due_score_raw\"]))\n",
    "\n",
    "    elif normalize:\n",
    "        mean = df[\"due_score_raw\"].mean()\n",
    "        std  = df[\"due_score_raw\"].std() or 1.0\n",
    "        df[\"due_score\"] = (df[\"due_score_raw\"] - mean) / std\n",
    "\n",
    "    else:\n",
    "        df[\"due_score\"] = df[\"due_score_raw\"]\n",
    "\n",
    "    return df\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MERGE HABIT FEATURES\n",
    "# ============================================================\n",
    "habit_df = build_habit_features(combined_df)\n",
    "\n",
    "combined_df = combined_df.merge(\n",
    "    habit_df,\n",
    "    on=\"itemId\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "combined_df[[\"habitFrequency\", \"habitSpan\", \"habitDecay\"]] = (\n",
    "    combined_df[[\"habitFrequency\", \"habitSpan\", \"habitDecay\"]].fillna(0.0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba56be9-f33c-4089-b0e9-aecf98c2e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## trim fat\n",
    "# find rows with freq_365 of 1 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdaa6a-c44f-4d32-97d2-7acdd4922864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NORMALIZE TO ENCODED_DF\n",
    "# ============================================================\n",
    "\n",
    "freq_cols = [c for c in combined_df.columns if c.startswith(\"freq_\")]\n",
    "weather_cols = [c for c in combined_df.columns if c.endswith(\"_5day_avg\")]\n",
    "holiday_cols = [c for c in combined_df.columns if \"holiday\" in c.lower()]\n",
    "school_cols = [c for c in combined_df.columns if \"school\" in c.lower()]\n",
    "birthday_cols = [\n",
    "    c for c in combined_df.columns\n",
    "    if c.startswith(\"daysUntilBirthday_\") or c.startswith(\"daysSinceBirthday_\")\n",
    "]\n",
    "\n",
    "daysSince_purchase_cols = [\"daysSinceLastPurchase\"]\n",
    "daysSince_trip_cols     = [\"daysSinceLastTrip\"]\n",
    "\n",
    "habit_cols = [\"habitFrequency\", \"habitSpan\", \"habitDecay\"]\n",
    "\n",
    "encoded_df = combined_df.copy()\n",
    "encoded_df = normalizeAndDropCols(encoded_df, freq_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, weather_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, holiday_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, school_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, birthday_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, daysSince_purchase_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, daysSince_trip_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, habit_cols)\n",
    "\n",
    "encoded_df.info()\n",
    "encoded_df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b8c92-d433-4873-9856-c3f7cb6e5f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------- CYCLICAL FEATURES ----------\n",
    "encoded_df[\"dow_sin\"]   = np.sin(2 * np.pi * encoded_df[\"dow\"] / 7.0)\n",
    "encoded_df[\"dow_cos\"]   = np.cos(2 * np.pi * encoded_df[\"dow\"] / 7.0)\n",
    "encoded_df[\"month_sin\"] = np.sin(2 * np.pi * encoded_df[\"month\"] / 12.0)\n",
    "encoded_df[\"month_cos\"] = np.cos(2 * np.pi * encoded_df[\"month\"] / 12.0)\n",
    "encoded_df[\"doy_sin\"]   = np.sin(2 * np.pi * encoded_df[\"doy\"] / 365.0)\n",
    "encoded_df[\"doy_cos\"]   = np.cos(2 * np.pi * encoded_df[\"doy\"] / 365.0)\n",
    "\n",
    "encoded_df = encoded_df.drop(columns=[\"dow\", \"month\", \"doy\"], errors=\"ignore\")\n",
    "\n",
    "# ---------- NON-CYCLIC TIME FEATURES ----------\n",
    "nonCycCols = [\"year\", \"day\", \"quarter\"]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, nonCycCols)\n",
    "\n",
    "# ---------- DROP NON-MODEL COLS ----------\n",
    "cols_to_drop = [\"source\", \"item\", \"date\"]\n",
    "encoded_df = encoded_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# ---------- FINAL CHECK ----------\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "\n",
    "encoded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d176f88-24c4-4d8a-899f-44c3a5d7cf71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_df[\"due_score\"] = (\n",
    "    1.5 * encoded_df[\"daysSinceLastPurchase_norm\"]\n",
    "  + 1.0 * encoded_df[\"freq_30_norm\"]\n",
    "  ##+ 0.5 * encoded_df[\"freq_90_norm\"]\n",
    ")\n",
    "\n",
    "####encoded_df[\"due_score\"] = 1 / (1 + np.exp(-encoded_df[\"due_score\"]))\n",
    "\n",
    "\n",
    "encoded_df.info()\n",
    "encoded_df.head()\n",
    "\n",
    "encoded_df.to_csv(\"encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d5b52-1522-4fb9-87a4-f72e07812c17",
   "metadata": {},
   "source": [
    "# TRAIN / BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9fcbe-e123-4498-9824-73f2f3f5d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75bb34-193a-47ba-8e96-db8eaf6a372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment( model, history, predictions, params, numeric_cols, item_id_to_idx, base_dir=\"experiments\"):\n",
    "    \"\"\"\n",
    "    Saves experiment artifacts with a clean, readable folder name.\n",
    "    Folder name encodes only primary structural decisions:\n",
    "      - embedding size\n",
    "      - hidden layer layout\n",
    "      - epoch count\n",
    "    \"\"\"\n",
    "    name_parts = []\n",
    "\n",
    "    if \"embedding_dim\" in params:\n",
    "        name_parts.append(f\"emb{params['embedding_dim']}\")\n",
    "    if \"hiddenLayers\" in params:\n",
    "        hl = \"-\".join(str(x) for x in params[\"hiddenLayers\"])\n",
    "        name_parts.append(f\"hl{hl}\")\n",
    "    if \"epochs\" in params:\n",
    "        name_parts.append(f\"ep{params['epochs']}\")\n",
    "\n",
    "    exp_name = \"__\".join(name_parts) if name_parts else \"exp_unlabeled\"\n",
    "    exp_dir = os.path.join(base_dir, exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Save artifacts (unchanged)\n",
    "    # ------------------------------------------------------------\n",
    "    model.save(os.path.join(exp_dir, \"model\"))\n",
    "    model.save_weights(os.path.join(exp_dir, \"weights.h5\"))\n",
    "\n",
    "    with open(os.path.join(exp_dir, \"history.json\"), \"w\") as f:\n",
    "        json.dump(history.history, f, indent=2)\n",
    "\n",
    "    with open(os.path.join(exp_dir, \"numeric_features.json\"), \"w\") as f:\n",
    "        json.dump(numeric_cols, f, indent=2)\n",
    "\n",
    "    with open(os.path.join(exp_dir, \"item_id_to_idx.json\"), \"w\") as f:\n",
    "        json.dump(\n",
    "            {str(int(k)): int(v) for k, v in item_id_to_idx.items()},\n",
    "            f,\n",
    "            indent=2\n",
    "        )\n",
    "\n",
    "    with open(os.path.join(exp_dir, \"hyperparams.json\"), \"w\") as f:\n",
    "        json.dump(params, f, indent=2)\n",
    "\n",
    "    predictions.to_csv(os.path.join(exp_dir, \"predictions.csv\"), index=False)\n",
    "\n",
    "    print(\"Saved experiment →\", exp_dir)\n",
    "##########################################################################################\n",
    "\n",
    "def build_and_compile_model( num_numeric_features, num_items, params):\n",
    "    num_in = layers.Input(shape=(num_numeric_features,))\n",
    "    item_in = layers.Input(shape=(), dtype=\"int32\")\n",
    "\n",
    "    emb = layers.Embedding(\n",
    "        input_dim=num_items,\n",
    "        output_dim=params[\"embedding_dim\"]\n",
    "    )(item_in)\n",
    "\n",
    "    x = layers.Concatenate()([num_in, layers.Flatten()(emb)])\n",
    "\n",
    "    for units in params[\"hiddenLayers\"]:\n",
    "        x = layers.Dense(units, activation=\"relu\")(x)\n",
    "\n",
    "    out = layers.Dense(\n",
    "        1,\n",
    "        activation=params.get(\"output_activation\", \"sigmoid\")\n",
    "    )(x)\n",
    "\n",
    "    model = models.Model([num_in, item_in], out)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=params.get(\"optimizer\", \"adam\"),\n",
    "        loss=params.get(\"loss\", \"mse\"),\n",
    "        metrics=params.get(\"metrics\", [\"mae\"])\n",
    "    )\n",
    "\n",
    "    return model\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9098d1-6323-4af8-8647-aa06ba56f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params = {\n",
    "    \"embedding_dim\": 64,\n",
    "    \"hiddenLayers\": [4096],\n",
    "    \"loss\": \"mse\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"metrics\": [\"mae\"],\n",
    "    \"output_activation\": \"sigmoid\"\n",
    "}\n",
    "\n",
    "\n",
    "## \n",
    "model = build_and_compile_model(num_numeric_features, num_items, params)\n",
    "\n",
    "save_experiment( model, history, predictions, params, numeric_cols, item_id_to_idx, base_dir=\"experiments\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d87fa-809d-464e-86e2-dd1356bd3569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATE-AWARE PREDICTION (TIGHTER PARAMS + ITEM NAMES)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def run_predictions( model, encoded_df, combined_df, feature_stats, birthdays, predict_date=None):\n",
    "    \"\"\"\n",
    "    Build one prediction row per item using:\n",
    "    - latest encoded feature state (encoded_df)\n",
    "    - raw timeline + names (combined_df)\n",
    "    - recomputed calendar features at predict_date\n",
    "    \"\"\"\n",
    "\n",
    "    if predict_date is None:\n",
    "        predict_date = pd.Timestamp.today().normalize()\n",
    "    else:\n",
    "        predict_date = pd.to_datetime(predict_date).normalize()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Discover numeric features (single source: encoded_df)\n",
    "    # --------------------------------------------------------\n",
    "    numeric_cols = [\n",
    "        c for c in encoded_df.columns\n",
    "        if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "    ]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Lookups from combined_df (single source of truth)\n",
    "    # --------------------------------------------------------\n",
    "    last_date_by_item = (\n",
    "        combined_df\n",
    "        .sort_values(\"date\")\n",
    "        .groupby(\"itemId\")[\"date\"]\n",
    "        .last()\n",
    "    )\n",
    "\n",
    "    item_lookup = (\n",
    "        combined_df[[\"itemId\", \"item\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"itemId\")[\"item\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for itemId, hist in encoded_df.groupby(\"itemId\"):\n",
    "        last = hist.iloc[-1]\n",
    "        last_date = pd.to_datetime(last_date_by_item.loc[itemId]).normalize()\n",
    "\n",
    "        row = {\n",
    "            \"itemId\": itemId,\n",
    "            \"item\": item_lookup.get(itemId, \"UNKNOWN\"),\n",
    "            \"itemIdx\": int(last[\"itemIdx\"])\n",
    "        }\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Copy model-stable numeric features (already normalized)\n",
    "        # ----------------------------------------------------\n",
    "        for col in numeric_cols:\n",
    "            row[col] = last[col]\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Recompute DATE-SENSITIVE features\n",
    "        # ----------------------------------------------------\n",
    "        raw_updates = {\n",
    "            \"daysSinceLastPurchase\": (predict_date - last_date).days,\n",
    "            \"daysUntilNextHoliday\": daysUntilNextHoliday(predict_date),\n",
    "            \"daysSinceLastHoliday\": daysSinceLastHoliday(predict_date),\n",
    "            \"holidayProximityIndex\": holidayProximityIndex(predict_date),\n",
    "            \"daysUntilSchoolStart\": daysUntilSchoolStart(predict_date),\n",
    "            \"daysUntilSchoolEnd\": daysUntilSchoolEnd(predict_date),\n",
    "            \"schoolSeasonIndex\": schoolSeasonIndex(predict_date),\n",
    "            \"year\": predict_date.year,\n",
    "            \"day\": predict_date.day,\n",
    "            \"quarter\": predict_date.quarter\n",
    "        }\n",
    "\n",
    "        # birthdays\n",
    "        for name, bday in birthdays.items():\n",
    "            raw_updates[f\"daysUntilBirthday_{name}\"] = daysUntilBirthday(predict_date, bday)\n",
    "            raw_updates[f\"daysSinceBirthday_{name}\"] = daysSinceBirthday(predict_date, bday)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # Normalize recomputed features\n",
    "        # ----------------------------------------------------\n",
    "        for raw, val in raw_updates.items():\n",
    "            norm_col = raw + \"_norm\"\n",
    "            if norm_col in numeric_cols and raw in feature_stats:\n",
    "                stats = feature_stats[raw]\n",
    "                row[norm_col] = (val - stats[\"mean\"]) / stats[\"std\"]\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    pred_df = pd.DataFrame(rows)\n",
    "\n",
    "    Xn = pred_df[numeric_cols].to_numpy(np.float32)\n",
    "    Xi = pred_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "\n",
    "    scores = model.predict([Xn, Xi], verbose=0).ravel()\n",
    "\n",
    "    pred_df[\"due_intensity\"] = scores\n",
    "\n",
    "    return (\n",
    "        pred_df[[\"itemId\", \"item\", \"due_intensity\"]]\n",
    "        .sort_values(\"due_intensity\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "###############################################################################\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN + PREDICT + SAVE (AUDITABLE, FINAL) — UPDATED\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ENSURE itemIdx\n",
    "# ------------------------------------------------------------\n",
    "item_ids = sorted(encoded_df[\"itemId\"].unique())\n",
    "item_id_to_idx = {iid: i for i, iid in enumerate(item_ids)}\n",
    "encoded_df[\"itemIdx\"] = encoded_df[\"itemId\"].map(item_id_to_idx).astype(\"int32\")\n",
    "NUM_ITEMS = len(item_ids)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FEATURES / TARGET\n",
    "# ------------------------------------------------------------\n",
    "numeric_cols = [\n",
    "    c for c in encoded_df.columns\n",
    "    if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "]\n",
    "\n",
    "Xn = encoded_df[numeric_cols].to_numpy(np.float32)\n",
    "Xi = encoded_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "y  = encoded_df[\"due_score\"].to_numpy(np.float32)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SPLIT\n",
    "# ------------------------------------------------------------\n",
    "Xn_tr, Xn_te, Xi_tr, Xi_te, y_tr, y_te = train_test_split(\n",
    "    Xn, Xi, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MODEL\n",
    "# ------------------------------------------------------------\n",
    "num_in = layers.Input(shape=(Xn_tr.shape[1],))\n",
    "itm_in = layers.Input(shape=(), dtype=\"int32\")\n",
    "\n",
    "emb = layers.Embedding(NUM_ITEMS, 64)(itm_in)\n",
    "emb = layers.Flatten()(emb)\n",
    "\n",
    "x = layers.Concatenate()([num_in, emb])\n",
    "x = layers.Dense(4096, activation=\"relu\")(x)\n",
    "#x = layers.Dense(2048, activation=\"relu\")(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = models.Model([num_in, itm_in], out)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(\n",
    "    [Xn_tr, Xi_tr],\n",
    "    y_tr,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# FEATURE STATS (ONLY recomputed features)\n",
    "# (NOTE: stats are for *_norm columns since inference writes *_norm)\n",
    "# ------------------------------------------------------------\n",
    "feature_stats = {}\n",
    "RECOMPUTED = [\n",
    "    \"daysSinceLastPurchase\",\n",
    "    \"daysUntilNextHoliday\",\n",
    "    \"daysSinceLastHoliday\",\n",
    "    \"holidayProximityIndex\",\n",
    "    \"daysUntilSchoolStart\",\n",
    "    \"daysUntilSchoolEnd\",\n",
    "    \"schoolSeasonIndex\",\n",
    "    \"year\", \"day\", \"quarter\",\n",
    "    \"daysUntilBirthday_steve\", \"daysSinceBirthday_steve\",\n",
    "    \"daysUntilBirthday_maggie\", \"daysSinceBirthday_maggie\",\n",
    "    \"daysUntilBirthday_mil\", \"daysSinceBirthday_mil\",\n",
    "    \"daysUntilBirthday_angie\", \"daysSinceBirthday_angie\",\n",
    "]\n",
    "\n",
    "for raw in RECOMPUTED:\n",
    "    col = raw + \"_norm\"\n",
    "    if col in encoded_df.columns:\n",
    "        std = encoded_df[col].std()\n",
    "        feature_stats[raw] = {\n",
    "            \"mean\": encoded_df[col].mean(),\n",
    "            \"std\": std if std != 0 else 1.0\n",
    "        }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BIRTHDAYS\n",
    "# ------------------------------------------------------------\n",
    "BIRTHDAYS = { \"steve\":  \"03-05-1980\", \"maggie\": \"03-03-2016\",\"mil\": \"01-27-1962\", \"angie\":  \"08-11-1981\"}\n",
    "birthdays = {k: pd.to_datetime(v) for k, v in BIRTHDAYS.items()}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PREDICT (UPDATED CALL)\n",
    "# ------------------------------------------------------------\n",
    "predictions = run_predictions(model=model, encoded_df=encoded_df, combined_df=combined_df, feature_stats=feature_stats, birthdays=birthdays, predict_date=None)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SAVE\n",
    "# ------------------------------------------------------------\n",
    "save_experiment( model=model, history=history, predictions=predictions, params={}, numeric_cols=numeric_cols, item_id_to_idx=item_id_to_idx)\n",
    "\n",
    "predictions.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169958bb-b50a-4e55-923f-f8b4bf5f8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import multiprocessing as mp\n",
    "# from datetime import datetime\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # EXPERIMENT CONFIG\n",
    "# # ============================================================\n",
    "\n",
    "# # RECOMMENDATION:\n",
    "# # Keep embedding sizes small. If 16 shows no benefit over 8, stop.\n",
    "# embedding_sizes = [4, 8, 16]\n",
    "\n",
    "# # RECOMMENDATION:\n",
    "# # Grocery behavior is shallow. 2 layers usually wins.\n",
    "# architectures = [\n",
    "#     [32, 16],\n",
    "#     [64, 32],\n",
    "#     [128, 64]\n",
    "# ]\n",
    "\n",
    "# # RECOMMENDATION:\n",
    "# # Use ReLU unless you observe dead neurons.\n",
    "# activations = [\"relu\"]\n",
    "\n",
    "# # RECOMMENDATION:\n",
    "# # Sigmoid for single-item probability.\n",
    "# output_activation = \"sigmoid\"\n",
    "\n",
    "# # RECOMMENDATION:\n",
    "# # Epoch count is NOT a design decision. Early stopping decides.\n",
    "# max_epochs = 50\n",
    "\n",
    "# base_experiment_dir = \"experiments\"\n",
    "\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # SINGLE EXPERIMENT RUN\n",
    "# # ============================================================\n",
    "\n",
    "# def run_experiment(config):\n",
    "#     (\n",
    "#         embedding_dim,\n",
    "#         dense_layers,\n",
    "#         activation,\n",
    "#         X_train_num,\n",
    "#         X_val_num,\n",
    "#         X_train_item,\n",
    "#         X_val_item,\n",
    "#         y_train,\n",
    "#         y_val,\n",
    "#         num_items\n",
    "#     ) = config\n",
    "\n",
    "#     exp_name = (\n",
    "#         f\"emb{embedding_dim}\"\n",
    "#         f\"_layers{'-'.join(map(str, dense_layers))}\"\n",
    "#         f\"_act{activation}\"\n",
    "#         f\"_epochs{max_epochs}\"\n",
    "#     )\n",
    "\n",
    "#     exp_dir = os.path.join(base_experiment_dir, exp_name)\n",
    "#     os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "#     model = build_model(\n",
    "#         num_numeric_features=X_train_num.shape[1],\n",
    "#         num_items=num_items,\n",
    "#         embedding_dim=embedding_dim,\n",
    "#         dense_layers=dense_layers,\n",
    "#         activation=activation\n",
    "#     )\n",
    "\n",
    "#     early_stop = callbacks.EarlyStopping(\n",
    "#         monitor=\"val_loss\",\n",
    "#         patience=5,\n",
    "#         restore_best_weights=True\n",
    "#     )\n",
    "\n",
    "#     history = model.fit(\n",
    "#         [X_train_num, X_train_item],\n",
    "#         y_train,\n",
    "#         validation_data=([X_val_num, X_val_item], y_val),\n",
    "#         epochs=max_epochs,\n",
    "#         callbacks=[early_stop],\n",
    "#         verbose=0\n",
    "#     )\n",
    "\n",
    "#     # ============================================================\n",
    "#     # SAVE EVERYTHING NEEDED FOR FORENSICS OR REUSE\n",
    "#     # ============================================================\n",
    "\n",
    "#     model.save(os.path.join(exp_dir, \"model\"))\n",
    "#     model.save_weights(os.path.join(exp_dir, \"weights.h5\"))\n",
    "\n",
    "#     with open(os.path.join(exp_dir, \"history.json\"), \"w\") as f:\n",
    "#         json.dump(history.history, f, indent=2)\n",
    "\n",
    "#     with open(os.path.join(exp_dir, \"hyperparams.json\"), \"w\") as f:\n",
    "#         json.dump(\n",
    "#             {\n",
    "#                 \"embedding_dim\": embedding_dim,\n",
    "#                 \"dense_layers\": dense_layers,\n",
    "#                 \"activation\": activation,\n",
    "#                 \"output_activation\": output_activation,\n",
    "#                 \"max_epochs\": max_epochs\n",
    "#             },\n",
    "#             f,\n",
    "#             indent=2\n",
    "#         )\n",
    "\n",
    "#     with open(os.path.join(exp_dir, \"summary.txt\"), \"w\") as f:\n",
    "#         model.summary(print_fn=lambda x: f.write(x + \"\\n\"))\n",
    "\n",
    "#     return exp_name\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # PARALLEL ORCHESTRATION\n",
    "# # ============================================================\n",
    "\n",
    "# def run_all_experiments(\n",
    "#     X_train_num,\n",
    "#     X_val_num,\n",
    "#     X_train_item,\n",
    "#     X_val_item,\n",
    "#     y_train,\n",
    "#     y_val,\n",
    "#     num_items\n",
    "# ):\n",
    "#     os.makedirs(base_experiment_dir, exist_ok=True)\n",
    "\n",
    "#     jobs = []\n",
    "\n",
    "#     for embedding_dim in embedding_sizes:\n",
    "#         for dense_layers in architectures:\n",
    "#             for activation in activations:\n",
    "#                 jobs.append(\n",
    "#                     (\n",
    "#                         embedding_dim,\n",
    "#                         dense_layers,\n",
    "#                         activation,\n",
    "#                         X_train_num,\n",
    "#                         X_val_num,\n",
    "#                         X_train_item,\n",
    "#                         X_val_item,\n",
    "#                         y_train,\n",
    "#                         y_val,\n",
    "#                         num_items\n",
    "#                     )\n",
    "#                 )\n",
    "\n",
    "#     # RECOMMENDATION:\n",
    "#     # Use processes, not threads. TensorFlow releases GIL poorly.\n",
    "#     with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "#         results = pool.map(run_experiment, jobs)\n",
    "\n",
    "#     return results\n",
    "\n",
    "\n",
    "# # ============================================================\n",
    "# # ENTRY POINT\n",
    "# # ============================================================\n",
    "\n",
    "# # Example call (you already have these objects):\n",
    "# # run_all_experiments(\n",
    "# #     X_train_num,\n",
    "# #     X_val_num,\n",
    "# #     X_train_item,\n",
    "# #     X_val_item,\n",
    "# #     y_train,\n",
    "# #     y_val,\n",
    "# #     num_items\n",
    "# # )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Grocery ML",
   "language": "python",
   "name": "grocery-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
