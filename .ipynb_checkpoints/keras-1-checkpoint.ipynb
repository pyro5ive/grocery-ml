{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192abc4e-1a90-416f-b168-9093c1493c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\source\\repos\\grocery-ml\n",
      "GPUs Available: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import os\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from temporal_features import TemporalFeatures\n",
    "from holiday_features import HolidayFeatures\n",
    "from wallmart_rcpt_parser import WallmartRecptParser\n",
    "from winn_dixie_recpt_parser import WinnDixieRecptParser \n",
    "from hidden_layer_param_builder import HiddenLayerParamSetBuilder\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 2000)\n",
    "\n",
    "print(os.getcwd())\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47046089-a2d9-4136-95bb-3888c94aab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_df_to_excel_table(df, file_path, sheet_name=\"Data\"):\n",
    "    \"\"\"\n",
    "    Export a pandas DataFrame to an Excel file as a proper Excel Table\n",
    "    with no duplicated header rows.\n",
    "    \"\"\"\n",
    "    from openpyxl import load_workbook\n",
    "    from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "\n",
    "    df.to_excel(file_path, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    workbook = load_workbook(file_path)\n",
    "    worksheet = workbook[sheet_name]\n",
    "\n",
    "    end_row = worksheet.max_row\n",
    "    end_col = worksheet.max_column\n",
    "    end_col_letter = worksheet.cell(row=1, column=end_col).column_letter\n",
    "\n",
    "    table_ref = f\"A1:{end_col_letter}{end_row}\"\n",
    "    table = Table(displayName=\"DataTable\", ref=table_ref)\n",
    "\n",
    "    style = TableStyleInfo(\n",
    "        name=\"TableStyleMedium9\",\n",
    "        showFirstColumn=False,\n",
    "        showLastColumn=False,\n",
    "        showRowStripes=True,\n",
    "        showColumnStripes=False\n",
    "    )\n",
    "\n",
    "    table.tableStyleInfo = style\n",
    "    worksheet.add_table(table)\n",
    "\n",
    "    workbook.save(file_path)\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalizeAndDropCols(df, cols):\n",
    "    for col in cols:\n",
    "        # Replace the sentinel 999 with NaN so it doesn't distort mean/std\n",
    "        df[col] = df[col].replace(999, np.nan)\n",
    "\n",
    "        # Compute mean/std ignoring NaN\n",
    "        mean = df[col].mean()\n",
    "        std  = df[col].std() or 1.0\n",
    "\n",
    "        # Normalize\n",
    "        df[col + \"_norm\"] = (df[col] - mean) / std\n",
    "\n",
    "        # After normalization: missing values become 0 (neutral)\n",
    "        df[col + \"_norm\"] = df[col + \"_norm\"].fillna(0.0)\n",
    "\n",
    "    return df.drop(columns=cols)\n",
    "\n",
    "\n",
    "#def normalizeAndDropCols(df, cols):\n",
    "#    for col in cols:\n",
    "#        std = df[col].std() or 1.0\n",
    "#        df[col + \"_norm\"] = (df[col] - df[col].mean()) / std\n",
    "#    return df.drop(columns=cols)\n",
    "\n",
    "\n",
    "\n",
    "def canonicalize_items(df, patterns, canonical_name):\n",
    "    \"\"\"\n",
    "    For each pattern in `patterns`, find rows where `item` contains the pattern\n",
    "    and replace df['item'] with `canonical_name`.\n",
    "    \"\"\"\n",
    "    for p in patterns:\n",
    "        mask = df[\"item\"].str.contains(p, case=False, na=False)\n",
    "        df.loc[mask, \"item\"] = canonical_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0fb5f82-f270-4a38-9414-341412e3c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WEATHER PREP ---\n",
    "weatherCols=[\"datetime\", \"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"]\n",
    "df_weather = pd.read_csv(\"datasets/VisualCrossing-70062 2000-01-01 to 2025-12-14.csv\", usecols=weatherCols)\n",
    "\n",
    "df_weather[\"datetime\"] = pd.to_datetime(df_weather[\"datetime\"])\n",
    "df_weather = df_weather.set_index(\"datetime\").sort_index()\n",
    "\n",
    "df_weather[\"temp_5day_avg_feat\"] = df_weather[\"temp\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"feelsLike_5day_avg_feat\"] = df_weather[\"feelslike\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"dew_5day_avg_feat\"] = df_weather[\"dew\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"humidity_5day_avg_feat\"] = df_weather[\"humidity\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"precip_5day_avg_feat\"] = df_weather[\"precip\"].rolling(5, min_periods=1).mean()\n",
    "\n",
    "df_weather = df_weather.drop(columns=[\"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"])\n",
    "\n",
    "# convert index to date for merging\n",
    "df_weather[\"date\"] = df_weather.index.date\n",
    "df_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"])\n",
    "df_weather = df_weather.set_index(\"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8622cefe-d5e0-496d-b547-0cfd003d4d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUP: 2025-08-02 00:00:00 10:29 PM → keep IMG_9693.txt ← drop IMG_9694.txt\n",
      "DUP: 2025-10-07 00:00:00 6:06 PM → keep IMG_0017.txt ← drop IMG_9669.txt\n",
      "DUP: 2025-10-14 00:00:00 4:06 PM → keep IMG_0015.txt ← drop IMG_9667.txt\n",
      "DUP: 2025-10-14 00:00:00 6:08 PM → keep IMG_0014.txt ← drop IMG_9666.txt\n",
      "DUP: 2025-10-17 00:00:00 9:18 PM → keep IMG_0013.txt ← drop IMG_9664.txt\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "recptParser  = WinnDixieRecptParser();\n",
    "\n",
    "for p in Path(\"winndixie rcpts/StevePhone2/pdf/text\").glob(\"*.txt\"):\n",
    "    result = recptParser.parse(p.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "    for r in result[\"items\"]:\n",
    "        rows.append({\n",
    "            \"source\": p.name,\n",
    "            \"date\": result[\"date\"],\n",
    "            \"time\": result[\"time\"],\n",
    "            #\"manager\": result[\"manager\"],\n",
    "            #\"cashier\": result[\"cashier\"],\n",
    "            \"item\": r[\"item\"]\n",
    "            #\"qty\": r[\"qty\"],\n",
    "            #\"reg\": r[\"reg\"],\n",
    "            #\"youPay\": r[\"youPay\"],\n",
    "            #\"reportedItemsSold\": result[\"reported\"],\n",
    "            #\"rowsMatchReported\": result[\"validation\"][\"rowsMatchReported\"],\n",
    "            #\"qtyMatchReported\": result[\"validation\"][\"qtyMatchReported\"],\n",
    "        })\n",
    "\n",
    "winndixie_df = pd.DataFrame(rows)\n",
    "\n",
    "winndixie_df[\"date\"] = pd.to_datetime(winndixie_df[\"date\"])\n",
    "winndixie_df[\"time\"] = winndixie_df[\"time\"].astype(str)\n",
    "\n",
    "winndixie_df = WinnDixieRecptParser.remove_duplicate_receipt_files(winndixie_df)\n",
    "\n",
    "winndixie_df = winndixie_df.sort_values(by=[\"date\", \"time\"]).reset_index(drop=True)\n",
    "winndixie_df = winndixie_df.drop(columns=[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a996e24-1868-4bf5-92e1-266b8d0719c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "wallmart_raw = WallmartRecptParser.ImportWallMart(\"./walmart\")\n",
    "\n",
    "## rename cols\n",
    "wallmart_df = wallmart_raw[[\"Order Date\",\"Product Description\", \"source\"]].copy()\n",
    "wallmart_df = wallmart_df.rename(columns={\n",
    "    \"Order Date\": \"date\",\n",
    "    \"Product Description\": \"item\"\n",
    "})\n",
    "\n",
    "wallmart_df[\"date\"] = pd.to_datetime(wallmart_df[\"date\"])\n",
    "winndixie_df[\"date\"] = pd.to_datetime(winndixie_df[\"date\"])\n",
    "\n",
    "combined_df = pd.concat(\n",
    "    [winndixie_df, wallmart_df[[\"date\", \"item\", \"source\"]]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# remove - \n",
    "combined_df[\"item\"] = (combined_df[\"item\"]\n",
    "        .str.replace(r\"^\\s*[-–—]\\s*\", \"\", regex=True)\n",
    "        .str.strip()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0cd148d-3caa-4967-bb0d-ec482d400ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "milk_patterns = [\"know-and-love-milk\", \"kandl-milk\", \"prairie-farm-milk\",\"kleinpeter-milk\", \"kl-milk\", \"Milk, Fat Free,\", \"Fat-Free Milk\"]\n",
    "canonicalize_items(combined_df, milk_patterns, \"milk\")\n",
    "\n",
    "bread_patterns = [\"bunny-bread\",\"se-grocers-bread\",\"seg-sandwich-bread\", \"seg-white-bread\"]\n",
    "canonicalize_items(combined_df, bread_patterns, \"bread\")\n",
    "\n",
    "cheese_patterns = [\"dandw-cheese\", \"kraft-cheese\", \"se-grocers-cheese\", \"know-and-love-cheese\"]\n",
    "canonicalize_items(combined_df, cheese_patterns, \"cheese\")\n",
    "\n",
    "mayo_patterns = [\"blue-plate-mayo\", \"blue-plate-mynnase\"]\n",
    "canonicalize_items(combined_df, mayo_patterns, \"mayo\")\n",
    "\n",
    "chicken_patterns = [\"chicken-cutlet\", \"chicken-leg\", \"chicken-thigh\", \"chicken-thighs\"]\n",
    "canonicalize_items(combined_df, chicken_patterns, \"chicken\")\n",
    "\n",
    "yogurt_patterns = [\"chobani-yogrt-flip\", \"chobani-yogurt\"]\n",
    "canonicalize_items(combined_df, yogurt_patterns, \"yogurt\")\n",
    "\n",
    "coke_patterns = [\"coca-cola\", \"coca-cola-cola\", \"cocacola-soda\", \"coke\", \"cola\"]\n",
    "canonicalize_items(combined_df, coke_patterns, \"coke\")\n",
    "\n",
    "hugbi_patterns = [\"hugbi-pies\", \"-hugbi-pies\"]\n",
    "canonicalize_items(combined_df, hugbi_patterns, \"hugbi-pies\")\n",
    "\n",
    "ceralPaterns  = [\"ceral\"]\n",
    "canonicalize_items(combined_df, ceralPaterns, \"ceral\")\n",
    "\n",
    "minute_maid_patterns = [\"minute-maid-drink\", \"minute-maid-drinks\", \"minute-maid-lmnade\"]\n",
    "canonicalize_items(combined_df, minute_maid_patterns, \"minute-maid-drink\")\n",
    "\n",
    "eggs_pattern = [\"egglands-best-egg\", \"egglands-best-eggs\", \"eggs\"]\n",
    "canonicalize_items(combined_df, eggs_pattern, \"eggs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a4b591-943b-444d-ae9c-1557b99a282f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataset_utils import DatasetUtils\n",
    "\n",
    "combined_df, id_to_item = DatasetUtils.CreateItemId(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0347749b-3977-40e3-a9a2-79c0f5c1389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Build full receipt × item table WITHOUT using qty\n",
    "# ============================================================\n",
    "\n",
    "# 1. Mark actual purchases in the raw receipt rows\n",
    "combined_df[\"didBuy_target\"] = 1\n",
    "\n",
    "# 2. Build complete grid\n",
    "all_items = combined_df[\"itemId\"].unique()\n",
    "all_dates = combined_df[\"date\"].unique()\n",
    "\n",
    "full = (\n",
    "    pd.MultiIndex.from_product(\n",
    "        [all_dates, all_items], \n",
    "        names=[\"date\", \"itemId\"]\n",
    "    ).to_frame(index=False)\n",
    ")\n",
    "\n",
    "# 3. Merge raw purchases onto the full grid\n",
    "df_full = full.merge(\n",
    "    combined_df[[\"date\", \"itemId\", \"item\", \"source\", \"didBuy_target\"]],\n",
    "    on=[\"date\", \"itemId\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 4. Fill missing purchases with didBuy=0\n",
    "df_full[\"didBuy_target\"] = df_full[\"didBuy_target\"].fillna(0).astype(int)\n",
    "\n",
    "# 5. NOW REPLACE combined_df with df_full\n",
    "combined_df = df_full.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd0664c4-32c3-4379-ba57-71018f599c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 121240 entries, 0 to 121239\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count   Dtype         \n",
      "---  ------                      --------------   -----         \n",
      " 0   date                        121240 non-null  datetime64[ns]\n",
      " 1   itemId                      121240 non-null  int64         \n",
      " 2   item                        1436 non-null    object        \n",
      " 3   source                      1436 non-null    object        \n",
      " 4   didBuy_target               121240 non-null  int32         \n",
      " 5   daysSinceLastTrip_feat      121240 non-null  float64       \n",
      " 6   avgDaysBetweenTrips_feat    121240 non-null  float64       \n",
      " 7   daysUntilNextHoliday_feat   121240 non-null  int64         \n",
      " 8   daysSinceLastHoliday_feat   121240 non-null  int64         \n",
      " 9   holidayProximityIndex_feat  121240 non-null  float64       \n",
      " 10  daysUntilSchoolStart_feat   121240 non-null  int64         \n",
      " 11  daysUntilSchoolEnd_feat     121240 non-null  int64         \n",
      " 12  schoolSeasonIndex_feat      121240 non-null  float64       \n",
      " 13  year_feat                   121240 non-null  int32         \n",
      " 14  month_cyc_feat              121240 non-null  int32         \n",
      " 15  day_cyc_feat                121240 non-null  int32         \n",
      " 16  dow_cyc_feat                121240 non-null  int32         \n",
      " 17  doy_feat                    121240 non-null  int32         \n",
      " 18  quarter_feat                121240 non-null  int32         \n",
      " 19  temp_5day_avg_feat          121240 non-null  float64       \n",
      " 20  feelsLike_5day_avg_feat     121240 non-null  float64       \n",
      " 21  dew_5day_avg_feat           121240 non-null  float64       \n",
      " 22  humidity_5day_avg_feat      121240 non-null  float64       \n",
      " 23  precip_5day_avg_feat        121240 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(9), int32(7), int64(5), object(2)\n",
      "memory usage: 19.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>itemId</th>\n",
       "      <th>item</th>\n",
       "      <th>source</th>\n",
       "      <th>didBuy_target</th>\n",
       "      <th>daysSinceLastTrip_feat</th>\n",
       "      <th>avgDaysBetweenTrips_feat</th>\n",
       "      <th>daysUntilNextHoliday_feat</th>\n",
       "      <th>daysSinceLastHoliday_feat</th>\n",
       "      <th>holidayProximityIndex_feat</th>\n",
       "      <th>daysUntilSchoolStart_feat</th>\n",
       "      <th>daysUntilSchoolEnd_feat</th>\n",
       "      <th>schoolSeasonIndex_feat</th>\n",
       "      <th>year_feat</th>\n",
       "      <th>month_cyc_feat</th>\n",
       "      <th>day_cyc_feat</th>\n",
       "      <th>dow_cyc_feat</th>\n",
       "      <th>doy_feat</th>\n",
       "      <th>quarter_feat</th>\n",
       "      <th>temp_5day_avg_feat</th>\n",
       "      <th>feelsLike_5day_avg_feat</th>\n",
       "      <th>dew_5day_avg_feat</th>\n",
       "      <th>humidity_5day_avg_feat</th>\n",
       "      <th>precip_5day_avg_feat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>695</td>\n",
       "      <td>spaghettios-pasta</td>\n",
       "      <td>IMG_9764.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>439</td>\n",
       "      <td>coke</td>\n",
       "      <td>IMG_9764.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>439</td>\n",
       "      <td>coke</td>\n",
       "      <td>IMG_9764.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>548</td>\n",
       "      <td>mandms-candies</td>\n",
       "      <td>IMG_9764.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>547</td>\n",
       "      <td>mandarins</td>\n",
       "      <td>IMG_9764.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>696</td>\n",
       "      <td>sparkling-ice-wtr</td>\n",
       "      <td>IMG_9764.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>696</td>\n",
       "      <td>sparkling-ice-wtr</td>\n",
       "      <td>IMG_9764.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-11-15</td>\n",
       "      <td>411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>19.764706</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>273</td>\n",
       "      <td>197</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>2024</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>320</td>\n",
       "      <td>4</td>\n",
       "      <td>72.080000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>65.260000</td>\n",
       "      <td>80.580000</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  itemId               item        source  didBuy_target  daysSinceLastTrip_feat  avgDaysBetweenTrips_feat  daysUntilNextHoliday_feat  daysSinceLastHoliday_feat  holidayProximityIndex_feat  daysUntilSchoolStart_feat  daysUntilSchoolEnd_feat  schoolSeasonIndex_feat  year_feat  month_cyc_feat  day_cyc_feat  dow_cyc_feat  doy_feat  quarter_feat  temp_5day_avg_feat  feelsLike_5day_avg_feat  dew_5day_avg_feat  humidity_5day_avg_feat  precip_5day_avg_feat\n",
       "0 2024-11-15     695  spaghettios-pasta  IMG_9764.txt              1               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200\n",
       "1 2024-11-15     439               coke  IMG_9764.txt              1               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200\n",
       "2 2024-11-15     439               coke  IMG_9764.txt              1               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200\n",
       "3 2024-11-15     548     mandms-candies  IMG_9764.txt              1               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200\n",
       "4 2024-11-15     547          mandarins  IMG_9764.txt              1               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200\n",
       "5 2024-11-15     696  sparkling-ice-wtr  IMG_9764.txt              1               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200\n",
       "6 2024-11-15     696  sparkling-ice-wtr  IMG_9764.txt              1               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200\n",
       "7 2024-11-15     491                NaN           NaN              0               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200\n",
       "8 2024-11-15     551                NaN           NaN              0               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200\n",
       "9 2024-11-15     411                NaN           NaN              0               29.000000                 19.764706                         13                          4                   -0.866667                        273                      197                0.460274       2024              11            15             4       320             4           72.080000                72.280000          65.260000               80.580000              0.116200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Build grouped table (one row per trip date)\n",
    "\n",
    "grouped = ( combined_df[[\"date\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"date\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "grouped[\"daysSinceLastTrip_feat\"] = TemporalFeatures.DaysSinceLastTrip(grouped)\n",
    "grouped[\"avgDaysBetweenTrips_feat\"] = TemporalFeatures.AvgDaysBetweenTrips(grouped)\n",
    "\n",
    "# 3. Holiday / School features\n",
    "grouped[\"daysUntilNextHoliday_feat\"] = grouped[\"date\"].apply(HolidayFeatures.daysUntilNextHoliday)\n",
    "grouped[\"daysSinceLastHoliday_feat\"] = grouped[\"date\"].apply(HolidayFeatures.daysSinceLastHoliday)\n",
    "grouped[\"holidayProximityIndex_feat\"] = grouped[\"date\"].apply(HolidayFeatures.holidayProximityIndex)\n",
    "grouped[\"daysUntilSchoolStart_feat\"] = grouped[\"date\"].apply(HolidayFeatures.daysUntilSchoolStart)\n",
    "grouped[\"daysUntilSchoolEnd_feat\"]   = grouped[\"date\"].apply(HolidayFeatures.daysUntilSchoolEnd)\n",
    "grouped[\"schoolSeasonIndex_feat\"]    = grouped[\"date\"].apply(HolidayFeatures.schoolSeasonIndex)\n",
    "\n",
    "\n",
    "grouped = TemporalFeatures.CreateDateFeatures(grouped)\n",
    "\n",
    "# merge in weather\n",
    "grouped = grouped.merge(df_weather, on=\"date\", how=\"left\")\n",
    "\n",
    "combined_df = combined_df.merge(grouped, on=\"date\", how=\"left\")\n",
    "combined_df.info()\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11cf2225-3a20-45fe-810f-624a5e984ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_2520\\3121297430.py:44: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(fill_freq)\n"
     ]
    }
   ],
   "source": [
    " def fill_freq(group):\n",
    "        group = group.copy()\n",
    "        group = group.sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "        history = []\n",
    "    \n",
    "        col_date = group.columns.get_loc(\"date\")\n",
    "        col_buy = group.columns.get_loc(\"didBuy_target\")\n",
    "        col_freq = {w: group.columns.get_loc(f\"freq_{w}_feat\") for w in freq_windows}\n",
    "    \n",
    "        for i in range(len(group)):\n",
    "            cur_date = group.iat[i, col_date]\n",
    "    \n",
    "            # record purchase\n",
    "            if group.iat[i, col_buy] == 1:\n",
    "                history.append(cur_date)\n",
    "    \n",
    "            # prune history ONCE using largest window\n",
    "            cutoff_max = cur_date - pd.Timedelta(days=max_w)\n",
    "            history = [d for d in history if d >= cutoff_max]\n",
    "    \n",
    "            # compute windowed counts\n",
    "            for w in freq_windows:\n",
    "                cutoff = cur_date - pd.Timedelta(days=w)\n",
    "                count = 0\n",
    "                for d in history:\n",
    "                    if d >= cutoff:\n",
    "                        count += 1\n",
    "                group.iat[i, col_freq[w]] = count\n",
    "    \n",
    "        return group\n",
    "####################################################################\n",
    "\n",
    "freq_windows = [7, 15, 30, 90, 365]\n",
    "max_w = max(freq_windows)\n",
    "\n",
    "# initialize columns\n",
    "for w in freq_windows:\n",
    "    combined_df[f\"freq_{w}_feat\"] = np.nan\n",
    "\n",
    "combined_df = (\n",
    "    combined_df\n",
    "    .groupby(\"itemId\", group_keys=False)\n",
    "    .apply(fill_freq)\n",
    ")\n",
    "###########################\n",
    "\n",
    "combined_df[\"freq7_over30_feat\"], combined_df[\"freq30_over365_feat\"]  = TemporalFeatures.compute_freq_ratios(combined_df[\"freq_7_feat\"],combined_df[\"freq_30_feat\"], combined_df[\"freq_365_feat\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24615814-55e6-4528-9f0c-cd58300e71b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df = combined_df.sort_values([\"itemId\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "combined_df[\"daysSinceLastPurchase_feat\"] = np.nan\n",
    "\n",
    "# Track last purchase date per item\n",
    "last_purchase_date = {}\n",
    "\n",
    "for i in range(len(combined_df)):\n",
    "    itemId = combined_df.at[i, \"itemId\"]\n",
    "    current_date = combined_df.at[i, \"date\"]\n",
    "\n",
    "    if itemId in last_purchase_date:\n",
    "        combined_df.at[i, \"daysSinceLastPurchase_feat\"] = (\n",
    "            current_date - last_purchase_date[itemId]\n",
    "        ).days\n",
    "    else:\n",
    "        combined_df.at[i, \"daysSinceLastPurchase_feat\"] = 0  # or 999 if you prefer\n",
    "\n",
    "    if combined_df.at[i, \"didBuy_target\"] == 1:\n",
    "        last_purchase_date[itemId] = current_date\n",
    "\n",
    "\n",
    "####################\n",
    "\n",
    "combined_df = combined_df.sort_values([\"itemId\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Purchase-to-purchase gaps\n",
    "purchase_gap = (\n",
    "    combined_df\n",
    "        .where(combined_df[\"didBuy_target\"] == 1)\n",
    "        .groupby(\"itemId\")[\"date\"]\n",
    "        .diff()\n",
    "        .dt.days\n",
    ")\n",
    "\n",
    "# Expanding (lifetime-so-far) average per item\n",
    "avg_gap = (\n",
    "    purchase_gap\n",
    "        .groupby(combined_df[\"itemId\"])\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    ")\n",
    "\n",
    "# Attach + forward-fill so ALL rows have a value\n",
    "combined_df[\"avgDaysBetweenPurchases_feat\"] = (\n",
    "    avg_gap\n",
    "        .groupby(combined_df[\"itemId\"])\n",
    "        .ffill()\n",
    "        .fillna(0)\n",
    ")\n",
    "\n",
    "#export_df_to_excel_table(combined_df, \"combined_df.xlsx\", \"combined_df\") \n",
    "##combined_df.to_csv(\"combined_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "548e477f-aa81-4d9e-bb31-2acece50cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ITEM-LEVEL HABIT FEATURES (TF-IDF ANALOG)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def build_habit_features(df, tau_days=120):\n",
    "    df = df.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    total_trips = df[\"date\"].nunique()\n",
    "    timeline_days = (df[\"date\"].max() - df[\"date\"].min()).days or 1\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for itemId, g in df.groupby(\"itemId\"):\n",
    "        buys = g[g[\"didBuy_target\"] == 1][\"date\"]\n",
    "\n",
    "        if len(buys) == 0:\n",
    "            rows.append({\n",
    "                \"itemId\": itemId,\n",
    "                \"habitFrequency_feat\": 0.0,\n",
    "                \"habitSpan_feat\": 0.0,\n",
    "                \"habitDecay_feat\": 0.0,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        first = buys.min()\n",
    "        last = buys.max()\n",
    "\n",
    "        habitFrequency = len(buys) / total_trips\n",
    "        habitSpan = (last - first).days / timeline_days\n",
    "        days_since_last = (df[\"date\"].max() - last).days\n",
    "        habitDecay = np.exp(-days_since_last / tau_days)\n",
    "\n",
    "        rows.append({\n",
    "            \"itemId\": itemId,\n",
    "            \"habitFrequency_feat\": habitFrequency,\n",
    "            \"habitSpan_feat\": habitSpan,\n",
    "            \"habitDecay_feat\": habitDecay,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def compute_due_score(df,itemId=None,use_sigmoid=True,normalize=False, weights=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if weights is None:\n",
    "        weights = {\n",
    "            \"daysSinceLastPurchase_feat\": 1.5,\n",
    "            \"freq_30\": 1.0,\n",
    "            \"freq_90\": 0.5\n",
    "        }\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Optional itemId filter\n",
    "    # --------------------------------------------------------\n",
    "    if itemId is not None:\n",
    "        df = df[df[\"itemId\"] == itemId].copy()\n",
    "    else:\n",
    "        df = df.copy()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # RAW linear score (pre-normalization)\n",
    "    # --------------------------------------------------------\n",
    "    df[\"due_score_raw\"] = (\n",
    "        weights[\"daysSinceLastPurchase_feat\"] * df[\"daysSinceLastPurchase_feat\"]\n",
    "      + weights[\"freq_30_feat\"]              * df[\"freq_30_feat\"]\n",
    "      + weights[\"freq_90_feat\"]              * df[\"freq_90_feat\"]\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # Final due_score\n",
    "    # --------------------------------------------------------\n",
    "    if use_sigmoid:\n",
    "        df[\"due_score_feat\"] = 1 / (1 + np.exp(-df[\"due_score_raw\"]))\n",
    "\n",
    "    elif normalize:\n",
    "        mean = df[\"due_score_raw\"].mean()\n",
    "        std  = df[\"due_score_raw\"].std() or 1.0\n",
    "        df[\"due_score\"] = (df[\"due_score_raw\"] - mean) / std\n",
    "\n",
    "    else:\n",
    "        df[\"due_score\"] = df[\"due_score_raw\"]\n",
    "\n",
    "    return df\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MERGE HABIT FEATURES\n",
    "# ============================================================\n",
    "habit_df = build_habit_features(combined_df)\n",
    "\n",
    "combined_df = combined_df.merge(habit_df, on=\"itemId\",how=\"left\")\n",
    "\n",
    "combined_df[[\"habitFrequency_feat\", \"habitSpan_feat\", \"habitDecay_feat\"]] = (\n",
    "    combined_df[[\"habitFrequency_feat\", \"habitSpan_feat\", \"habitDecay_feat\"]].fillna(0.0)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c53ab4d-52a4-4136-8830-ad464c457e43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 121240 entries, 0 to 121239\n",
      "Data columns (total 37 columns):\n",
      " #   Column                        Non-Null Count   Dtype         \n",
      "---  ------                        --------------   -----         \n",
      " 0   date                          121240 non-null  datetime64[ns]\n",
      " 1   itemId                        121240 non-null  int64         \n",
      " 2   item                          1436 non-null    object        \n",
      " 3   source                        1436 non-null    object        \n",
      " 4   didBuy_target                 121240 non-null  int32         \n",
      " 5   daysSinceLastTrip_feat        121240 non-null  float64       \n",
      " 6   avgDaysBetweenTrips_feat      121240 non-null  float64       \n",
      " 7   daysUntilNextHoliday_feat     121240 non-null  int64         \n",
      " 8   daysSinceLastHoliday_feat     121240 non-null  int64         \n",
      " 9   holidayProximityIndex_feat    121240 non-null  float64       \n",
      " 10  daysUntilSchoolStart_feat     121240 non-null  int64         \n",
      " 11  daysUntilSchoolEnd_feat       121240 non-null  int64         \n",
      " 12  schoolSeasonIndex_feat        121240 non-null  float64       \n",
      " 13  year_feat                     121240 non-null  int32         \n",
      " 14  month_cyc_feat                121240 non-null  int32         \n",
      " 15  day_cyc_feat                  121240 non-null  int32         \n",
      " 16  dow_cyc_feat                  121240 non-null  int32         \n",
      " 17  doy_feat                      121240 non-null  int32         \n",
      " 18  quarter_feat                  121240 non-null  int32         \n",
      " 19  temp_5day_avg_feat            121240 non-null  float64       \n",
      " 20  feelsLike_5day_avg_feat       121240 non-null  float64       \n",
      " 21  dew_5day_avg_feat             121240 non-null  float64       \n",
      " 22  humidity_5day_avg_feat        121240 non-null  float64       \n",
      " 23  precip_5day_avg_feat          121240 non-null  float64       \n",
      " 24  freq_7_feat                   121240 non-null  float64       \n",
      " 25  freq_15_feat                  121240 non-null  float64       \n",
      " 26  freq_30_feat                  121240 non-null  float64       \n",
      " 27  freq_90_feat                  121240 non-null  float64       \n",
      " 28  freq_365_feat                 121240 non-null  float64       \n",
      " 29  freq7_over30_feat             121240 non-null  float64       \n",
      " 30  freq30_over365_feat           121240 non-null  float64       \n",
      " 31  daysSinceLastPurchase_feat    121240 non-null  float64       \n",
      " 32  avgDaysBetweenPurchases_feat  121240 non-null  float64       \n",
      " 33  habitFrequency_feat           121240 non-null  float64       \n",
      " 34  habitSpan_feat                121240 non-null  float64       \n",
      " 35  habitDecay_feat               121240 non-null  float64       \n",
      " 36  item_due_ratio_feat           121240 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(22), int32(7), int64(5), object(2)\n",
      "memory usage: 31.0+ MB\n"
     ]
    }
   ],
   "source": [
    "def compute_due_ratio(df, cap=3.0):\n",
    "    ratio = df[\"daysSinceLastPurchase_feat\"] / df[\"avgDaysBetweenPurchases_feat\"]\n",
    "    ratio = ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    return ratio.clip(0, cap)\n",
    "###############################################################################\n",
    "\n",
    "combined_df[\"item_due_ratio_feat\"] = compute_due_ratio(combined_df)\n",
    "\n",
    "#combined_df[\"purchaseToTripRatio\"] = combined_df[\"daysSinceLastPurchase\"] / combined_df[\"avgDaysBetweenPurchases\"]\n",
    "\n",
    "\n",
    "# encoded_df[\"due_score\"] = (\n",
    "#     1.5 * encoded_df[\"daysSinceLastPurchase_norm\"]\n",
    "#   + 1.0 * encoded_df[\"freq_30_norm\"]\n",
    "#   + 0.5 * encoded_df[\"freq_90_norm\"]\n",
    "# )\n",
    "\n",
    "#encoded_df[\"due_score\"] = 1 / (1 + np.exp(-encoded_df[\"due_score\"]))\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5129b-e368-4afd-aace-fca6172f903d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# NORMALIZE \n",
    "# ============================================================\n",
    "\n",
    "\n",
    "freq_cols = [c for c in combined_df.columns if c.startswith(\"freq\")]\n",
    "weather_cols = [c for c in combined_df.columns if c.endswith(\"_5day_avg\")]\n",
    "holiday_cols = [c for c in combined_df.columns if \"holiday\" in c.lower()]\n",
    "school_cols = [c for c in combined_df.columns if \"school\" in c.lower()]\n",
    "\n",
    "daysSince_purchase_cols = [c for c in combined_df.columns if \"days\" in c.lower() and \"purchase\" in c.lower()]\n",
    "daysSince_trip_cols     = [c for c in combined_df.columns if \"days\" in c.lower() and \"trip\" in c.lower()]\n",
    "\n",
    "days_cols = daysSince_purchase_cols + daysSince_trip_cols\n",
    "\n",
    "habit_cols = [\"habitFrequency\", \"habitSpan\", \"habitDecay\"]\n",
    "\n",
    "normalized_df = combined_df.copy()\n",
    "\n",
    "normalized_df = normalizeAndDropCols(normalized_df, [\"item_due_ratio\"])\n",
    "normalized_df = normalizeAndDropCols(normalized_df, freq_cols)\n",
    "normalized_df = normalizeAndDropCols(normalized_df, weather_cols)\n",
    "normalized_df = normalizeAndDropCols(normalized_df, holiday_cols)\n",
    "normalized_df = normalizeAndDropCols(normalized_df, school_cols)\n",
    "normalized_df = normalizeAndDropCols(normalized_df, days_cols)\n",
    "normalized_df = normalizeAndDropCols(normalized_df, habit_cols)\n",
    "\n",
    "# ---------- CYCLICAL FEATURES ----------\n",
    "\n",
    "normalized_df[\"dow_sin\"], normalized_df[\"dow_cos\"] = TemporalFeatures.encode_sin_cos( normalized_df[\"dow\"], 7.0)\n",
    "normalized_df[\"month_sin\"], normalized_df[\"month_cos\"] = TemporalFeatures.encode_sin_cos(normalized_df[\"month\"], 12.0)\n",
    "normalized_df[\"doy_sin\"], normalized_df[\"doy_cos\"] = TemporalFeatures.encode_sin_cos(normalized_df[\"doy\"], 365.0)\n",
    "\n",
    "normalized_df = normalized_df.drop(columns=[\"dow\", \"month\", \"doy\"], errors=\"ignore\")\n",
    "\n",
    "# ---------- NON-CYCLIC TIME FEATURES ----------\n",
    "nonCycCols = [\"year\", \"day\", \"quarter\"]\n",
    "normalized_df = normalizeAndDropCols(normalized_df, nonCycCols)\n",
    "\n",
    "# ---------- DROP NON-MODEL COLS ----------\n",
    "cols_to_drop = [\"source\", \"item\", \"date\"]\n",
    "normalized_df = normalized_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "export_df_to_excel_table(normalized_df, \"normalized_df.xlsx\", \"normalized_df\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d5b52-1522-4fb9-87a4-f72e07812c17",
   "metadata": {},
   "source": [
    "# TRAIN / BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc58c2-b5bc-4754-adab-9c6e6885e017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###############################################\n",
    "\n",
    "def export_df(dataframes, dir):\n",
    "    for name, df in dataframes.items():\n",
    "        csv_path = os.path.join(dir, f\"{name}.csv\")\n",
    "        xlsxPath = os.path.join(dir, f\"{name}.xlsx\")\n",
    "        print(f\"Writing CSV: {csv_path}\")\n",
    "        df.to_csv(csv_path, index=True)\n",
    "        print(f\"Writing XLSX: {xlsxPath}\")\n",
    "        export_df_to_excel_table(df, xlsxPath, sheet_name=f\"{name}\")\n",
    "\n",
    "\n",
    "###############################################\n",
    "\n",
    "def save_experiment(model, history, dataframes, build_params, train_params, feature_cols, item_id_to_idx, base_dir):\n",
    "    name_parts = []\n",
    "\n",
    "   \n",
    "    if \"embedding_dim\" in build_params:\n",
    "        name_parts.append(f\"emb{build_params['embedding_dim']}\")\n",
    "\n",
    "    if \"layers\" in build_params:\n",
    "        hl = \"-\".join(str(x) for x in build_params[\"layers\"])\n",
    "        name_parts.append(f\"hl{hl}\")\n",
    "\n",
    "    if \"epochs\" in train_params:\n",
    "        name_parts.append(f\"ep{train_params['epochs']}\")\n",
    "\n",
    "    if \"output_activation\" in build_params:\n",
    "        name_parts.append(f\"outAct_{build_params['output_activation']}\")\n",
    "\n",
    "    exp_name = \"__\".join(name_parts) if name_parts else \"exp_unlabeled\"\n",
    "    exp_dir = os.path.join(base_dir, exp_name)\n",
    "    print(\"Saving Exp: \", exp_dir)\n",
    "    \n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "    export_df(dataframes, exp_dir)\n",
    "\n",
    "    model.save(os.path.join(exp_dir, \"model\"))\n",
    "    model.save_weights(os.path.join(exp_dir, \"weights.h5\"))\n",
    "\n",
    "    history_path = os.path.join(exp_dir, \"history.json\")\n",
    "    history_file = open(history_path, \"w\")\n",
    "    json.dump(history.history, history_file, indent=2)\n",
    "    history_file.close()\n",
    "\n",
    "    feature_path = os.path.join(exp_dir, \"feature_cols.json\")\n",
    "    feature_file = open(feature_path, \"w\")\n",
    "    json.dump(feature_cols, feature_file, indent=2)\n",
    "    feature_file.close()\n",
    "\n",
    "    item_map_path = os.path.join(exp_dir, \"item_id_to_idx.json\")\n",
    "    item_map_file = open(item_map_path, \"w\")\n",
    "    json.dump({str(int(k)): int(v) for k, v in item_id_to_idx.items()}, item_map_file, indent=2)\n",
    "    item_map_file.close()\n",
    "\n",
    "    build_params_path = os.path.join(exp_dir, \"build_params.json\")\n",
    "    build_params_file = open(build_params_path, \"w\")\n",
    "    json.dump(build_params, build_params_file, indent=2)\n",
    "    build_params_file.close()\n",
    "\n",
    "    train_params_path = os.path.join(exp_dir, \"train_params.json\")\n",
    "    train_params_file = open(train_params_path, \"w\")\n",
    "    json.dump(train_params, train_params_file, indent=2)\n",
    "    train_params_file.close()\n",
    "\n",
    "    print(\"Saved experiment →\", exp_dir)\n",
    "\n",
    "###############################################\n",
    "\n",
    "def build_and_compile_model(featColsCount, itemCount, params):\n",
    "    num_in = layers.Input(shape=(featColsCount,))\n",
    "    item_in = layers.Input(shape=(), dtype=\"int32\")\n",
    "\n",
    "    print(f\"featColsCount is : {featColsCount}\")\n",
    "    \n",
    "    emb = layers.Embedding(\n",
    "        input_dim=itemCount,\n",
    "        output_dim=params[\"embedding_dim\"]\n",
    "    )(item_in)\n",
    "\n",
    "    x = layers.Concatenate()([num_in, layers.Flatten()(emb)])\n",
    "    \n",
    "    for neuronCount in params[\"layers\"]:\n",
    "        x = layers.Dense(neuronCount, activation=params[\"activation\"])(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=params[\"output_activation\"])(x)\n",
    "\n",
    "    model = models.Model([num_in, item_in], out)\n",
    "\n",
    "    optimizer_name = params.get(\"optimizer\", \"adam\")\n",
    "    learning_rate = params.get(\"learning_rate\")\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=params.get(\"loss\", \"mse\"),\n",
    "        metrics=params.get(\"metrics\", [\"mae\"])\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "###############################################\n",
    "\n",
    "def get_feature_cols(encoded_df):\n",
    "    feature_cols = []\n",
    "    for c in encoded_df.columns:\n",
    "        if c == \"due_score\":\n",
    "            continue\n",
    "        if c.endswith(\"_norm\") or c.endswith(\"_sin\") or c.endswith(\"_cos\"):\n",
    "            feature_cols.append(c)\n",
    "    return feature_cols\n",
    "\n",
    "###############################################\n",
    "\n",
    "def train_model(model, encoded_df, feature_cols, params):\n",
    "\n",
    "    featureCols = encoded_df[feature_cols].to_numpy(np.float32)\n",
    "    indexCol = encoded_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "    #targetVar = encoded_df[\"due_score\"].to_numpy(np.float32)\n",
    "    targetVar = encoded_df[\"didBuy\"]\n",
    "\n",
    "    featureCols_tr, featureCols_te, indexCol_tr, indexCol_te, targetVar_tr, targetVar_te = train_test_split(\n",
    "        featureCols, indexCol, targetVar, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        [featureCols_tr, indexCol_tr],\n",
    "        targetVar_tr,\n",
    "        validation_split=.1,\n",
    "        epochs=params[\"epochs\"],\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefa8ad-6e21-473d-8d77-ec42dafb034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "def fit_normalization_params(combined_df):\n",
    "    params = {}\n",
    "    feature_cols = [c for c in combined_df.columns if c.endswith(\"_feat\")]\n",
    "    cyc_cols = [c for c in feature_cols if c.endswith(\"_cyc_feat\")]\n",
    "    num_cols = [c for c in feature_cols if c not in cyc_cols]\n",
    "\n",
    "    for col in num_cols:\n",
    "        params[col] = {\n",
    "            \"mean\": combined_df[col].mean(),\n",
    "            \"std\": combined_df[col].std()\n",
    "        }\n",
    "\n",
    "    for col in cyc_cols:\n",
    "        params[col] = {\n",
    "            \"period\": TemporalFeatures.get_period_for_column(col)\n",
    "        }\n",
    "\n",
    "    return params\n",
    "###############################################\n",
    "\n",
    "\n",
    "###############################################\n",
    "def normalize_features(combined_df, norm_params):\n",
    "    normalized_df = combined_df.copy()\n",
    "\n",
    "    for col, cfg in norm_params.items():\n",
    "\n",
    "        if col.endswith(\"_cyc_feat\"):\n",
    "            sin_col, cos_col = TemporalFeatures.encode_sin_cos(\n",
    "                combined_df[col], cfg[\"period\"]\n",
    "            )\n",
    "            normalized_df[f\"{col}_sin_norm\"] = sin_col\n",
    "            normalized_df[f\"{col}_cos_norm\"] = cos_col\n",
    "            normalized_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "        else:\n",
    "            mean_val = cfg[\"mean\"]\n",
    "            std_val = cfg[\"std\"]\n",
    "            norm_col = col.replace(\"_feat\", \"_norm\")\n",
    "\n",
    "            if std_val == 0:\n",
    "                normalized_df[norm_col] = 0.0\n",
    "            else:\n",
    "                normalized_df[norm_col] = (combined_df[col] - mean_val) / std_val\n",
    "\n",
    "            normalized_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return normalized_df\n",
    "###############################################\n",
    "\n",
    "\n",
    "###############################################\n",
    "def build_and_compile_model(feat_cols_count, item_count, build_params):\n",
    "    num_in = layers.Input(shape=(feat_cols_count,))\n",
    "    item_in = layers.Input(shape=(), dtype=\"int32\")\n",
    "\n",
    "    emb = layers.Embedding(\n",
    "        input_dim=item_count,\n",
    "        output_dim=build_params[\"embedding_dim\"]\n",
    "    )(item_in)\n",
    "\n",
    "    x = layers.Concatenate()([num_in, layers.Flatten()(emb)])\n",
    "\n",
    "    for neuron_count in build_params[\"layers\"]:\n",
    "        x = layers.Dense(neuron_count, activation=build_params[\"activation\"])(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=build_params[\"output_activation\"])(x)\n",
    "\n",
    "    model = models.Model(inputs=[num_in, item_in], outputs=out)\n",
    "\n",
    "    optimizer_name = build_params.get(\"optimizer\", \"adam\")\n",
    "    learning_rate = build_params.get(\"learning_rate\")\n",
    "\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=build_params.get(\"loss\", \"mse\"),\n",
    "        metrics=build_params.get(\"metrics\", [\"mae\"])\n",
    "    )\n",
    "\n",
    "    return model\n",
    "###############################################\n",
    "\n",
    "\n",
    "###############################################\n",
    "def train_model(model, df, feature_cols, target_col, train_params):\n",
    "    x_feat = df[feature_cols].to_numpy(np.float32)\n",
    "    x_item = df[\"itemId\"].to_numpy(np.int32)\n",
    "    y = df[target_col].to_numpy(np.float32)\n",
    "\n",
    "    x_feat_tr, x_feat_te, x_item_tr, x_item_te, y_tr, y_te = train_test_split(\n",
    "        x_feat, x_item, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        [x_feat_tr, x_item_tr],\n",
    "        y_tr,\n",
    "        validation_split=0.1,\n",
    "        epochs=train_params[\"epochs\"],\n",
    "        batch_size=train_params.get(\"batch_size\", 32),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return history\n",
    "###############################################\n",
    "\n",
    "\n",
    "###############################################\n",
    "def build_prediction_input_df(combined_df, prediction_date, norm_params):\n",
    "    latest_rows = (\n",
    "        combined_df.sort_values(\"date\")\n",
    "        .groupby(\"itemId\")\n",
    "        .tail(1)\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    latest_rows[\"date\"] = prediction_date\n",
    "\n",
    "    latest_rows[\"daysSinceLastTrip_feat\"] = TemporalFeatures.daysSinceLastTrip(prediction_date)\n",
    "    latest_rows[\"avgDaysBetweenTrips_feat\"] = TemporalFeatures.avgDaysBetweenTrips(prediction_date)\n",
    "\n",
    "    latest_rows[\"daysUntilNextHoliday_feat\"] = HolidayFeatures.daysUntilNextHoliday(prediction_date)\n",
    "    latest_rows[\"daysSinceLastHoliday_feat\"] = HolidayFeatures.daysSinceLastHoliday(prediction_date)\n",
    "    latest_rows[\"holidayProximityIndex_feat\"] = HolidayFeatures.holidayProximityIndex(prediction_date)\n",
    "\n",
    "    latest_rows[\"daysUntilSchoolStart_feat\"] = SchoolFeatures.daysUntilSchoolStart(prediction_date)\n",
    "    latest_rows[\"daysUntilSchoolEnd_feat\"] = SchoolFeatures.daysUntilSchoolEnd(prediction_date)\n",
    "    latest_rows[\"schoolSeasonIndex_feat\"] = SchoolFeatures.schoolSeasonIndex(prediction_date)\n",
    "\n",
    "    latest_rows[\"year_feat\"] = prediction_date.year\n",
    "    latest_rows[\"month_cyc_feat\"] = prediction_date.month\n",
    "    latest_rows[\"day_cyc_feat\"] = prediction_date.day\n",
    "    latest_rows[\"dow_cyc_feat\"] = prediction_date.weekday()\n",
    "    latest_rows[\"doy_feat\"] = prediction_date.timetuple().tm_yday\n",
    "    latest_rows[\"quarter_feat\"] = ((prediction_date.month - 1) // 3) + 1\n",
    "\n",
    "    for item_id in latest_rows[\"itemId\"].values:\n",
    "        hist = combined_df[combined_df[\"itemId\"] == item_id]\n",
    "\n",
    "        FeatureBuilders.compute_frequency_features(\n",
    "            hist, latest_rows, item_id, prediction_date\n",
    "        )\n",
    "\n",
    "        FeatureBuilders.compute_habit_features(\n",
    "            hist, latest_rows, item_id, prediction_date\n",
    "        )\n",
    "\n",
    "    if \"didBuy_target\" in latest_rows.columns:\n",
    "        latest_rows.drop(columns=[\"didBuy_target\"], inplace=True)\n",
    "\n",
    "    normalized_pred_df = normalize_features(latest_rows, norm_params)\n",
    "\n",
    "    feature_cols = [c for c in normalized_pred_df.columns if c.endswith(\"_norm\")]\n",
    "\n",
    "    x_features = normalized_pred_df[feature_cols].to_numpy(np.float32)\n",
    "    x_item_idx = normalized_pred_df[\"itemId\"].to_numpy(np.int32)\n",
    "\n",
    "    return {\n",
    "        \"prediction_df\": normalized_pred_df,\n",
    "        \"x_features\": x_features,\n",
    "        \"x_item_idx\": x_item_idx,\n",
    "        \"feature_cols\": feature_cols\n",
    "    }\n",
    "###############################################\n",
    "\n",
    "\n",
    "def RunExperiment(combined_df, modelBuildParams, modelTrainParams, baseDir):\n",
    "    norm_params = fit_normalization_params(combined_df)\n",
    "    normalized_df = normalize_features(combined_df, norm_params)\n",
    "\n",
    "    feature_cols = [c for c in normalized_df.columns if c.endswith(\"_norm\")]\n",
    "    target_cols = [c for c in normalized_df.columns if c.endswith(\"_target\")]\n",
    "\n",
    "    if len(target_cols) != 1:\n",
    "        raise ValueError(\"Exactly one target column is required\")\n",
    "\n",
    "    target_col = target_cols[0]\n",
    "\n",
    "    feat_cols_count = len(feature_cols)\n",
    "    item_count = int(normalized_df[\"itemId\"].max()) + 1\n",
    "\n",
    "    model = build_and_compile_model(\n",
    "        feat_cols_count, item_count, modelBuildParams\n",
    "    )\n",
    "\n",
    "    history = train_model(\n",
    "        model, normalized_df, feature_cols, target_col, modelTrainParams\n",
    "    )\n",
    "\n",
    "    pred_input = build_prediction_input_df(\n",
    "        combined_df, normalized_df[\"date\"].max(), norm_params\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(\n",
    "        [pred_input[\"x_features\"], pred_input[\"x_item_idx\"]]\n",
    "    )\n",
    "\n",
    "    prediction_df = pred_input[\"prediction_df\"]\n",
    "    prediction_df[\"prediction\"] = predictions\n",
    "\n",
    "    save_experiment(\n",
    "        model,\n",
    "        history,\n",
    "        [combined_df, normalized_df, prediction_df],\n",
    "        modelBuildParams,\n",
    "        modelTrainParams,\n",
    "        item_id_to_idx=None,\n",
    "        base_dir=baseDir\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"history\": history,\n",
    "        \"normalized_df\": normalized_df,\n",
    "        \"prediction_df\": prediction_df,\n",
    "        \"norm_params\": norm_params\n",
    "    }\n",
    "###############################################\n",
    "\n",
    "\n",
    "def RunPredictionsOnly(combined_df,model_dir,prediction_date):\n",
    "    \"\"\"\n",
    "    Loads a trained model + artifacts and runs predictions only.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.load_model(f\"{model_dir}/model.keras\")\n",
    "\n",
    "    with open(f\"{model_dir}/norm_params.json\", \"r\") as f:\n",
    "        norm_params = json.load(f)\n",
    "\n",
    "    pred_input = build_prediction_input_df(\n",
    "        combined_df=combined_df,\n",
    "        prediction_date=prediction_date,\n",
    "        norm_params=norm_params\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(\n",
    "        [pred_input[\"x_features\"], pred_input[\"x_item_idx\"]]\n",
    "    )\n",
    "\n",
    "    prediction_df = pred_input[\"prediction_df\"].copy()\n",
    "    prediction_df[\"prediction\"] = predictions\n",
    "\n",
    "    return prediction_df\n",
    "###############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79230f4-ae6d-4643-a645-f3adc7bb4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParamsList = []\n",
    "# modelParamsList.append({\n",
    "#     \"trainParams\": {\n",
    "#         \"epochs\": 20\n",
    "#     },\n",
    "#     \"buildParams\": {\n",
    "#         \"embedding_dim\": 32,\n",
    "#         \"layers\": [1024,768,512,256,128,64,32,16,8],\n",
    "#         \"activation\": \"relu\",\n",
    "#         \"output_activation\": \"relu\",\n",
    "#         \"optimizer\": \"adam\",\n",
    "#         \"learning_rate\": 0.01,\n",
    "#         \"loss\": \"mse\",\n",
    "#         \"metrics\": [\"mae\"]\n",
    "#     }\n",
    "# })\n",
    "############\n",
    "modelParamsList.append({\n",
    "    \"trainParams\": {\n",
    "        \"epochs\": 40\n",
    "    },\n",
    "    \"buildParams\": {\n",
    "        \"embedding_dim\": 12,\n",
    "        \"layers\": [1024,512,256,128,64,32],\n",
    "        \"activation\": \"relu\",\n",
    "        \"output_activation\": \"sigmoid\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"loss\": \"mse\",\n",
    "        \"metrics\": [\"mae\"]\n",
    "    }\n",
    "})\n",
    "############\n",
    "\n",
    "modelParamsList.append({\n",
    "    \"trainParams\": {\n",
    "        \"epochs\": 40\n",
    "    },\n",
    "    \"buildParams\": {\n",
    "        \"embedding_dim\": 12,\n",
    "        \"layers\": [1024,512,256,128,64,32],\n",
    "        \"activation\": \"relu\",\n",
    "        \"output_activation\": \"linear\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"loss\": \"mse\",\n",
    "        \"metrics\": [\"mae\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "for modelParams in modelParamsList:\n",
    "    print(f\"{modelParams['buildParams']['layers']}\")\n",
    "    runExp(combined_df, encoded_df, modelParams[\"buildParams\"], modelParams[\"trainParams\"], f\"exp/keras/{ts}\")\n",
    "\n",
    "\n",
    "# baseline = {\n",
    "#     \"trainParams\": {\n",
    "#         \"epochs\": 30\n",
    "#     },\n",
    "#     \"buildParams\": {\n",
    "#         \"embedding_dim\": 64,\n",
    "#         \"layers\": [30],\n",
    "#         \"activation\": \"relu\",\n",
    "#         \"output_activation\": \"sigmoid\",\n",
    "#         \"optimizer\": \"adam\",\n",
    "#         \"learning_rate\": 0.0001,\n",
    "#         \"loss\": \"mse\",\n",
    "#         \"metrics\": [\"mae\"]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# paramList = HiddenLayerParamSetBuilder.BuildHiddenLayerSizeSets(baseline, 30, 20, 4096)\n",
    "\n",
    "# ts = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "# for modelParams in paramList:\n",
    "#     print(f\"{modelParams['buildParams']['layers']}\")\n",
    "#     runExp(combined_df, encoded_df, modelParams[\"buildParams\"], modelParams[\"trainParams\"], f\"exp/keras/{ts}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568b004-172d-416d-91df-f1b435d022a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "runPredictionOnly(\n",
    "    \"exp\\\\keras\\\\nuerons_sizes\\\\emb32__hl100__ep40\",\n",
    "    combined_df = combined_df,\n",
    "    encoded_df = encoded_df,\n",
    "    predict_date = \"12-19-2025\",\n",
    "    baseDir = \"pred\"\n",
    ")\n",
    "\n",
    "runPredictionOnly(\n",
    "    \"exp\\\\keras\\\\nuerons_sizes\\\\emb32__hl100__ep40\",\n",
    "    combined_df = combined_df,\n",
    "    encoded_df = encoded_df,\n",
    "    predict_date = \"12-20-2025\",\n",
    "    baseDir = \"pred\"\n",
    ")\n",
    "\n",
    "# runPredictionOnly(\n",
    "#     \"exp\\\\keras\\\\nuerons_sizes\\\\emb32__hl100__ep40\",\n",
    "#     combined_df = combined_df,\n",
    "#     encoded_df = encoded_df,\n",
    "#     predict_date = \"12/21/2025\",\n",
    "#     baseDir = \"pred\"\n",
    "# )\n",
    "\n",
    "# runPredictionOnly(\n",
    "#     \"exp\\\\keras\\\\nuerons_sizes\\\\emb32__hl100__ep40\",\n",
    "#     combined_df = combined_df,\n",
    "#     encoded_df = encoded_df,\n",
    "#     predict_date = \"12/22/2025\",\n",
    "#     baseDir = \"pred\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb629f9a-ee07-49bd-acd8-6251947e90b0",
   "metadata": {},
   "source": [
    "# OLD OLD OLD OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ffa6f-2e04-4692-8ba8-602cf8047147",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def build_prediction_df(encoded_df, combined_df, predict_date):\n",
    "\n",
    "#     print(\"Building Pred DF\")\n",
    "\n",
    "#     if predict_date is None:\n",
    "#         predict_date = pd.Timestamp.today().normalize()\n",
    "#     else:\n",
    "#         predict_date = pd.to_datetime(predict_date).normalize()\n",
    "\n",
    "   \n",
    "\n",
    "# ###############################################\n",
    "\n",
    "# def run_predictions(model, encoded_df, combined_df, feature_cols, predict_date=None):\n",
    "\n",
    "#     pred_df = build_prediction_df(encoded_df, combined_df, predict_date)\n",
    "#     pred_df.info()\n",
    "    \n",
    "#     print(\"Running predicitons\")\n",
    "#     featureCols = pred_df[feature_cols].to_numpy(np.float32)\n",
    "#     indexCol = pred_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "\n",
    "#     scores = model.predict([featureCols, indexCol], verbose=0).ravel()\n",
    "#     pred_df[\"due_intensity\"] = scores\n",
    "\n",
    "#     return pred_df.sort_values(\"due_intensity\", ascending=False).reset_index(drop=True)    \n",
    "\n",
    "# ###############################################\n",
    "\n",
    "# def runExp(combined_df, encoded_df, buildParams, trainParams, baseDir, tripDate=None):\n",
    "#     item_ids = sorted(encoded_df[\"itemId\"].unique())\n",
    "#     item_id_to_idx = {iid: i for i, iid in enumerate(item_ids)}\n",
    "#     encoded_df[\"itemIdx\"] = encoded_df[\"itemId\"].map(item_id_to_idx).astype(\"int32\")\n",
    "\n",
    "#     print(\"Running Exp....\")\n",
    "#     feature_cols = get_feature_cols(encoded_df)\n",
    "\n",
    "#     model = build_and_compile_model(len(feature_cols), len(item_ids), buildParams)\n",
    "#     history = train_model(model, encoded_df, feature_cols, trainParams)\n",
    "\n",
    "#     predictions_df = run_predictions( model, encoded_df, combined_df, feature_cols, predict_date=tripDate)\n",
    "\n",
    "#     dataframes = {\n",
    "#         \"predictions\": predictions_df,\n",
    "#         \"encoded_features\": encoded_df,\n",
    "#         \"combined_df\": combined_df\n",
    "#     }\n",
    "\n",
    "#     save_experiment(model, history, dataframes, buildParams, trainParams, feature_cols, item_id_to_idx, base_dir=baseDir)\n",
    "\n",
    "# ###############################################\n",
    "\n",
    "# def runPredictionOnly(modelDir, combined_df, encoded_df, predict_date, baseDir):\n",
    "\n",
    "#     model = tf.keras.models.load_model(os.path.join(modelDir, \"model\"))\n",
    "#     feature_cols = get_feature_cols(encoded_df)\n",
    "#     item_map_path = os.path.join(modelDir, \"item_id_to_idx.json\")\n",
    "#     with open(item_map_path, \"r\") as f:\n",
    "#         item_id_to_idx = {int(k): int(v) for k, v in json.load(f).items()}\n",
    "#     encoded_df = encoded_df.copy()\n",
    "#     encoded_df[\"itemIdx\"] = encoded_df[\"itemId\"].map(item_id_to_idx).astype(\"int32\")\n",
    "    \n",
    "#     predictions = run_predictions(model,encoded_df,combined_df,feature_cols,predict_date=predict_date)\n",
    "\n",
    "#     #exp_dir = os.path.join(baseDir, f\"predict_{predict_date}\")\n",
    "#     predict_date_str = pd.to_datetime(predict_date).strftime(\"%Y-%m-%d\")\n",
    "#     exp_dir = os.path.join(baseDir, f\"predict_{predict_date_str}\")\n",
    "#     os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "#     predictions.to_csv(os.path.join(exp_dir, \"predictions.csv\"), index=False)\n",
    "\n",
    "#     print(\"Saved prediction →\", exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8bc72b-8687-449b-9b06-6ea14a86af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# def build_and_compile_model(featColsCount, itemCount, params):\n",
    "#     num_in = layers.Input(shape=(featColsCount,))\n",
    "#     item_in = layers.Input(shape=(), dtype=\"int32\")\n",
    "\n",
    "#     emb = layers.Embedding(\n",
    "#         input_dim=itemCount,\n",
    "#         output_dim=params[\"embedding_dim\"]\n",
    "#     )(item_in)\n",
    "\n",
    "#     x = layers.Concatenate()([num_in, layers.Flatten()(emb)])\n",
    "\n",
    "#     for units in params[\"hiddenLayers\"]:\n",
    "#         x = layers.Dense(units, activation=\"relu\")(x)\n",
    "\n",
    "#     out = layers.Dense(\n",
    "#         1,\n",
    "#         activation=params.get(\"output_activation\", \"sigmoid\")\n",
    "#     )(x)\n",
    "\n",
    "#     model = models.Model([num_in, item_in], out)\n",
    "\n",
    "#     optimizer = tf.keras.optimizers.Adam(\n",
    "#         learning_rate=params.get(\"learning_rate\", 0.001)\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=params.get(\"loss\", \"mse\"),\n",
    "#         metrics=params.get(\"metrics\", [\"mae\"])\n",
    "#     )\n",
    "\n",
    "#     return model\n",
    "# ##########################################################################################\n",
    "\n",
    "# def get_feature_cols(encoded_df):\n",
    "#     feature_cols = []\n",
    "#     for c in encoded_df.columns:\n",
    "#         if c == \"due_score\":\n",
    "#             continue\n",
    "#         if c.endswith(\"_norm\") or c.endswith(\"_sin\") or c.endswith(\"_cos\"):\n",
    "#             feature_cols.append(c)\n",
    "#     return feature_cols\n",
    "# ##########################################################################################\n",
    "\n",
    "# def train_model(model, encoded_df, feature_cols, params):\n",
    "\n",
    "#     featureCols = encoded_df[feature_cols].to_numpy(np.float32)\n",
    "#     indexCol = encoded_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "#     targetVar = encoded_df[\"due_score\"].to_numpy(np.float32)\n",
    "\n",
    "#     featureCols_tr, featureCols_te, indexCol_tr, indexCol_te, targetVar_tr, targetVar_te = train_test_split(\n",
    "#         featureCols, indexCol, targetVar, test_size=0.2, random_state=42\n",
    "#     )\n",
    "\n",
    "#     history = model.fit(\n",
    "#         [featureCols_tr, indexCol_tr],\n",
    "#         targetVar_tr,\n",
    "#         validation_split=params[\"validation_split\"],\n",
    "#         epochs=params[\"epochs\"],\n",
    "#         batch_size=params[\"batch_size\"],\n",
    "#         verbose=1\n",
    "#     )\n",
    "\n",
    "#     return history\n",
    "# ##########################################################################################\n",
    "\n",
    "# def build_prediction_df(encoded_df, combined_df, predict_date):\n",
    "\n",
    "#     if predict_date is None:\n",
    "#         predict_date = pd.Timestamp.today().normalize()\n",
    "#     else:\n",
    "#         predict_date = pd.to_datetime(predict_date).normalize()\n",
    "\n",
    "#     last_trip_date_by_item = (combined_df.sort_values(\"date\").groupby(\"itemId\")[\"date\"].last())\n",
    "\n",
    "#     last_purchase_date_by_item = (combined_df[combined_df[\"didBuy\"] == 1].sort_values(\"date\").groupby(\"itemId\")[\"date\"].last() )\n",
    "\n",
    "#     item_lookup = (combined_df[[\"itemId\", \"item\"]].drop_duplicates().set_index(\"itemId\")[\"item\"].to_dict())\n",
    "\n",
    "#     rows = []\n",
    "\n",
    "#     for itemId, hist in encoded_df.groupby(\"itemId\"):\n",
    "#         last = hist.iloc[-1].copy()\n",
    "#         row = last.to_dict()\n",
    "\n",
    "#         row[\"itemId\"] = itemId\n",
    "#         row[\"item\"] = item_lookup.get(itemId, \"UNKNOWN\")\n",
    "\n",
    "#         last_trip_date = pd.to_datetime(last_trip_date_by_item.loc[itemId]).normalize()\n",
    "#         row[\"daysSinceLastTrip_norm\"] = (predict_date - last_trip_date).days\n",
    "\n",
    "#         if itemId in last_purchase_date_by_item.index:\n",
    "#             last_purchase_date = pd.to_datetime(last_purchase_date_by_item.loc[itemId]).normalize()\n",
    "#             row[\"daysSinceLastPurchase_norm\"] = (predict_date - last_purchase_date).days\n",
    "\n",
    "#         row[\"daysUntilNextHoliday_norm\"] = HolidayFeatures.daysUntilNextHoliday(predict_date)\n",
    "#         row[\"daysSinceLastHoliday_norm\"] = HolidayFeatures.daysSinceLastHoliday(predict_date)\n",
    "#         row[\"holidayProximityIndex_norm\"] = HolidayFeatures.holidayProximityIndex(predict_date)\n",
    "#         row[\"daysUntilSchoolStart_norm\"] = HolidayFeatures.daysUntilSchoolStart(predict_date)\n",
    "#         row[\"daysUntilSchoolEnd_norm\"] = HolidayFeatures.daysUntilSchoolEnd(predict_date)\n",
    "#         row[\"schoolSeasonIndex_norm\"] = HolidayFeatures.schoolSeasonIndex(predict_date)\n",
    "\n",
    "#         row[\"year_norm\"] = float(predict_date.year)\n",
    "#         row[\"day_norm\"] = float(predict_date.day)\n",
    "#         row[\"quarter_norm\"] = float(predict_date.quarter)\n",
    "\n",
    "#         row[\"dow_sin\"], row[\"dow_cos\"] = TemporalFeatures.encode_sin_cos(predict_date.weekday(), 7)\n",
    "#         row[\"month_sin\"], row[\"month_cos\"] = TemporalFeatures.encode_sin_cos(predict_date.month, 12)\n",
    "#         row[\"doy_sin\"], row[\"doy_cos\"] = TemporalFeatures.encode_sin_cos(predict_date.dayofyear, 365)\n",
    "\n",
    "#         rows.append(row)\n",
    "\n",
    "#     return pd.DataFrame(rows)\n",
    "# ##########################################################################################\n",
    "\n",
    "# def run_predictions(model, encoded_df, combined_df, feature_cols, predict_date=None):\n",
    "\n",
    "#     pred_df = build_prediction_df(encoded_df, combined_df, predict_date)\n",
    "\n",
    "#     featureCols = pred_df[feature_cols].to_numpy(np.float32)\n",
    "#     indexCol = pred_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "\n",
    "#     scores = model.predict([featureCols, indexCol], verbose=0).ravel()\n",
    "#     pred_df[\"due_intensity\"] = scores\n",
    "\n",
    "#     return (\n",
    "#         pred_df[[\"itemId\", \"item\", \"due_intensity\"]]\n",
    "#         .sort_values(\"due_intensity\", ascending=False)\n",
    "#         .reset_index(drop=True)\n",
    "#     )\n",
    "# ##########################################################################################\n",
    "\n",
    "# def runExp(feature_stats, combined_df, encoded_df, buildParams, trainParams, baseDir, tripDate=None):\n",
    "#     #\n",
    "#     item_ids = sorted(encoded_df[\"itemId\"].unique())\n",
    "#     item_id_to_idx = {iid: i for i, iid in enumerate(item_ids)}\n",
    "#     encoded_df[\"itemIdx\"] = encoded_df[\"itemId\"].map(item_id_to_idx).astype(\"int32\")\n",
    "#     itemCount = len(item_ids)\n",
    "#     #\n",
    "#     numeric_cols = [\n",
    "#         c for c in encoded_df.columns\n",
    "#         if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "#     ]\n",
    "#     featColsCount = len(numeric_cols)\n",
    "#     #\n",
    "#     model = build_and_compile_model(featColsCount, itemCount, buildParams)\n",
    "#     #\n",
    "#     history = train_model(model, encoded_df, trainParams)\n",
    "#     #\n",
    "#     predictions = run_predictions(\n",
    "#         model,\n",
    "#         encoded_df,\n",
    "#         combined_df,\n",
    "#         feature_stats,\n",
    "#         predict_date=tripDate\n",
    "#     )\n",
    "#     #\n",
    "#     dataframes = {\n",
    "#         \"predictions\": predictions,\n",
    "#         \"encoded_features\": encoded_df,\n",
    "#         \"combined_df\": combined_df\n",
    "#     }\n",
    "#     save_experiment(model,history, dataframes, buildParams,trainParams,\n",
    "#         numeric_cols,\n",
    "#         item_id_to_idx,\n",
    "#         base_dir=baseDir\n",
    "#     )\n",
    "# ####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9098d1-6323-4af8-8647-aa06ba56f79b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ---------------- ENTRY POINT ----------------\n",
    "# trainParams = {\n",
    "#     \"loss\": \"mse\",\n",
    "#     \"optimizer\": \"adam\",\n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     \"metrics\": [\"mae\"],\n",
    "#     \"epochs\": 40,\n",
    "#     \"batch_size\": 32,\n",
    "#     \"validation_split\": 0.1\n",
    "# }\n",
    "\n",
    "# buildParams_neurons_sigmoid = {\n",
    "#     \"embedding_dim\": 32,\n",
    "#     \"hiddenLayers\": [1],\n",
    "#     \"output_activation\": \"sigmoid\"\n",
    "# }\n",
    "\n",
    "# paramSets = HiddenLayerParamSetBuilder.BuildHiddenLayerSizeSets(buildParams_neurons_sigmoid, 10, 5, 512)\n",
    "\n",
    "# for eachBuildParams in paramSets:\n",
    "#     print(f\"{eachBuildParams['hiddenLayers']}\")\n",
    "#     runExp(combined_df, encoded_df, eachBuildParams, trainParams, \"exp/keras/nuerons_sizes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75bb34-193a-47ba-8e96-db8eaf6a372c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def export_df(dataframes, dir):\n",
    "#     for name, df in dataframes.items():\n",
    "#         csv_path = os.path.join(dir, f\"{name}.csv\")\n",
    "#         df.to_csv(csv_path, index=True)\n",
    "# #\n",
    "# def build_and_compile_model(featColsCount, itemCount, params):\n",
    "#     num_in = layers.Input(shape=(featColsCount,))\n",
    "#     item_in = layers.Input(shape=(), dtype=\"int32\")\n",
    "\n",
    "#     emb = layers.Embedding(\n",
    "#         input_dim=itemCount,\n",
    "#         output_dim=params[\"embedding_dim\"]\n",
    "#     )(item_in)\n",
    "\n",
    "#     x = layers.Concatenate()([num_in, layers.Flatten()(emb)])\n",
    "\n",
    "#     for units in params[\"hiddenLayers\"]:\n",
    "#         x = layers.Dense(units, activation=\"relu\")(x)\n",
    "\n",
    "#     out = layers.Dense(\n",
    "#         1,\n",
    "#         activation=params.get(\"output_activation\", \"sigmoid\")\n",
    "#     )(x)\n",
    "\n",
    "#     model = models.Model([num_in, item_in], out)\n",
    "\n",
    "#     optimizer_name = params.get(\"optimizer\", \"adam\")\n",
    "#     learning_rate = params.get(\"learning_rate\", 0.001)\n",
    "\n",
    "#     if optimizer_name == \"adam\":\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#     else:\n",
    "#         optimizer = optimizer_name\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=params.get(\"loss\", \"mse\"),\n",
    "#         metrics=params.get(\"metrics\", [\"mae\"])\n",
    "#     )\n",
    "\n",
    "#     return model\n",
    "# ##########################################################################################\n",
    "\n",
    "# def train_model(model, encoded_df, params):\n",
    "   \n",
    "#     numeric_cols = [\n",
    "#         c for c in encoded_df.columns\n",
    "#         if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "#     ]\n",
    "\n",
    "#     featureCols = encoded_df[numeric_cols].to_numpy(np.float32)\n",
    "#     indexCol = encoded_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "#     targetVar  = encoded_df[\"due_score\"].to_numpy(np.float32)\n",
    "\n",
    "#     featuresCols_train, featuresCols_test, indexCol_train, indexCol_test, targetVar_tr, targetVar_te = train_test_split(featureCols, indexCol, targetVar, test_size=0.2, random_state=42)\n",
    "\n",
    "#     history = model.fit(\n",
    "#         [featuresCols_train, indexCol_train],\n",
    "#         targetVar_tr,\n",
    "#         validation_split=0.1,\n",
    "#         epochs=params[\"epochs\"],\n",
    "#         batch_size=32,\n",
    "#         verbose=1\n",
    "#     )\n",
    "\n",
    "#     return history\n",
    "# ##########################################################################################\n",
    "\n",
    "# def build_prediction_df(encoded_df, combined_df, feature_stats, predict_date):\n",
    "\n",
    "#     numeric_cols = [\n",
    "#         c for c in encoded_df.columns\n",
    "#         if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "#     ]\n",
    "\n",
    "#     last_trip_date_by_item = (combined_df.sort_values(\"date\").groupby(\"itemId\")[\"date\"].last())\n",
    "\n",
    "#     last_purchase_date_by_item = (combined_df[combined_df[\"didBuy\"] == 1].sort_values(\"date\").groupby(\"itemId\")[\"date\"].last())\n",
    "\n",
    "#     item_lookup = (combined_df[[\"itemId\", \"item\"]].drop_duplicates().set_index(\"itemId\")[\"item\"].to_dict())\n",
    "\n",
    "#     rows = []\n",
    "\n",
    "#     for itemId, hist in encoded_df.groupby(\"itemId\"):\n",
    "#         last = hist.iloc[-1]\n",
    "\n",
    "#         last_trip_date = pd.to_datetime(\n",
    "#             last_trip_date_by_item.loc[itemId]\n",
    "#         ).normalize()\n",
    "\n",
    "#         last_purchase_date = pd.to_datetime(\n",
    "#             last_purchase_date_by_item.loc[itemId]\n",
    "#         ).normalize()\n",
    "\n",
    "#         row = {\n",
    "#             \"itemId\": itemId,\n",
    "#             \"item\": item_lookup.get(itemId, \"UNKNOWN\"),\n",
    "#             \"itemIdx\": int(last[\"itemIdx\"])\n",
    "#         }\n",
    "\n",
    "#         # carry-forward observed features\n",
    "#         for col in numeric_cols:\n",
    "#             if (\n",
    "#                 \"temp_\" in col or\n",
    "#                 \"feelsLike_\" in col or\n",
    "#                 \"dew_\" in col or\n",
    "#                 \"humidity_\" in col or\n",
    "#                 \"precip_\" in col or\n",
    "#                 \"freq_\" in col or\n",
    "#                 \"habit\" in col or\n",
    "#                 \"avgDaysBetweenPurchases\" in col\n",
    "#             ):\n",
    "#                 row[col] = last[col]\n",
    "\n",
    "#         raw_updates = {\n",
    "#             \"daysSinceLastTrip\": (predict_date - last_trip_date).days,\n",
    "#             \"daysSinceLastPurchase\": (predict_date - last_purchase_date).days,\n",
    "#             \"daysUntilNextHoliday\": HolidayFeatures.daysUntilNextHoliday(predict_date),\n",
    "#             \"daysSinceLastHoliday\": HolidayFeatures.daysSinceLastHoliday(predict_date),\n",
    "#             \"holidayProximityIndex\": HolidayFeatures.holidayProximityIndex(predict_date),\n",
    "#             \"daysUntilSchoolStart\": HolidayFeatures.daysUntilSchoolStart(predict_date),\n",
    "#             \"daysUntilSchoolEnd\": HolidayFeatures.daysUntilSchoolEnd(predict_date),\n",
    "#             \"schoolSeasonIndex\": HolidayFeatures.schoolSeasonIndex(predict_date),\n",
    "#             \"year\": predict_date.year,\n",
    "#             \"day\": predict_date.day,\n",
    "#             \"quarter\": predict_date.quarter,\n",
    "#             \"dow_sin\": np.sin(2 * np.pi * predict_date.weekday() / 7),\n",
    "#             \"dow_cos\": np.cos(2 * np.pi * predict_date.weekday() / 7),\n",
    "#             \"month_sin\": np.sin(2 * np.pi * predict_date.month / 12),\n",
    "#             \"month_cos\": np.cos(2 * np.pi * predict_date.month / 12),\n",
    "#             \"doy_sin\": np.sin(2 * np.pi * predict_date.dayofyear / 365),\n",
    "#             \"doy_cos\": np.cos(2 * np.pi * predict_date.dayofyear / 365),\n",
    "#         }\n",
    "\n",
    "#         for raw, val in raw_updates.items():\n",
    "#             norm_col = raw + \"_norm\"\n",
    "#             if norm_col in numeric_cols and raw in feature_stats:\n",
    "#                 stats = feature_stats[raw]\n",
    "#                 row[norm_col] = (val - stats[\"mean\"]) / stats[\"std\"]\n",
    "\n",
    "#         rows.append(row)\n",
    "\n",
    "#     return pd.DataFrame(rows)\n",
    "#     ######################################################################################\n",
    "    \n",
    "# def run_predictions(model, encoded_df, combined_df, feature_stats, predict_date=None):\n",
    "\n",
    "#         if predict_date is None:\n",
    "#             predict_date = pd.Timestamp.today().normalize()\n",
    "#         else:\n",
    "#             predict_date = pd.to_datetime(predict_date).normalize()\n",
    "    \n",
    "#         pred_df = build_prediction_df(\n",
    "#             encoded_df, combined_df, feature_stats, predict_date\n",
    "#         )\n",
    "    \n",
    "#         numeric_cols = [\n",
    "#             c for c in pred_df.columns\n",
    "#             if c.endswith(\"_norm\")\n",
    "#         ]\n",
    "    \n",
    "#         featureCols = pred_df[numeric_cols].to_numpy(np.float32)\n",
    "#         indexCol = pred_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "    \n",
    "#         scores = model.predict([featureCols, indexCol], verbose=0).ravel()\n",
    "#         pred_df[\"due_intensity\"] = scores\n",
    "    \n",
    "#         return (\n",
    "#             pred_df[[\"itemId\", \"item\", \"due_intensity\"]]\n",
    "#             .sort_values(\"due_intensity\", ascending=False)\n",
    "#             .reset_index(drop=True)\n",
    "#         )\n",
    "# #####################################################################################\n",
    "\n",
    "\n",
    "# def BuildParamSets( baseline_params, property_name, start, step, stop):\n",
    "#     \"\"\"\n",
    "#     Creates multiple fully independent parameter dictionaries by varying one property.\n",
    "#     Each iteration produces a brand-new baseline object.\n",
    "#     \"\"\"\n",
    "#     import copy\n",
    "#     results = []\n",
    "\n",
    "#     value = start\n",
    "#     while value <= stop:\n",
    "#         params_copy = copy.deepcopy(baseline_params)\n",
    "#         params_copy[property_name] = value\n",
    "#         results.append(params_copy)\n",
    "#         value += step\n",
    "\n",
    "#     return results\n",
    "# ###############################################################################\n",
    "\n",
    "# def runExp(feature_stats, combined_df, encoded_df, buildParams, trainParams, baseDir):\n",
    "#     #\n",
    "#     # item index\n",
    "#     item_ids = sorted(encoded_df[\"itemId\"].unique())\n",
    "#     item_id_to_idx = {iid: i for i, iid in enumerate(item_ids)}\n",
    "#     encoded_df[\"itemIdx\"] = encoded_df[\"itemId\"].map(item_id_to_idx).astype(\"int32\")\n",
    "#     itemCount = len(item_ids)\n",
    "#     #\n",
    "#     numeric_cols = [\n",
    "#         c for c in encoded_df.columns\n",
    "#         if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "#     ]\n",
    "#     featColsCount = len(numeric_cols)\n",
    "#     #\n",
    "#     model = build_and_compile_model(featColsCount, itemCount, buildParams)\n",
    "#     #\n",
    "#     history = train_model(model, encoded_df, trainParams)\n",
    "#     #\n",
    "              \n",
    "#     predictions = run_predictions(model, encoded_df, combined_df, feature_stats)\n",
    "#     # \n",
    "#     dataframes = {\n",
    "#         \"predictions\": predictions,\n",
    "#         \"encoded_features\": encoded_df,\n",
    "#         \"combined_df\": combined_df\n",
    "#     }\n",
    "#     save_experiment(model, history, dataframes, buildParams, trainParams, numeric_cols, item_id_to_idx, base_dir= baseDir)\n",
    "# ################################################################################################################################\n",
    "\n",
    "# import multiprocessing as mp\n",
    "# def run_param_sets_multiprocess(buildParamsSets, trainParams, max_parallel, feature_stats, combined_df,encoded_df, baseDir ):\n",
    "#     #\n",
    "#     processes = []\n",
    "\n",
    "#     for buildParams in buildParamsSets:\n",
    "#         p = mp.Process(\n",
    "#             target=runExp,\n",
    "#             args=(feature_stats, combined_df, encoded_df, buildParams, trainParams, baseDir)\n",
    "#         )\n",
    "#         p.start()\n",
    "#         processes.append(p)\n",
    "\n",
    "#         # limit concurrency\n",
    "#         if len(processes) >= max_parallel:\n",
    "#             for proc in processes:\n",
    "#                 proc.join()\n",
    "#             processes = []\n",
    "\n",
    "#     # wait for remaining\n",
    "#     for proc in processes:\n",
    "#         proc.join()\n",
    "# ################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf905ec1-d0c8-4467-ae6f-ae1382e183a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# buildParams_embeddingsTest = {\n",
    "#     \"embedding_dim\": 1,\n",
    "#     \"hiddenLayers\": [512],\n",
    "#     \"output_activation\": \"sigmoid\"\n",
    "# }\n",
    "\n",
    "# # buildParams_embeddingsTest_relu = {\n",
    "# #     \"embedding_dim\": 1,\n",
    "# #     \"hiddenLayers\": [1024],\n",
    "# #     \"output_activation\": \"relu\"\n",
    "# # }\n",
    "\n",
    "\n",
    "# trainParams = {\n",
    "#     \"loss\": \"mse\",\n",
    "#     \"optimizer\": \"adam\",\n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     \"metrics\": [\"mae\"],\n",
    "#     \"epochs\": 40,\n",
    "#     \"batch_size\": 32,\n",
    "#     \"validation_split\": 0.1\n",
    "# }\n",
    "\n",
    "# # build sets\n",
    "# paramSets = BuildParamSets(buildParams_embeddingsTest, \"embedding_dim\", 33, 2, 64)\n",
    "# # run\n",
    "# run_param_sets_multiprocess(paramSets, trainParams, 4, feature_stats, combined_df,encoded_df, \"exp_mp\")\n",
    "# #paramSets_embeddingeTest_relu = BuildParamSets(buildParams_embeddingsTest_relu, \"embedding_dim\", 1, 2, 32)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d87fa-809d-464e-86e2-dd1356bd3569",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def run_predictions( model, encoded_df, combined_df, feature_stats, predict_date=None):\n",
    "#     \"\"\"\n",
    "#     Build one prediction row per item using:\n",
    "#     - latest encoded feature state (encoded_df)\n",
    "#     - raw timeline + names (combined_df)\n",
    "#     - recomputed calendar features at predict_date\n",
    "#     \"\"\"\n",
    "\n",
    "#     if predict_date is None:\n",
    "#         predict_date = pd.Timestamp.today().normalize()\n",
    "#     else:\n",
    "#         predict_date = pd.to_datetime(predict_date).normalize()\n",
    "\n",
    "#     # --------------------------------------------------------\n",
    "#     # Discover numeric features (single source: encoded_df)\n",
    "#     # --------------------------------------------------------\n",
    "#     numeric_cols = [\n",
    "#         c for c in encoded_df.columns\n",
    "#         if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "#     ]\n",
    "\n",
    "#     # --------------------------------------------------------\n",
    "#     # Lookups from combined_df (single source of truth)\n",
    "#     # --------------------------------------------------------\n",
    "#     last_date_by_item = (\n",
    "#         combined_df\n",
    "#         .sort_values(\"date\")\n",
    "#         .groupby(\"itemId\")[\"date\"]\n",
    "#         .last()\n",
    "#     )\n",
    "\n",
    "#     item_lookup = (\n",
    "#         combined_df[[\"itemId\", \"item\"]]\n",
    "#         .drop_duplicates()\n",
    "#         .set_index(\"itemId\")[\"item\"]\n",
    "#         .to_dict()\n",
    "#     )\n",
    "\n",
    "#     rows = []\n",
    "\n",
    "#     for itemId, hist in encoded_df.groupby(\"itemId\"):\n",
    "#         last = hist.iloc[-1]\n",
    "#         last_date = pd.to_datetime(last_date_by_item.loc[itemId]).normalize()\n",
    "\n",
    "#         row = {\n",
    "#             \"itemId\": itemId,\n",
    "#             \"item\": item_lookup.get(itemId, \"UNKNOWN\"),\n",
    "#             \"itemIdx\": int(last[\"itemIdx\"])\n",
    "#         }\n",
    "\n",
    "#         # ----------------------------------------------------\n",
    "#         # Copy model-stable numeric features (already normalized)\n",
    "#         # ----------------------------------------------------\n",
    "#         for col in numeric_cols:\n",
    "#             row[col] = last[col]\n",
    "\n",
    "#         # ----------------------------------------------------\n",
    "#         # Recompute DATE-SENSITIVE features\n",
    "#         # ----------------------------------------------------\n",
    "#         raw_updates = {\n",
    "#             \"daysSinceLastPurchase\": (predict_date - last_date).days,\n",
    "#             \"daysUntilNextHoliday\": HolidayFeatures.daysUntilNextHoliday(predict_date),\n",
    "#             \"daysSinceLastHoliday\": HolidayFeatures.daysSinceLastHoliday(predict_date),\n",
    "#             \"holidayProximityIndex\": HolidayFeatures.holidayProximityIndex(predict_date),\n",
    "#             \"daysUntilSchoolStart\": HolidayFeatures.daysUntilSchoolStart(predict_date),\n",
    "#             \"daysUntilSchoolEnd\": HolidayFeatures.daysUntilSchoolEnd(predict_date),\n",
    "#             \"schoolSeasonIndex\": HolidayFeatures.schoolSeasonIndex(predict_date),\n",
    "#             \"year\": predict_date.year,\n",
    "#             \"day\": predict_date.day,\n",
    "#             \"quarter\": predict_date.quarter\n",
    "#         }\n",
    "\n",
    "  \n",
    "#         # ----------------------------------------------------\n",
    "#         # Normalize recomputed features\n",
    "#         # ----------------------------------------------------\n",
    "#         for raw, val in raw_updates.items():\n",
    "#             norm_col = raw + \"_norm\"\n",
    "#             if norm_col in numeric_cols and raw in feature_stats:\n",
    "#                 stats = feature_stats[raw]\n",
    "#                 row[norm_col] = (val - stats[\"mean\"]) / stats[\"std\"]\n",
    "\n",
    "#         rows.append(row)\n",
    "\n",
    "#     pred_df = pd.DataFrame(rows)\n",
    "\n",
    "#     Xn = pred_df[numeric_cols].to_numpy(np.float32)\n",
    "#     Xi = pred_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "\n",
    "#     scores = model.predict([Xn, Xi], verbose=0).ravel()\n",
    "\n",
    "#     pred_df[\"due_intensity\"] = scores\n",
    "\n",
    "#     return (\n",
    "#         pred_df[[\"itemId\", \"item\", \"due_intensity\"]]\n",
    "#         .sort_values(\"due_intensity\", ascending=False)\n",
    "#         .reset_index(drop=True)\n",
    "#      )\n",
    "# ###############################################################################\n",
    "\n",
    "\n",
    "\n",
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # ENSURE itemIdx\n",
    "# # ------------------------------------------------------------\n",
    "# item_ids = sorted(encoded_df[\"itemId\"].unique())\n",
    "# item_id_to_idx = {iid: i for i, iid in enumerate(item_ids)}\n",
    "# encoded_df[\"itemIdx\"] = encoded_df[\"itemId\"].map(item_id_to_idx).astype(\"int32\")\n",
    "# NUM_ITEMS = len(item_ids)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # FEATURES / TARGET\n",
    "# # ------------------------------------------------------------\n",
    "# numeric_cols = [\n",
    "#     c for c in encoded_df.columns\n",
    "#     if c.endswith(\"_norm\") and c != \"due_score\"\n",
    "# ]\n",
    "\n",
    "# Xn = encoded_df[numeric_cols].to_numpy(np.float32)\n",
    "# Xi = encoded_df[\"itemIdx\"].to_numpy(np.int32)\n",
    "# y  = encoded_df[\"due_score\"].to_numpy(np.float32)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # SPLIT\n",
    "# # ------------------------------------------------------------\n",
    "# Xn_tr, Xn_te, Xi_tr, Xi_te, y_tr, y_te = train_test_split(\n",
    "#     Xn, Xi, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # MODEL\n",
    "# # ------------------------------------------------------------\n",
    "# num_in = layers.Input(shape=(Xn_tr.shape[1],))\n",
    "# itm_in = layers.Input(shape=(), dtype=\"int32\")\n",
    "\n",
    "# emb = layers.Embedding(NUM_ITEMS, 64)(itm_in)\n",
    "# emb = layers.Flatten()(emb)\n",
    "\n",
    "# x = layers.Concatenate()([num_in, emb])\n",
    "# x = layers.Dense(4096, activation=\"relu\")(x)\n",
    "# #x = layers.Dense(2048, activation=\"relu\")(x)\n",
    "# out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# model = models.Model([num_in, itm_in], out)\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(0.0001), loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "# history = model.fit(\n",
    "#     [Xn_tr, Xi_tr],\n",
    "#     y_tr,\n",
    "#     validation_split=0.1,\n",
    "#     epochs=10,\n",
    "#     batch_size=32,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # FEATURE STATS (ONLY recomputed features)\n",
    "# # (NOTE: stats are for *_norm columns since inference writes *_norm)\n",
    "# # ------------------------------------------------------------\n",
    "# feature_stats = {}\n",
    "# RECOMPUTED = [\n",
    "#     \"daysSinceLastPurchase\",\n",
    "#     \"daysUntilNextHoliday\",\n",
    "#     \"daysSinceLastHoliday\",\n",
    "#     \"holidayProximityIndex\",\n",
    "#     \"daysUntilSchoolStart\",\n",
    "#     \"daysUntilSchoolEnd\",\n",
    "#     \"schoolSeasonIndex\",\n",
    "#     \"year\", \"day\", \"quarter\",\n",
    "#     \"daysUntilBirthday_steve\", \"daysSinceBirthday_steve\",\n",
    "#     \"daysUntilBirthday_maggie\", \"daysSinceBirthday_maggie\",\n",
    "#     \"daysUntilBirthday_mil\", \"daysSinceBirthday_mil\",\n",
    "#     \"daysUntilBirthday_angie\", \"daysSinceBirthday_angie\",\n",
    "# ]\n",
    "\n",
    "# for raw in RECOMPUTED:\n",
    "#     col = raw + \"_norm\"\n",
    "#     if col in encoded_df.columns:\n",
    "#         std = encoded_df[col].std()\n",
    "#         feature_stats[raw] = {\n",
    "#             \"mean\": encoded_df[col].mean(),\n",
    "#             \"std\": std if std != 0 else 1.0\n",
    "#         }\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Grocery ML",
   "language": "python",
   "name": "grocery-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
