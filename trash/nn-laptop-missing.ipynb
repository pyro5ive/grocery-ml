{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192abc4e-1a90-416f-b168-9093c1493c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from winn_dixie_recpt_parser import WinnDixieRecptParser \n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "\n",
    "print(os.getcwd())\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e514d2-7c8e-45ea-a1bd-88d5d4e6ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grouped(grouped, rows=10):\n",
    "    # collect only the daysSinceLastPurchase_* columns\n",
    "    feature_cols = [c for c in grouped.columns if c.startswith(\"daysSinceLastPurchase_\")]\n",
    "\n",
    "    for i in range(min(rows, len(grouped))):\n",
    "        print(\"Row:\", i)\n",
    "        print(\"Date:\", grouped.iloc[i][\"date\"])\n",
    "        print(\"Time:\", grouped.iloc[i][\"time\"])\n",
    "        print(\"Items:\", grouped.iloc[i][\"item\"])\n",
    "        print(\"------ daysSinceLastPurchase ------\")\n",
    "\n",
    "        for col in feature_cols:\n",
    "            print(f\"{col}: {grouped.iloc[i][col]}\")\n",
    "\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "def show_encoded(encoded_df, rows=10):\n",
    "    # Identify columns\n",
    "    days_cols = [c for c in encoded_df.columns if c.startswith(\"daysSinceLastPurchase_\")]\n",
    "    weather_cols = [c for c in encoded_df.columns if c.endswith(\"_5day_avg\")]\n",
    "    item_cols = [\n",
    "        c for c in encoded_df.columns \n",
    "        if c not in days_cols \n",
    "        and c not in weather_cols\n",
    "        and c not in [\"date\", \"time\"]\n",
    "    ]\n",
    "\n",
    "    for i in range(min(rows, len(encoded_df))):\n",
    "        print(\"Row:\", i)\n",
    "        print(\"Date:\", encoded_df.iloc[i][\"date\"])\n",
    "        print(\"Time:\", encoded_df.iloc[i][\"time\"])\n",
    "\n",
    "        # Show the items purchased (reverse one-hot)\n",
    "        purchased_items = []\n",
    "        row_vals = encoded_df.iloc[i]\n",
    "\n",
    "        for item in item_cols:\n",
    "            if row_vals[item] == 1:\n",
    "                purchased_items.append(item)\n",
    "\n",
    "        print(\"Items:\", purchased_items)\n",
    "\n",
    "        print(\"------ daysSinceLastPurchase ------\")\n",
    "        for col in days_cols:\n",
    "            print(f\"{col}: {encoded_df.iloc[i][col]}\")\n",
    "\n",
    "        print(\"------ weather (rolling windows) ------\")\n",
    "        for col in weather_cols:\n",
    "            print(f\"{col}: {encoded_df.iloc[i][col]}\")\n",
    "\n",
    "        print(\"-----------------------------------\")\n",
    "\n",
    "\n",
    "def remove_duplicate_receipt_files(df):\n",
    "    \"\"\"\n",
    "    Remove whole source files that contain an identical receipt\n",
    "    to another file with the same date+time.\n",
    "    Minimal console output. Resets index at end.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"__signature\"] = (\n",
    "        df[\"date\"].astype(str) + \"|\" +\n",
    "        df[\"time\"].astype(str) + \"|\" +\n",
    "        df[\"item\"].astype(str) + \"|\" +\n",
    "        df[\"qty\"].astype(str) + \"|\" +\n",
    "        df[\"youPay\"].astype(str) + \"|\" +\n",
    "        df[\"reg\"].astype(str) + \"|\" +\n",
    "        df[\"reportedItemsSold\"].astype(str) + \"|\" +\n",
    "        df[\"cashier\"].astype(str) + \"|\" +\n",
    "        df[\"manager\"].astype(str)\n",
    "    )\n",
    "\n",
    "    keep_sources = set()\n",
    "\n",
    "    for (dt_date, dt_time), group in df.groupby([\"date\", \"time\"]):\n",
    "\n",
    "        # Build signature per source\n",
    "        source_signatures = {}\n",
    "        for source, rows in group.groupby(\"source\"):\n",
    "            sig = tuple(sorted(rows[\"__signature\"].tolist()))\n",
    "            source_signatures[source] = sig\n",
    "\n",
    "        # signature → list of sources\n",
    "        signature_groups = {}\n",
    "        for src, sig in source_signatures.items():\n",
    "            signature_groups.setdefault(sig, []).append(src)\n",
    "\n",
    "        # Handle duplicates\n",
    "        for sig, sources in signature_groups.items():\n",
    "            if len(sources) == 1:\n",
    "                keep_sources.add(sources[0])\n",
    "                continue\n",
    "\n",
    "            sorted_sources = sorted(sources)\n",
    "            kept = sorted_sources[0]\n",
    "            removed = sorted_sources[1:]\n",
    "\n",
    "            # Minimal output\n",
    "            print(f\"DUP: {dt_date} {dt_time} → keep {kept} ← drop {', '.join(removed)}\")\n",
    "\n",
    "            keep_sources.add(kept)\n",
    "\n",
    "    # Filter and clean\n",
    "    result = df[df[\"source\"].isin(keep_sources)].copy()\n",
    "    result.drop(columns=[\"__signature\"], inplace=True)\n",
    "\n",
    "    # ✔ Reset index here\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def rolling_freq(df, window_days):\n",
    "    out = []\n",
    "    for idx, row in df.iterrows():\n",
    "        item = row[\"item\"]\n",
    "        cutoff = row[\"date\"] - pd.Timedelta(days=window_days)\n",
    "        count = df[(df[\"item\"] == item) &\n",
    "                   (df[\"date\"] > cutoff) &\n",
    "                   (df[\"date\"] < row[\"date\"])].shape[0]\n",
    "        out.append(count)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47046089-a2d9-4136-95bb-3888c94aab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daysUntilNextHoliday(d):\n",
    "    d = pd.to_datetime(d)\n",
    "    holidays = USFederalHolidayCalendar().holidays()\n",
    "    diffs = (holidays - d).days\n",
    "    diffs = diffs[diffs >= 0]\n",
    "    return diffs.min() if len(diffs) > 0 else np.nan\n",
    "####################################################################\n",
    "\n",
    "def daysSinceLastHoliday(d):\n",
    "    d = pd.to_datetime(d)\n",
    "    holidays = USFederalHolidayCalendar().holidays()\n",
    "    diffs = (d - holidays).days\n",
    "    diffs = diffs[diffs >= 0]\n",
    "    return diffs.min() if len(diffs) > 0 else np.nan\n",
    "####################################################################\n",
    "\n",
    "def holidayProximityIndex(d, scale=30):\n",
    "    \"\"\"\n",
    "    Returns a smooth value between -1 and +1 depending on\n",
    "    distance to holidays. Neural networks LOVE this.\n",
    "    Negative = after holiday\n",
    "    Positive = before holiday\n",
    "    \"\"\"\n",
    "    before = daysUntilNextHoliday(d)\n",
    "    after = daysSinceLastHoliday(d)\n",
    "\n",
    "    if pd.isna(before) and pd.isna(after):\n",
    "        return 0\n",
    "\n",
    "    # choose the nearest side (before or after)\n",
    "    if before <= after:\n",
    "        return +max(0, (scale - before) / scale)\n",
    "    else:\n",
    "        return -max(0, (scale - after) / scale)\n",
    "####################################################################\n",
    "\n",
    "def daysUntilBirthday(d, bday):\n",
    "    d = pd.to_datetime(d)\n",
    "    bday = pd.to_datetime(bday)\n",
    "\n",
    "    this_year = pd.Timestamp(d.year, bday.month, bday.day)\n",
    "    if d <= this_year:\n",
    "        return (this_year - d).days\n",
    "    else:\n",
    "        next_year = pd.Timestamp(d.year + 1, bday.month, bday.day)\n",
    "        return (next_year - d).days\n",
    "####################################################################\n",
    "\n",
    "def daysSinceBirthday(d, bday):\n",
    "    d = pd.to_datetime(d)\n",
    "    bday = pd.to_datetime(bday)\n",
    "\n",
    "    this_year = pd.Timestamp(d.year, bday.month, bday.day)\n",
    "    if d >= this_year:\n",
    "        return (d - this_year).days\n",
    "    else:\n",
    "        last_year = pd.Timestamp(d.year - 1, bday.month, bday.day)\n",
    "        return (d - last_year).days\n",
    "####################################################################\n",
    "\n",
    "def tempDeviation(actualTemp, avgTemp):\n",
    "    \"\"\"Signed deviation (continuous). Neural-network gold.\"\"\"\n",
    "    return actualTemp - avgTemp\n",
    "####################################################################\n",
    "\n",
    "def humidityDeviation(actualHumidity, avgHumidity):\n",
    "    return actualHumidity - avgHumidity\n",
    "####################################################################\n",
    "\n",
    "def precipDeviation(actual, avg):\n",
    "    return actual - avg\n",
    "####################################################################\n",
    "\n",
    "def daysUntilSchoolStart(d):\n",
    "    d = pd.to_datetime(d)\n",
    "    start = pd.Timestamp(d.year, 8, 15)\n",
    "    if d <= start:\n",
    "        return (start - d).days\n",
    "    else:\n",
    "        next_start = pd.Timestamp(d.year + 1, 8, 15)\n",
    "        return (next_start - d).days\n",
    "####################################################################\n",
    "\n",
    "def daysUntilSchoolEnd(d):\n",
    "    d = pd.to_datetime(d)\n",
    "    end = pd.Timestamp(d.year, 5, 31)\n",
    "    if d <= end:\n",
    "        return (end - d).days\n",
    "    else:\n",
    "        next_end = pd.Timestamp(d.year + 1, 5, 31)\n",
    "        return (next_end - d).days\n",
    "####################################################################\n",
    "\n",
    "def schoolSeasonIndex(d):\n",
    "    \"\"\"\n",
    "    Smooth 0→1 curve inside school season.\n",
    "    <0 before season, >1 after.\n",
    "    Good for neural nets.\n",
    "    \"\"\"\n",
    "    d = pd.to_datetime(d)\n",
    "    start = pd.Timestamp(d.year, 8, 15)\n",
    "    end   = pd.Timestamp(d.year, 5, 31)\n",
    "\n",
    "    # If date is after Dec, school season continues in Jan–May.\n",
    "    if d < start:\n",
    "        return -((start - d).days) / 365.0\n",
    "    elif start <= d <= end:\n",
    "        return (d - start).days / (end - start).days\n",
    "    else:\n",
    "        return (d - end).days / 365.0\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "def normalizeAndDropCols(df, cols):\n",
    "    for col in cols:\n",
    "        # Replace the sentinel 999 with NaN so it doesn't distort mean/std\n",
    "        df[col] = df[col].replace(999, np.nan)\n",
    "\n",
    "        # Compute mean/std ignoring NaN\n",
    "        mean = df[col].mean()\n",
    "        std  = df[col].std() or 1.0\n",
    "\n",
    "        # Normalize\n",
    "        df[col + \"_norm\"] = (df[col] - mean) / std\n",
    "\n",
    "        # After normalization: missing values become 0 (neutral)\n",
    "        df[col + \"_norm\"] = df[col + \"_norm\"].fillna(0.0)\n",
    "\n",
    "    return df.drop(columns=cols)\n",
    "\n",
    "\n",
    "#def normalizeAndDropCols(df, cols):\n",
    "#    for col in cols:\n",
    "#        std = df[col].std() or 1.0\n",
    "#        df[col + \"_norm\"] = (df[col] - df[col].mean()) / std\n",
    "#    return df.drop(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb5f82-f270-4a38-9414-341412e3c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WEATHER PREP ---\n",
    "weatherCols=[\"datetime\", \"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"]\n",
    "df_weather = pd.read_csv(\"datasets/VisualCrossing-70062 2000-01-01 to 2025-12-14.csv\", usecols=weatherCols)\n",
    "\n",
    "df_weather[\"datetime\"] = pd.to_datetime(df_weather[\"datetime\"])\n",
    "df_weather = df_weather.set_index(\"datetime\").sort_index()\n",
    "\n",
    "df_weather[\"temp_5day_avg\"] = df_weather[\"temp\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"feelsLike_5day_avg\"] = df_weather[\"feelslike\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"dew_5day_avg\"] = df_weather[\"dew\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"humidity_5day_avg\"] = df_weather[\"humidity\"].rolling(5, min_periods=1).mean()\n",
    "df_weather[\"precip_5day_avg\"] = df_weather[\"precip\"].rolling(5, min_periods=1).mean()\n",
    "\n",
    "df_weather = df_weather.drop(columns=[\"temp\", \"humidity\", \"feelslike\", \"dew\", \"precip\"])\n",
    "\n",
    "# convert index to date for merging\n",
    "df_weather[\"date\"] = df_weather.index.date\n",
    "df_weather[\"date\"] = pd.to_datetime(df_weather[\"date\"])\n",
    "df_weather = df_weather.set_index(\"date\")\n",
    "\n",
    "#grouped.to_csv(\"grouped.csv\", index=False)\n",
    "#grouped.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622cefe-d5e0-496d-b547-0cfd003d4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "recptParser  = WinnDixieRecptParser();\n",
    "\n",
    "for p in Path(\"winndixie rcpts/StevePhone2/pdf/text\").glob(\"*.txt\"):\n",
    "    result = recptParser.parse(p.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "    for r in result[\"items\"]:\n",
    "        rows.append({\n",
    "            \"source\": p.name,\n",
    "            \"date\": result[\"date\"],\n",
    "            \"time\": result[\"time\"],\n",
    "            \"manager\": result[\"manager\"],\n",
    "            \"cashier\": result[\"cashier\"],\n",
    "            \"item\": r[\"item\"],\n",
    "            \"qty\": r[\"qty\"],\n",
    "            \"reg\": r[\"reg\"],\n",
    "            \"youPay\": r[\"youPay\"],\n",
    "            \"reportedItemsSold\": result[\"reported\"],\n",
    "            #\"rowsMatchReported\": result[\"validation\"][\"rowsMatchReported\"],\n",
    "            \"qtyMatchReported\": result[\"validation\"][\"qtyMatchReported\"],\n",
    "        })\n",
    "\n",
    "df_winndixie = pd.DataFrame(rows)\n",
    "\n",
    "df_winndixie[\"date\"] = pd.to_datetime(df_winndixie[\"date\"])\n",
    "df_winndixie[\"time\"] = df_winndixie[\"time\"].astype(str)\n",
    "df_winndixie = remove_duplicate_receipt_files(df_winndixie)\n",
    "df_winndixie = df_winndixie.sort_values(by=[\"date\", \"time\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4b591-943b-444d-ae9c-1557b99a282f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### CREATE ITEM IDs\n",
    "unique_items = sorted(df_winndixie[\"item\"].unique())\n",
    "item_to_id = {item: idx for idx, item in enumerate(unique_items)}\n",
    "id_to_item = {idx: item for item, idx in item_to_id.items()}\n",
    "df_winndixie[\"itemId\"] = df_winndixie[\"item\"].map(item_to_id)\n",
    "df_winndixie.reset_index(drop=True, inplace=True)\n",
    "df_winndixie.info()\n",
    "df_winndixie.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347749b-3977-40e3-a9a2-79c0f5c1389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build full rctps\n",
    "all_items = df_winndixie[\"itemId\"].unique()\n",
    "all_dates = df_winndixie[\"date\"].unique()\n",
    "\n",
    "full = (\n",
    "    pd.MultiIndex.from_product([all_dates, all_items], names=[\"date\", \"itemId\"])\n",
    "    .to_frame(index=False)\n",
    ")\n",
    "\n",
    "# Merge\n",
    "df_winndixie = full.merge(df_winndixie, on=[\"date\", \"itemId\"], how=\"left\")\n",
    "\n",
    "# NOW create didBuy field safely\n",
    "df_winndixie[\"didBuy\"] = df_winndixie[\"qty\"].notna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955044d3-645d-44ad-b9b0-f3129b1e565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build grouped table (one row per trip date)\n",
    "\n",
    "grouped = ( df_winndixie[[\"date\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"date\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 2. daysSinceLastTrip\n",
    "grouped[\"daysSinceLastTrip\"] = grouped[\"date\"].diff().dt.days.fillna(0)\n",
    "\n",
    "\n",
    "# 3. Holiday / Birthday / School features\n",
    "grouped[\"daysUntilNextHoliday\"] = grouped[\"date\"].apply(daysUntilNextHoliday)\n",
    "grouped[\"daysSinceLastHoliday\"] = grouped[\"date\"].apply(daysSinceLastHoliday)\n",
    "grouped[\"holidayProximityIndex\"] = grouped[\"date\"].apply(holidayProximityIndex)\n",
    "grouped[\"daysUntilSchoolStart\"] = grouped[\"date\"].apply(daysUntilSchoolStart)\n",
    "grouped[\"daysUntilSchoolEnd\"]   = grouped[\"date\"].apply(daysUntilSchoolEnd)\n",
    "grouped[\"schoolSeasonIndex\"]    = grouped[\"date\"].apply(schoolSeasonIndex)\n",
    "\n",
    "dt = grouped[\"date\"]\n",
    "grouped[\"year\"]    = dt.dt.year\n",
    "grouped[\"month\"]   = dt.dt.month\n",
    "grouped[\"day\"]     = dt.dt.day\n",
    "grouped[\"dow\"]     = dt.dt.dayofweek\n",
    "grouped[\"doy\"]     = dt.dt.dayofyear\n",
    "grouped[\"quarter\"] = dt.dt.quarter\n",
    "\n",
    "\n",
    "\n",
    "BIRTHDAYS = {\n",
    "    \"steve\":  \"03-05-1980\",  # fill with your real dates\n",
    "    \"maggie\": \"03-03-2016\",\n",
    "    \"mil\":    \"01-27-1962\",\n",
    "    \"angie\":  \"08-11-1981\",\n",
    "}\n",
    "\n",
    "grouped[\"daysUntilBirthday_steve\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, BIRTHDAYS[\"steve\"]))\n",
    "grouped[\"daysSinceBirthday_steve\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, BIRTHDAYS[\"steve\"]))\n",
    "\n",
    "\n",
    "grouped[\"daysUntilBirthday_maggie\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, BIRTHDAYS[\"maggie\"]))\n",
    "grouped[\"daysSinceBirthday_maggie\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, BIRTHDAYS[\"maggie\"]))\n",
    "\n",
    "grouped[\"daysUntilBirthday_mil\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, BIRTHDAYS[\"mil\"]))\n",
    "grouped[\"daysSinceBirthday_mil\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, BIRTHDAYS[\"mil\"]))\n",
    "\n",
    "grouped[\"daysUntilBirthday_angie\"] = grouped[\"date\"].apply(lambda d: daysUntilBirthday(d, BIRTHDAYS[\"angie\"]))\n",
    "grouped[\"daysSinceBirthday_angie\"] = grouped[\"date\"].apply(lambda d: daysSinceBirthday(d, BIRTHDAYS[\"angie\"]))\n",
    "\n",
    "\n",
    "grouped[\"shopper\"]    = grouped[\"date\"].apply(schoolSeasonIndex)\n",
    "\n",
    "# merge in weather\n",
    "grouped = grouped.merge(df_weather, on=\"date\", how=\"left\")\n",
    "\n",
    "df_winndixie = df_winndixie.merge(grouped, on=\"date\", how=\"left\")\n",
    "df_winndixie.info()\n",
    "df_winndixie.head(10)\n",
    "\n",
    "df_winndixie.to_csv(\"df_merged_group_level_features.csv\", index=False)\n",
    "\n",
    "### DAYS SINCE LAST PURCHASED PER ITEM\n",
    "# Must be sorted so diff() works correctly\n",
    "df_winndixie = df_winndixie.sort_values([\"itemId\", \"date\"])\n",
    "\n",
    "# Compute days since last purchase for each item\n",
    "df_winndixie[\"daysSinceLastPurchase\"] = (\n",
    "    df_winndixie.groupby(\"itemId\")[\"date\"].diff().dt.days\n",
    ")\n",
    "\n",
    "# First-ever purchase of an item = no previous date\n",
    "df_winndixie[\"daysSinceLastPurchase\"] = df_winndixie[\"daysSinceLastPurchase\"].fillna(999)\n",
    "\n",
    "df_winndixie.to_csv(\"df_merged_daysSinceLastPurchased.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf2225-3a20-45fe-810f-624a5e984ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "### Frequency per item\n",
    "\n",
    "#df_merged[\"freq_7\"]  = rolling_freq(df_merged, 7)\n",
    "#df_merged[\"freq_15\"]  = rolling_freq(df_merged, 15)\n",
    "#df_merged[\"freq_30\"] = rolling_freq(df_merged, 30)\n",
    "#df_merged[\"freq_90\"] = rolling_freq(df_merged, 90)\n",
    "#df_merged[\"freq_180\"] = rolling_freq(df_merged, 180)\n",
    "#df_merged[\"freq_365\"] = rolling_freq(df_merged, 365)\n",
    "\n",
    "\n",
    "#df_merged.to_csv(\"df_merged_with_freq.csv\", index=False)\n",
    "#df_merged.info()\n",
    "#df_merged.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514ec48-2b1f-4dd3-bba9-39d8e3eca0eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all unique item names\n",
    "#unique_items = grouped[\"item\"].explode().unique()\n",
    "\n",
    "# Map each item → index\n",
    "#item_to_index = {item: idx for idx, item in enumerate(unique_items)}\n",
    "\n",
    "#num_items = len(unique_items)\n",
    "#vectors = []\n",
    "\n",
    "# Build one-hot vector for each trip\n",
    "#for item_list in grouped[\"item\"]:\n",
    "#    vector = np.zeros(num_items, dtype=np.int32)\n",
    "#    for name in item_list:\n",
    "#        vector[item_to_index[name]] = 1\n",
    "#    vectors.append(vector)\n",
    "\n",
    "# Convert to DataFrame (THIS was missing)\n",
    "#encoded_items_df = pd.DataFrame(vectors, columns=unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cdaa6a-c44f-4d32-97d2-7acdd4922864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### NORMALIZE TO ENCODED_DF\n",
    "#freq_cols = [c for c in df_winndixie.columns if c.startswith(\"freq_\")]\n",
    "weather_cols = [c for c in df_winndixie.columns if c.endswith(\"_5day_avg\")]\n",
    "holiday_cols = [c for c in df_winndixie.columns if \"holiday\" in c.lower()]\n",
    "school_cols = [c for c in df_winndixie.columns if \"school\" in c.lower()]\n",
    "birthday_cols = [c for c in df_winndixie.columns if c.startswith(\"daysUntilBirthday_\") or c.startswith(\"daysSinceBirthday_\")]\n",
    "\n",
    "daysSince_purchase_cols = [\"daysSinceLastPurchase\"]\n",
    "daysSince_trip_cols     = [\"daysSinceLastTrip\"]\n",
    "\n",
    "\n",
    "encoded_df = df_winndixie.copy()\n",
    "\n",
    "#encoded_df = normalizeAndDropCols(encoded_df, freq_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, weather_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, holiday_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, school_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, birthday_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, daysSince_purchase_cols)\n",
    "encoded_df = normalizeAndDropCols(encoded_df, daysSince_trip_cols)\n",
    "\n",
    "encoded_df.info();\n",
    "encoded_df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b8c92-d433-4873-9856-c3f7cb6e5f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4. SINE COSINE on CYCLICAL FEATURES\n",
    "# ============================================================\n",
    "\n",
    "encoded_df[\"dow_sin\"] =   np.sin(2 * np.pi * encoded_df[\"dow\"] / 7.0)\n",
    "encoded_df[\"dow_cos\"] =   np.cos(2 * np.pi * encoded_df[\"dow\"] / 7.0)\n",
    "encoded_df[\"month_sin\"] = np.sin(2 * np.pi * encoded_df[\"month\"] / 12.0)\n",
    "encoded_df[\"month_cos\"] = np.cos(2 * np.pi * encoded_df[\"month\"] / 12.0)\n",
    "encoded_df[\"doy_sin\"] =   np.sin(2 * np.pi * encoded_df[\"doy\"] / 365.0)\n",
    "encoded_df[\"doy_cos\"] =   np.cos(2 * np.pi * encoded_df[\"doy\"] / 365.0)\n",
    "\n",
    "encoded_df = encoded_df.drop(columns=[\"dow\",\"month\",\"doy\"])\n",
    "\n",
    "## NON-CYCLIC TIME FEATURES\n",
    "nonCycCols = [\"year\",\"day\",\"quarter\"]\n",
    "encoded_df = normalizeAndDropCols(encoded_df, nonCycCols)\n",
    "#\n",
    "\n",
    "cols_to_drop = [\"source\",\"manager\",\"time\", \"cashier\", \"qty\", \"item\", \"reg\", \"youPay\", \"reportedItemsSold\", \"qtyMatchReported\", \"shopper\" ,\"date\"]\n",
    "encoded_df = encoded_df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "encoded_df.to_csv(\"encoded.csv\", index=False)\n",
    "encoded_df.info()\n",
    "encoded_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d5b52-1522-4fb9-87a4-f72e07812c17",
   "metadata": {},
   "source": [
    "# TRAIN / BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3235c3-00b6-4179-ae63-61e7c428ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULLY SELF-CONTAINED TRAINING + SAVING METHODS (FINAL)\n",
    "# ============================================================\n",
    "\n",
    "import os, json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING PROGRESS CALLBACK\n",
    "# ============================================================\n",
    "class EpochStatus(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: \"\n",
    "            f\"loss={logs.get('loss'):.4f}  \"\n",
    "            f\"val_loss={logs.get('val_loss'):.4f}  \"\n",
    "            f\"acc={logs.get('accuracy'):.4f}\"\n",
    "        )\n",
    "\n",
    "# ============================================================\n",
    "# SAVE EXPERIMENT (MODEL, METRICS, HYPERPARAMS)\n",
    "# ============================================================\n",
    "def save_experiment(model, history, HP):\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    name = f\"exp_e{HP['epochs']}_b{HP['batch_size']}_lr{HP['learning_rate']}\"\n",
    "    exp_dir = os.path.join(\"experiments\", f\"{timestamp}_{name}\")\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "    # Save hyperparameters\n",
    "    with open(os.path.join(exp_dir, \"hyperparams.json\"), \"w\") as f:\n",
    "        json.dump(HP, f, indent=4)\n",
    "\n",
    "    # Save metrics CSV\n",
    "    pd.DataFrame(history.history).to_csv(\n",
    "        os.path.join(exp_dir, \"training_metrics.csv\"), index=False\n",
    "    )\n",
    "\n",
    "    # Save model architecture\n",
    "    with open(os.path.join(exp_dir, \"model_arch.json\"), \"w\") as f:\n",
    "        f.write(model.to_json())\n",
    "\n",
    "    # Save weights\n",
    "    model.save_weights(os.path.join(exp_dir, \"model.weights.h5\"))\n",
    "\n",
    "    print(f\"\\nSaved experiment to: {exp_dir}\")\n",
    "\n",
    "    return exp_dir\n",
    "##################################################################################\n",
    "\n",
    "def get_last_trip_weather_norm(df_raw, encoded_df):\n",
    "    \"\"\"\n",
    "    Returns a dict of { weather_norm_col: value } from the last trip.\n",
    "    \"\"\"\n",
    "    last_trip_date = df_raw[\"date\"].max()\n",
    "    mask = df_raw[\"date\"] == last_trip_date\n",
    "    idx = df_raw.index[mask][0]\n",
    "\n",
    "    weather_cols = [\n",
    "        \"temp_5day_avg_norm\",\n",
    "        \"feelsLike_5day_avg_norm\",\n",
    "        \"dew_5day_avg_norm\",\n",
    "        \"humidity_5day_avg_norm\",\n",
    "        \"precip_5day_avg_norm\",\n",
    "    ]\n",
    "\n",
    "    weather = {}\n",
    "    for col in weather_cols:\n",
    "        weather[col] = encoded_df.loc[idx, col]\n",
    "\n",
    "    return weather, last_trip_date\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "def build_today_trip_features(df_raw, encoded_df):\n",
    "    \"\"\"\n",
    "    Builds all trip-level features for *today*, normalized,\n",
    "    using last trip for weather and daysSinceLastTrip.\n",
    "    \"\"\"\n",
    "    today = pd.Timestamp.today().normalize()\n",
    "\n",
    "    weather_norm, last_trip_date = get_last_trip_weather_norm(df_raw, encoded_df)\n",
    "\n",
    "    # --- raw trip-level values for today ---\n",
    "    days_since_last_trip_raw = (today - last_trip_date).days\n",
    "\n",
    "    d_unext  = daysUntilNextHoliday(today)\n",
    "    d_slast  = daysSinceLastHoliday(today)\n",
    "    h_index  = holidayProximityIndex(today)\n",
    "\n",
    "    d_ss     = daysUntilSchoolStart(today)\n",
    "    d_se     = daysUntilSchoolEnd(today)\n",
    "    s_index  = schoolSeasonIndex(today)\n",
    "\n",
    "    year_raw     = today.year\n",
    "    month_raw    = today.month\n",
    "    day_raw      = today.day\n",
    "    dow_raw      = today.dayofweek        # 0–6\n",
    "    doy_raw      = today.dayofyear        # 1–366\n",
    "    quarter_raw  = (month_raw - 1) // 3 + 1\n",
    "\n",
    "    # birthdays\n",
    "    d_steve_u = daysUntilBirthday(today, BIRTHDAYS[\"steve\"])\n",
    "    d_steve_s = daysSinceBirthday(today, BIRTHDAYS[\"steve\"])\n",
    "    d_mag_u   = daysUntilBirthday(today, BIRTHDAYS[\"maggie\"])\n",
    "    d_mag_s   = daysSinceBirthday(today, BIRTHDAYS[\"maggie\"])\n",
    "    d_mil_u   = daysUntilBirthday(today, BIRTHDAYS[\"mil\"])\n",
    "    d_mil_s   = daysSinceBirthday(today, BIRTHDAYS[\"mil\"])\n",
    "    d_ang_u   = daysUntilBirthday(today, BIRTHDAYS[\"angie\"])\n",
    "    d_ang_s   = daysSinceBirthday(today, BIRTHDAYS[\"angie\"])\n",
    "\n",
    "    # --- normalize using training stats ---\n",
    "    feats = {}\n",
    "\n",
    "    feats[\"daysSinceLastTrip_norm\"]        = normalize_single_value(df_raw, \"daysSinceLastTrip\",       days_since_last_trip_raw)\n",
    "    feats[\"daysUntilNextHoliday_norm\"]     = normalize_single_value(df_raw, \"daysUntilNextHoliday\",    d_unext)\n",
    "    feats[\"daysSinceLastHoliday_norm\"]     = normalize_single_value(df_raw, \"daysSinceLastHoliday\",    d_slast)\n",
    "    feats[\"holidayProximityIndex_norm\"]    = normalize_single_value(df_raw, \"holidayProximityIndex\",   h_index)\n",
    "\n",
    "    feats[\"daysUntilSchoolStart_norm\"]     = normalize_single_value(df_raw, \"daysUntilSchoolStart\",    d_ss)\n",
    "    feats[\"daysUntilSchoolEnd_norm\"]       = normalize_single_value(df_raw, \"daysUntilSchoolEnd\",      d_se)\n",
    "    feats[\"schoolSeasonIndex_norm\"]        = normalize_single_value(df_raw, \"schoolSeasonIndex\",       s_index)\n",
    "\n",
    "    feats[\"year_norm\"]                     = normalize_single_value(df_raw, \"year\",                    year_raw)\n",
    "    feats[\"day_norm\"]                      = normalize_single_value(df_raw, \"day\",                     day_raw)\n",
    "    feats[\"quarter_norm\"]                  = normalize_single_value(df_raw, \"quarter\",                 quarter_raw)\n",
    "\n",
    "    feats[\"daysUntilBirthday_steve_norm\"]  = normalize_single_value(df_raw, \"daysUntilBirthday_steve\", d_steve_u)\n",
    "    feats[\"daysSinceBirthday_steve_norm\"]  = normalize_single_value(df_raw, \"daysSinceBirthday_steve\", d_steve_s)\n",
    "    feats[\"daysUntilBirthday_maggie_norm\"] = normalize_single_value(df_raw, \"daysUntilBirthday_maggie\", d_mag_u)\n",
    "    feats[\"daysSinceBirthday_maggie_norm\"] = normalize_single_value(df_raw, \"daysSinceBirthday_maggie\", d_mag_s)\n",
    "    feats[\"daysUntilBirthday_mil_norm\"]    = normalize_single_value(df_raw, \"daysUntilBirthday_mil\",    d_mil_u)\n",
    "    feats[\"daysSinceBirthday_mil_norm\"]    = normalize_single_value(df_raw, \"daysSinceBirthday_mil\",    d_mil_s)\n",
    "    feats[\"daysUntilBirthday_angie_norm\"]  = normalize_single_value(df_raw, \"daysUntilBirthday_angie\",  d_ang_u)\n",
    "    feats[\"daysSinceBirthday_angie_norm\"]  = normalize_single_value(df_raw, \"daysSinceBirthday_angie\",  d_ang_s)\n",
    "\n",
    "    # cyclical encodings for today (no normalization)\n",
    "    feats[\"dow_sin\"]   = np.sin(2 * np.pi * dow_raw / 7.0)\n",
    "    feats[\"dow_cos\"]   = np.cos(2 * np.pi * dow_raw / 7.0)\n",
    "    feats[\"month_sin\"] = np.sin(2 * np.pi * (month_raw - 1) / 12.0)\n",
    "    feats[\"month_cos\"] = np.cos(2 * np.pi * (month_raw - 1) / 12.0)\n",
    "    feats[\"doy_sin\"]   = np.sin(2 * np.pi * (doy_raw - 1) / 365.0)\n",
    "    feats[\"doy_cos\"]   = np.cos(2 * np.pi * (doy_raw - 1) / 365.0)\n",
    "\n",
    "    # copy over last-trip normalized weather as requested\n",
    "    feats.update(weather_norm)\n",
    "\n",
    "    return feats\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "def build_item_features_today(item_id, df_raw):\n",
    "    \"\"\"\n",
    "    Compute daysSinceLastPurchase_norm for this item, for *today*.\n",
    "    \"\"\"\n",
    "    today = pd.Timestamp.today().normalize()\n",
    "\n",
    "    mask = df_raw[\"itemId\"] == item_id\n",
    "    last_purchase_date = df_raw.loc[mask, \"date\"].max()\n",
    "\n",
    "    days_since_last_purchase_raw = (today - last_purchase_date).days\n",
    "\n",
    "    feats = {}\n",
    "    feats[\"daysSinceLastPurchase_norm\"] = normalize_single_value(\n",
    "        df_raw,\n",
    "        \"daysSinceLastPurchase\",\n",
    "        days_since_last_purchase_raw\n",
    "    )\n",
    "    return feats\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "def build_feature_row_for_item_today(item_id, df_raw, encoded_df, trip_feats, feature_cols):\n",
    "    \"\"\"\n",
    "    Build a 1D np.array in the exact column order that the model was trained on.\n",
    "    \"\"\"\n",
    "    item_feats = build_item_features_today(item_id, df_raw)\n",
    "\n",
    "    values = {}\n",
    "\n",
    "    # identity\n",
    "    values[\"itemId\"] = item_id\n",
    "\n",
    "    # trip-level\n",
    "    values.update(trip_feats)\n",
    "\n",
    "    # item-level\n",
    "    values.update(item_feats)\n",
    "\n",
    "    # now order according to feature_cols\n",
    "    row = []\n",
    "    for col in feature_cols:\n",
    "        if col == \"didBuy\":\n",
    "            continue\n",
    "        row.append(values[col])\n",
    "    return np.array(row, dtype=np.float32)\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "def predict_all_items_today(model, df_raw, encoded_df):\n",
    "    \"\"\"\n",
    "    Runs EVERY itemId through the model for *today*\n",
    "    and returns ALL items with item names and probabilities,\n",
    "    sorted highest → lowest.\n",
    "    \"\"\"\n",
    "\n",
    "    # build itemId -> itemName lookup\n",
    "    item_lookup = (\n",
    "        df_raw[[\"itemId\", \"item\"]]\n",
    "        .drop_duplicates()\n",
    "        .set_index(\"itemId\")[\"item\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # exact training feature order\n",
    "    feature_cols = [c for c in encoded_df.columns if c != \"didBuy\"]\n",
    "\n",
    "    # shared trip features for today\n",
    "    trip_feats = build_today_trip_features(df_raw, encoded_df)\n",
    "\n",
    "    # all unique itemIds\n",
    "    item_ids = sorted(df_raw[\"itemId\"].unique())\n",
    "\n",
    "    X_rows = []\n",
    "    items = []\n",
    "\n",
    "    for item_id in item_ids:\n",
    "        row = build_feature_row_for_item_today(\n",
    "            item_id,\n",
    "            df_raw,\n",
    "            encoded_df,\n",
    "            trip_feats,\n",
    "            feature_cols\n",
    "        )\n",
    "        X_rows.append(row)\n",
    "        items.append(item_id)\n",
    "\n",
    "    X = np.stack(X_rows, axis=0)\n",
    "\n",
    "    # predict\n",
    "    probs = model.predict(X).reshape(-1)\n",
    "\n",
    "    # assemble output\n",
    "    result = pd.DataFrame({\n",
    "        \"itemId\": items,\n",
    "        \"itemName\": [item_lookup[i] for i in items],\n",
    "        \"prob\": probs\n",
    "    })\n",
    "\n",
    "    result = result.sort_values(\"prob\", ascending=False)\n",
    "\n",
    "    return result\n",
    "##################################################################################\n",
    "\n",
    "def predict_next_trip(model, encoded_df, input_feature_cols, frequent_items):\n",
    "    \"\"\"\n",
    "    Build a single input row for prediction using the most recent trip in encoded_df.\n",
    "    Returns a sorted DataFrame of predicted probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # 1. Get the most recent row (latest trip)\n",
    "    # ------------------------------\n",
    "    last = encoded_df.iloc[-1]\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2. Build a new row using last-known feature values\n",
    "    # ------------------------------\n",
    "    x = {}\n",
    "\n",
    "    for col in input_feature_cols:\n",
    "        if col in encoded_df.columns:\n",
    "            x[col] = last[col]\n",
    "        else:\n",
    "            # safety: unknown column\n",
    "            x[col] = 0.0\n",
    "\n",
    "    # Convert to model input shape\n",
    "    X_input = np.array([x[col] for col in input_feature_cols], dtype=np.float32)\n",
    "    X_input = X_input.reshape(1, -1)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 3. Predict probabilities\n",
    "    # ------------------------------\n",
    "    y_pred = model.predict(X_input)[0]   # shape: (num_items,)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 4. Build labeled output table\n",
    "    # ------------------------------\n",
    "    result = pd.DataFrame({\n",
    "        \"item\": frequent_items,\n",
    "        \"probability\": y_pred\n",
    "    })\n",
    "\n",
    "    # Sort highest-probability first\n",
    "    result = result.sort_values(by=\"probability\", ascending=False)\n",
    "\n",
    "    return result\n",
    "##################################################################################\n",
    "\n",
    "def normalize_single_value(df_raw, col_name, raw_value):\n",
    "    \"\"\"\n",
    "    Use the same mean/std logic as normalizeAndDropCols for a single scalar.\n",
    "    \"\"\"\n",
    "    col = df_raw[col_name].replace(999, np.nan)\n",
    "    mean = col.mean()\n",
    "    std = col.std() or 1.0\n",
    "\n",
    "    if pd.isna(raw_value):\n",
    "        return 0.0\n",
    "\n",
    "    return (raw_value - mean) / std\n",
    "##################################################################################\n",
    "\n",
    "def save_predictions(pred_df, experiment_name):\n",
    "    out_dir = f\"experiments/{experiment_name}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    pred_df.to_csv(f\"{out_dir}/predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11545566-b668-40e1-aa69-c4a4c7e583ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [10, 20, 30, 40, 50, 75, 100, 150, 200, 250, 350, 512]\n",
    "\n",
    "# --------------------------------\n",
    "# Fixed hyperparameters\n",
    "# --------------------------------\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Training data\n",
    "feature_cols = [\n",
    "    c for c in encoded_df.columns\n",
    "    if c != \"didBuy\" and np.issubdtype(encoded_df[c].dtype, np.number)\n",
    "]\n",
    "\n",
    "X = encoded_df[feature_cols].to_numpy(dtype=np.float32)\n",
    "y = encoded_df[\"didBuy\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Class weights for imbalance\n",
    "pos = y_train.sum()\n",
    "neg = len(y_train) - pos\n",
    "cw = min(50.0, neg / (pos + 1e-6))\n",
    "class_weights = {0: cw}\n",
    "\n",
    "# ===================================================================\n",
    "# LOOP OVER HIDDEN SIZES\n",
    "# ===================================================================\n",
    "for h in hidden_sizes:\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\" RUNNING EXPERIMENT: hidden={h}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # HP for this experiment\n",
    "    # ---------------------------------------------------------------\n",
    "    HP = {\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"hidden\": h\n",
    "    }\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Build model\n",
    "    # ---------------------------------------------------------------\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(X_train.shape[1],)),\n",
    "        layers.Dense(h, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Train\n",
    "    # ---------------------------------------------------------------\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        class_weight=class_weights,\n",
    "        verbose=0,\n",
    "        callbacks=[EpochStatus()]\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Save (model, history, HP)\n",
    "    # ---------------------------------------------------------------\n",
    "    exp_name = f\"exp_hidden{h}\"\n",
    "    save_experiment(model=model, history=history, HP=HP)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Run predictions on ALL items for TODAY\n",
    "    # ---------------------------------------------------------------\n",
    "    pred_df = predict_all_items_today(model, df_winndixie, encoded_df)\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Save predictions\n",
    "    # ---------------------------------------------------------------\n",
    "    save_predictions(pred_df, exp_name)\n",
    "\n",
    "    print(f\"Completed: {exp_name}\")\n",
    "    print(pred_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e2ba3-e77e-46ee-ac02-69de35218408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
